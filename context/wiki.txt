import pandas as pd
import subprocess
import io
import json
from datetime import datetime, timedelta

def run_steampipe_query(sql):
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except Exception as e:
        print(f"Steampipe query error:\n{e}")
        return pd.DataFrame()

# Get S3 buckets
print("üîç Fetching S3 buckets...")
df_buckets = run_steampipe_query("SELECT name FROM aws_s3_bucket;")
bucket_names = df_buckets["name"].tolist()
print(f"‚úÖ Found {len(bucket_names)} buckets")

# Query metrics for each bucket
bucket_data = []
end_date = datetime.utcnow()
start_date = end_date - timedelta(days=7)

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT value AS size_bytes, timestamp
    FROM aws_cloudwatch_metric_data_point
    WHERE id = 'e1'
      AND metric_stat = '{json.dumps(metric_stat)}'
      AND timestamp BETWEEN TIMESTAMP '{start_date.strftime('%Y-%m-%dT00:00:00Z')}'
                        AND TIMESTAMP '{end_date.strftime('%Y-%m-%dT00:00:00Z')}'
    ORDER BY value DESC
    LIMIT 1;
    """

    df_metric = run_steampipe_query(sql)
    if not df_metric.empty and "size_bytes" in df_metric.columns:
        size_gb = round(df_metric.iloc[0]["size_bytes"] / 1024 / 1024 / 1024, 2)
        bucket_data.append({
            "Bucket": bucket,
            "Size_GB": size_gb,
            "Timestamp": df_metric.iloc[0]["timestamp"]
        })
    else:
        print(f"‚ö†Ô∏è No metric found for: {bucket}")

# Final output
df_sizes = pd.DataFrame(bucket_data)
if df_sizes.empty:
    print("‚ùå No valid S3 bucket size metrics found.")
else:
    df_sizes = df_sizes.sort_values(by="Size_GB", ascending=False)
    print("\nüìä Top 10 S3 Buckets by Size:\n")
    print(df_sizes.head(10).to_string(index=False))

    df_sizes.to_csv("s3_bucket_sizes_final.csv", index=False)
    print("\n‚úÖ Full S3 size data saved to: s3_bucket_sizes_final.csv")




------
import pandas as pd
import subprocess
import io
import json

def run_steampipe_query(sql):
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except Exception as e:
        print(f"Error running Steampipe query:\n{e}")
        return pd.DataFrame()

# Step 1: Get all S3 buckets
print("Fetching bucket list...")
bucket_query = "SELECT name FROM aws_s3_bucket;"
df_buckets = run_steampipe_query(bucket_query)
bucket_names = df_buckets["name"].tolist()

# Step 2: Get sizes from CloudWatch
bucket_data = []
print(f"Found {len(bucket_names)} buckets. Fetching sizes...")

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT value AS size_bytes, timestamp
    FROM aws_cloudwatch_metric_data_point
    WHERE id = 'e1' AND metric_stat = '{json.dumps(metric_stat)}'
    ORDER BY timestamp DESC
    LIMIT 1;
    """

    df_metric = run_steampipe_query(sql)
    if not df_metric.empty and "size_bytes" in df_metric.columns:
        size_gb = round(df_metric.iloc[0]["size_bytes"] / 1024 / 1024 / 1024, 2)
        bucket_data.append({
            "Bucket": bucket,
            "Size_GB": size_gb
        })

# Step 3: Convert to DataFrame
df_sizes = pd.DataFrame(bucket_data)

if df_sizes.empty:
    print("No size data found for any buckets.")
else:
    df_sizes = df_sizes.sort_values(by="Size_GB", ascending=False)
    print("\nTop 10 Buckets by Size:\n")
    print(df_sizes.head(10).to_string(index=False))

    # Optional: Save full list
    df_sizes.to_csv("s3_bucket_sizes_verified.csv", index=False)
    print("\nSaved: s3_bucket_sizes_verified.csv")



-----
scp ec2-user@3.122.45.67:/home/ec2-user/keycloak.log C:\Users\Kiran\Downloads\

----
Internal Helm Chart Management & Deployment Flow

üéØ Objective
We have designed an improved and fully automated pipeline to manage external Helm charts and internal customizations for deployment into our private EKS cluster using Harness. This process ensures separation of concerns, upstream chart integrity, and fast, secure packaging & deployment cycles.

üèóÔ∏è Architectural Flow

1. Source Split:
Upstream Helm Charts Repo (helm-upstreams)
Mirrors official charts (e.g., Bitnami, Jetstack) into a read-only Bitbucket repo.
No customization or modification is performed in this repo.
Internal Customizations Repo (helm-overlays)
A second Bitbucket repo where teams:
Copy charts from upstream repo.
Customize with values-*.yaml (prod, stage, etc.).
Override templates if necessary (templates/, hooks, patches).
This repo is the source of truth for packaging.
2. CI/CD Pipeline Overview
Pipeline #1: Upstream Mirror Pipeline	Pipeline #2: Sync Upstream to Overlays Repo	Pipeline #3: Helm Package & JFrog Push Pipeline	Pipeline #4: Harness Helm Deployment Pipeline
Pull Helm charts from public Helm repo	Sync upstream chart content into helm-overlays/<chart>	Package chart with custom values.yaml + push to JFrog	Deploy Helm chart from JFrog Helm repo to EKS
Push to helm-upstreams Bitbucket repo	Protect custom values/overlays while syncing templates	Version & tag Helm chart (e.g., 1.0.0-prod)	Run Helm deploy step inside Harness workflow
Scheduled (or manual)	PR-based sync into overlays repo (automated via CI)	Update Artifactory index.yaml	Target namespace in EKS via Harness connector
3. Toolchain

Helm: Chart packaging and templating.
Jenkins Pipelines: CI automation for mirroring, syncing, packaging, pushing.
Checkmarx One: IaC and Helm template security scanning in packaging pipeline.
JFrog Artifactory: Artifact repository for Helm charts (replacing GitHub Pages).
Harness.io: CD pipeline to deploy Helm charts into EKS environments.
4. Benefits

Separation of Upstream vs Internal Ownership:
Pure upstream charts stay clean.
Internal customization repo owns environment-specific configurations.
Improved Security & Compliance:
Automated Checkmarx One scanning in CI/CD.
JFrog Artifactory with RBAC and immutable releases.
Automated Artifact Lifecycle:
CI pipelines handle chart packaging, semver tagging, and publishing to Artifactory.
Streamlined CD to EKS:
Harness Helm pipelines deploy charts from Artifactory into EKS using dynamic inputs.
5. Target Outcomes

Faster Helm chart adoption cycle.
No fork drift between upstream charts and custom internal overlays.
Enhanced artifact traceability (via Artifactory promotions and tagging).
Decoupled customization from upstream vendor charts.
Audit-friendly PR and CI pipelines with enforced static analysis.
