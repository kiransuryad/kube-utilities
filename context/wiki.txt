import pandas as pd
import subprocess
import io
import json

def run_steampipe_query(sql):
    """Execute a Steampipe SQL query and return as DataFrame."""
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except subprocess.CalledProcessError as e:
        print(f"Steampipe error:\n{e.stderr.decode()}")
        return pd.DataFrame()
    except pd.errors.EmptyDataError:
        print("Steampipe query returned empty CSV.")
        return pd.DataFrame()

# 1Ô∏è‚É£ Get all S3 bucket names
bucket_query = "SELECT name FROM aws_s3_bucket;"
df_buckets = run_steampipe_query(bucket_query)
bucket_names = df_buckets["name"].tolist()

# 2Ô∏è‚É£ Loop through buckets and retrieve their size from CloudWatch metrics
bucket_sizes = []

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT value AS size_bytes, timestamp
    FROM aws_cloudwatch_metric_data_point
    WHERE id = 'e1' AND metric_stat = '{json.dumps(metric_stat)}'
    ORDER BY timestamp DESC
    LIMIT 1;
    """

    df_metric = run_steampipe_query(sql)
    if not df_metric.empty and "size_bytes" in df_metric.columns:
        size_bytes = df_metric.iloc[0]["size_bytes"]
        size_gb = round(size_bytes / 1024 / 1024 / 1024, 2)
        bucket_sizes.append({
            "Bucket Name": bucket,
            "Size (GB)": size_gb,
            "Timestamp": df_metric.iloc[0]["timestamp"]
        })

# 3Ô∏è‚É£ Convert to DataFrame and sort
df_sizes = pd.DataFrame(bucket_sizes)
df_sizes.sort_values(by="Size (GB)", ascending=False, inplace=True)

# 4Ô∏è‚É£ Display Top 10
print("\nüìä Top 10 S3 Buckets by Size (GB):\n")
print(df_sizes.head(10).to_string(index=False))

# 5Ô∏è‚É£ Optional: Save full results to CSV
df_sizes.to_csv("s3_bucket_sizes.csv", index=False)
print("\n‚úÖ Full bucket size report saved to: s3_bucket_sizes.csv")


-----
scp ec2-user@3.122.45.67:/home/ec2-user/keycloak.log C:\Users\Kiran\Downloads\

----
Internal Helm Chart Management & Deployment Flow

üéØ Objective
We have designed an improved and fully automated pipeline to manage external Helm charts and internal customizations for deployment into our private EKS cluster using Harness. This process ensures separation of concerns, upstream chart integrity, and fast, secure packaging & deployment cycles.

üèóÔ∏è Architectural Flow

1. Source Split:
Upstream Helm Charts Repo (helm-upstreams)
Mirrors official charts (e.g., Bitnami, Jetstack) into a read-only Bitbucket repo.
No customization or modification is performed in this repo.
Internal Customizations Repo (helm-overlays)
A second Bitbucket repo where teams:
Copy charts from upstream repo.
Customize with values-*.yaml (prod, stage, etc.).
Override templates if necessary (templates/, hooks, patches).
This repo is the source of truth for packaging.
2. CI/CD Pipeline Overview
Pipeline #1: Upstream Mirror Pipeline	Pipeline #2: Sync Upstream to Overlays Repo	Pipeline #3: Helm Package & JFrog Push Pipeline	Pipeline #4: Harness Helm Deployment Pipeline
Pull Helm charts from public Helm repo	Sync upstream chart content into helm-overlays/<chart>	Package chart with custom values.yaml + push to JFrog	Deploy Helm chart from JFrog Helm repo to EKS
Push to helm-upstreams Bitbucket repo	Protect custom values/overlays while syncing templates	Version & tag Helm chart (e.g., 1.0.0-prod)	Run Helm deploy step inside Harness workflow
Scheduled (or manual)	PR-based sync into overlays repo (automated via CI)	Update Artifactory index.yaml	Target namespace in EKS via Harness connector
3. Toolchain

Helm: Chart packaging and templating.
Jenkins Pipelines: CI automation for mirroring, syncing, packaging, pushing.
Checkmarx One: IaC and Helm template security scanning in packaging pipeline.
JFrog Artifactory: Artifact repository for Helm charts (replacing GitHub Pages).
Harness.io: CD pipeline to deploy Helm charts into EKS environments.
4. Benefits

Separation of Upstream vs Internal Ownership:
Pure upstream charts stay clean.
Internal customization repo owns environment-specific configurations.
Improved Security & Compliance:
Automated Checkmarx One scanning in CI/CD.
JFrog Artifactory with RBAC and immutable releases.
Automated Artifact Lifecycle:
CI pipelines handle chart packaging, semver tagging, and publishing to Artifactory.
Streamlined CD to EKS:
Harness Helm pipelines deploy charts from Artifactory into EKS using dynamic inputs.
5. Target Outcomes

Faster Helm chart adoption cycle.
No fork drift between upstream charts and custom internal overlays.
Enhanced artifact traceability (via Artifactory promotions and tagging).
Decoupled customization from upstream vendor charts.
Audit-friendly PR and CI pipelines with enforced static analysis.
