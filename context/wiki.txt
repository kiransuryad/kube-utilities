Here's a breakdown of the process and commands to configure AppRole in HashiCorp Vault and use it in Harness for establishing connectivity:

I. Configure AppRole in HashiCorp Vault:

Step 1: Enable the AppRole Auth Method (if not already enabled)

Bash
vault auth enable approle
Step 2: Create a Vault Policy defining the permissions for Harness

This policy will specify what secrets and paths Harness will be allowed to access in Vault.

Terraform
# Example policy named "harness-approle-policy.hcl"
path "secret/data/my-application/*" {
  capabilities = ["read"]
}
path "secret/metadata/my-application/*" {
  capabilities = ["list"]
}
# Add more paths and capabilities as needed
Apply the policy to Vault:

Bash
vault policy write harness-approle-policy harness-approle-policy.hcl
Step 3: Create an AppRole

Define the AppRole name and link it to the policy you just created.

Bash
vault write auth/approle/role/harness-approle \
    token_policies="harness-approle-policy" \
    token_ttl="1h" \
    token_max_ttl="24h"
harness-approle: This is the name of the AppRole you're creating. Choose a descriptive name.
token_policies="harness-approle-policy": This links the AppRole to the policy you created earlier.
token_ttl="1h": Sets the initial Time-To-Live for tokens generated using this AppRole.
token_max_ttl="24h": Sets the maximum Time-To-Live for tokens generated using this AppRole.
Step 4: Retrieve the Role ID

Harness will need this Role ID to identify the AppRole.

Bash
vault read auth/approle/role/harness-approle/role-id
The output will look something like:

Key        Value
---        -----
role_id    xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
Step 5: Generate a Secret ID

Harness will use this Secret ID along with the Role ID to authenticate with Vault and obtain a client token. Note: Secret IDs can be configured for single use or multiple uses. For security in Harness, it's generally recommended to use a bound Secret ID if your Harness setup allows it, which ties the Secret ID to specific client IPs or other constraints. However, for simplicity in this example, we'll create a standard Secret ID.

Bash
vault write -f auth/approle/role/harness-approle/secret-id
The output will contain the Secret ID:

Key             Value
---             -----
secret_id       yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy
secret_id_accessor  zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz
Important Security Considerations for Secret IDs:

Single-Use vs. Multiple-Use: By default, Secret IDs can be used multiple times. For enhanced security, you can create single-use Secret IDs using the -once flag:
Bash
vault write -f auth/approle/role/harness-approle/secret-id -once
Bound Secret IDs (Recommended for Production): You can restrict the usage of a Secret ID to specific client IPs or other criteria. This is highly recommended for production environments. Refer to the Vault documentation for details on bound Secret IDs.
II. Configure Harness to Connect to Vault using AppRole:

The exact steps might vary slightly depending on your specific Harness version and setup. However, the general process is as follows:

Step 1: Navigate to Secret Management in Harness

In your Harness project or organization, go to Security and then Secrets Management.
Step 2: Add a New Secret Manager

Click on + Add Secret Manager.
Choose HashiCorp Vault.
Step 3: Configure the Vault Connection

Name: Give your Secret Manager a descriptive name (e.g., Vault-AppRole).
Vault URL: Enter the base URL of your HashiCorp Vault instance (e.g., https://my-vault.example.com:8200).
Authentication Type: Select AppRole.
Role ID: Enter the Role ID you retrieved from Vault in Step I.4 (xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx).
Secret ID: Enter the Secret ID you generated in Vault in Step I.5 (yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy).
Namespace (Optional): If your Vault instance uses namespaces, specify the relevant namespace.
Token TTL (Optional): You can configure a specific TTL for the tokens Harness obtains from Vault. If left blank, it will use the default configured in Vault.
Use Vault Agent (Optional): If you are using Vault Agent in your Harness environment, you can configure it here.
SSL Configuration (Optional): Configure SSL settings if your Vault instance uses TLS/SSL with custom certificates.
Step 4: Test the Connection

Harness usually provides an option to test the connection to your Vault instance. Make sure the test is successful.
Step 5: Save the Secret Manager

Once the connection is tested successfully, save the Secret Manager configuration.
III. Using the Vault Connection in Harness

Now that you have configured the Vault Secret Manager using AppRole, you can use it in your Harness pipelines and other configurations to access secrets stored in Vault.

Example: Referencing a Secret in a Harness Pipeline

Let's say you have a secret named my-secret with a key api_key stored in Vault at the path secret/data/my-application/.

In your Harness pipeline step where you need to use this secret, you can reference it using the following syntax (the exact syntax might depend on the specific Harness component you are using):

${secrets.getValue("my-secret", "api_key")}
Or, if you are using the full path:

${secrets.getValue("secret/data/my-application/my-secret", "api_key")}
Important Considerations:

Security: Treat the Secret ID as sensitive information. Avoid storing it directly in version control systems. Harness securely stores the Secret ID after you configure the Secret Manager.
Vault Agent: For more advanced scenarios and enhanced security, consider using Vault Agent with AppRole in Harness. Vault Agent can handle the token lifecycle management.
Error Handling: Ensure your Harness pipelines and applications have proper error handling in case the connection to Vault fails or the required secrets are not accessible.
Auditing: Vault provides audit logs that track all actions, including authentication attempts. Monitor these logs for any suspicious activity.
Policy Granularity: Design your Vault policies with the principle of least privilege in mind. Grant only the necessary permissions to the Harness AppRole.
Secret ID Rotation: Depending on your security requirements, you might want to implement a process for rotating Secret IDs periodically.
By following these steps, you can successfully configure AppRole authentication for your HashiCorp Vault and use it within Harness to securely access your secrets. Remember to adapt the commands and configurations to your specific Vault and Harness environment.

2 things, 1. I tried the above process but unable to establish successful connection, 2. I tried successfully using vault token but after 24 hrs I need to manually generate a new token which is defeating the purpose.
Okay, let's troubleshoot the AppRole connection issue and address the Vault token expiration problem.

1. Troubleshooting AppRole Connection Failure:

Since the previous steps didn't lead to a successful connection, let's systematically check the configuration on both Vault and Harness.

Vault Side Checks:

Verify Vault URL: Ensure the Vault URL you're using in Harness is correct and accessible from where your Harness Delegate is running. You can try to curl or telnet to this URL from the Delegate host.
Verify AppRole Auth Method is Enabled: Run the following command in your Vault CLI:
Bash
vault auth list
You should see approle/ listed under the Path column. If not, enable it using vault auth enable approle.
Verify Policy is Created:
Bash
vault policy list
Confirm that the harness-approle-policy (or whatever name you used) is listed.
Verify Policy Contents: Ensure the policy has the correct paths and capabilities required by Harness.
Bash
vault policy read harness-approle-policy
Verify AppRole is Created:
Bash
vault read auth/approle/role/harness-approle
Confirm that the harness-approle role exists.
Verify Policy Attached to AppRole: In the output of the command above, under the token_policies section, you should see your policy listed (e.g., harness-approle-policy).
Verify Role ID: Double-check the Role ID you copied from Vault. You can retrieve it again:
Bash
vault read auth/approle/role/harness-approle/role-id
Verify Secret ID: Ensure you copied the correct Secret ID. If you created a single-use Secret ID, it might have already been consumed. Try generating a new Secret ID (unless you specifically configured bound Secret IDs that require specific conditions to be met).
Bash
vault write -f auth/approle/role/harness-approle/secret-id
vault read auth/approle/role/harness-approle/secret-id
Vault Server Logs: Check your Vault server logs for any error messages related to authentication attempts from Harness. These logs can provide valuable insights into why the connection is failing. Look for messages related to AppRole, invalid Role ID, or invalid Secret ID.
Network Connectivity: Ensure there are no firewall rules or network restrictions preventing communication between your Harness Delegate and the Vault server on the specified port (usually 8200).
Harness Side Checks:

Verify Secret Manager Configuration: Double-check all the details you entered when configuring the HashiCorp Vault Secret Manager in Harness:
Name: Ensure it's the correct Secret Manager you are trying to use.
Vault URL: Make absolutely sure this matches your Vault server's address. Pay attention to http vs. https and the port number.
Authentication Type: Confirm that AppRole is selected.
Role ID: Carefully re-enter or paste the Role ID you obtained from Vault.
Secret ID: Carefully re-enter or paste the Secret ID you obtained from Vault.
Namespace: If you are using namespaces in Vault, ensure the correct namespace is specified in Harness.






-------------
import pandas as pd
import subprocess
import io
import json
from datetime import datetime, timedelta

def run_steampipe_query(sql):
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except Exception as e:
        print(f"Steampipe query error:\n{e}")
        return pd.DataFrame()

# Get S3 buckets
print("üîç Fetching S3 buckets...")
df_buckets = run_steampipe_query("SELECT name FROM aws_s3_bucket;")
bucket_names = df_buckets["name"].tolist()
print(f"‚úÖ Found {len(bucket_names)} buckets")

# Query metrics for each bucket
bucket_data = []
end_date = datetime.utcnow()
start_date = end_date - timedelta(days=7)

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT value AS size_bytes, timestamp
    FROM aws_cloudwatch_metric_data_point
    WHERE id = 'e1'
      AND metric_stat = '{json.dumps(metric_stat)}'
      AND timestamp BETWEEN TIMESTAMP '{start_date.strftime('%Y-%m-%dT00:00:00Z')}'
                        AND TIMESTAMP '{end_date.strftime('%Y-%m-%dT00:00:00Z')}'
    ORDER BY value DESC
    LIMIT 1;
    """

    df_metric = run_steampipe_query(sql)
    if not df_metric.empty and "size_bytes" in df_metric.columns:
        size_gb = round(df_metric.iloc[0]["size_bytes"] / 1024 / 1024 / 1024, 2)
        bucket_data.append({
            "Bucket": bucket,
            "Size_GB": size_gb,
            "Timestamp": df_metric.iloc[0]["timestamp"]
        })
    else:
        print(f"‚ö†Ô∏è No metric found for: {bucket}")

# Final output
df_sizes = pd.DataFrame(bucket_data)
if df_sizes.empty:
    print("‚ùå No valid S3 bucket size metrics found.")
else:
    df_sizes = df_sizes.sort_values(by="Size_GB", ascending=False)
    print("\nüìä Top 10 S3 Buckets by Size:\n")
    print(df_sizes.head(10).to_string(index=False))

    df_sizes.to_csv("s3_bucket_sizes_final.csv", index=False)
    print("\n‚úÖ Full S3 size data saved to: s3_bucket_sizes_final.csv")




------
import pandas as pd
import subprocess
import io
import json

def run_steampipe_query(sql):
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except Exception as e:
        print(f"Error running Steampipe query:\n{e}")
        return pd.DataFrame()

# Step 1: Get all S3 buckets
print("Fetching bucket list...")
bucket_query = "SELECT name FROM aws_s3_bucket;"
df_buckets = run_steampipe_query(bucket_query)
bucket_names = df_buckets["name"].tolist()

# Step 2: Get sizes from CloudWatch
bucket_data = []
print(f"Found {len(bucket_names)} buckets. Fetching sizes...")

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT value AS size_bytes, timestamp
    FROM aws_cloudwatch_metric_data_point
    WHERE id = 'e1' AND metric_stat = '{json.dumps(metric_stat)}'
    ORDER BY timestamp DESC
    LIMIT 1;
    """

    df_metric = run_steampipe_query(sql)
    if not df_metric.empty and "size_bytes" in df_metric.columns:
        size_gb = round(df_metric.iloc[0]["size_bytes"] / 1024 / 1024 / 1024, 2)
        bucket_data.append({
            "Bucket": bucket,
            "Size_GB": size_gb
        })

# Step 3: Convert to DataFrame
df_sizes = pd.DataFrame(bucket_data)

if df_sizes.empty:
    print("No size data found for any buckets.")
else:
    df_sizes = df_sizes.sort_values(by="Size_GB", ascending=False)
    print("\nTop 10 Buckets by Size:\n")
    print(df_sizes.head(10).to_string(index=False))

    # Optional: Save full list
    df_sizes.to_csv("s3_bucket_sizes_verified.csv", index=False)
    print("\nSaved: s3_bucket_sizes_verified.csv")



-----
scp ec2-user@3.122.45.67:/home/ec2-user/keycloak.log C:\Users\Kiran\Downloads\

----
Internal Helm Chart Management & Deployment Flow

üéØ Objective
We have designed an improved and fully automated pipeline to manage external Helm charts and internal customizations for deployment into our private EKS cluster using Harness. This process ensures separation of concerns, upstream chart integrity, and fast, secure packaging & deployment cycles.

üèóÔ∏è Architectural Flow

1. Source Split:
Upstream Helm Charts Repo (helm-upstreams)
Mirrors official charts (e.g., Bitnami, Jetstack) into a read-only Bitbucket repo.
No customization or modification is performed in this repo.
Internal Customizations Repo (helm-overlays)
A second Bitbucket repo where teams:
Copy charts from upstream repo.
Customize with values-*.yaml (prod, stage, etc.).
Override templates if necessary (templates/, hooks, patches).
This repo is the source of truth for packaging.
2. CI/CD Pipeline Overview
Pipeline #1: Upstream Mirror Pipeline	Pipeline #2: Sync Upstream to Overlays Repo	Pipeline #3: Helm Package & JFrog Push Pipeline	Pipeline #4: Harness Helm Deployment Pipeline
Pull Helm charts from public Helm repo	Sync upstream chart content into helm-overlays/<chart>	Package chart with custom values.yaml + push to JFrog	Deploy Helm chart from JFrog Helm repo to EKS
Push to helm-upstreams Bitbucket repo	Protect custom values/overlays while syncing templates	Version & tag Helm chart (e.g., 1.0.0-prod)	Run Helm deploy step inside Harness workflow
Scheduled (or manual)	PR-based sync into overlays repo (automated via CI)	Update Artifactory index.yaml	Target namespace in EKS via Harness connector
3. Toolchain

Helm: Chart packaging and templating.
Jenkins Pipelines: CI automation for mirroring, syncing, packaging, pushing.
Checkmarx One: IaC and Helm template security scanning in packaging pipeline.
JFrog Artifactory: Artifact repository for Helm charts (replacing GitHub Pages).
Harness.io: CD pipeline to deploy Helm charts into EKS environments.
4. Benefits

Separation of Upstream vs Internal Ownership:
Pure upstream charts stay clean.
Internal customization repo owns environment-specific configurations.
Improved Security & Compliance:
Automated Checkmarx One scanning in CI/CD.
JFrog Artifactory with RBAC and immutable releases.
Automated Artifact Lifecycle:
CI pipelines handle chart packaging, semver tagging, and publishing to Artifactory.
Streamlined CD to EKS:
Harness Helm pipelines deploy charts from Artifactory into EKS using dynamic inputs.
5. Target Outcomes

Faster Helm chart adoption cycle.
No fork drift between upstream charts and custom internal overlays.
Enhanced artifact traceability (via Artifactory promotions and tagging).
Decoupled customization from upstream vendor charts.
Audit-friendly PR and CI pipelines with enforced static analysis.
