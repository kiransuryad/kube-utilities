Hi [Recipient's Name],

Thank you for setting up the meeting. I appreciate your time.

This discussion is regarding the AWS account EDP-mgmt (1212212122). To ensure our call is as productive as possible, please let me know if there are any specific aspects of this account youâ€™d like to focus on. Alternatively, I can walk you through the key services running, such as EC2, EKS clusters, and others.

Looking forward to your thoughts.

Best regards,



-------
Hey Komi,

Just wanted to take a moment to appreciate the help you provided in resolving the EKS â†’ NLB â†’ Ingress-Nginx â†’ Keycloak issue. Your insights on traffic flow, HTTPS handling, and NGINX configurations made a huge difference in getting things sorted.

Debugging this wasnâ€™t straightforward, but your clear approach and patience really helped in breaking it down step by step. Looking forward to wrapping up the remaining bits with your continued support!

Thanks again for the solid guidanceâ€”much appreciated!

Best,




------
helm uninstall ingress-nginx -n ingress-nginx
Step 2: Reinstall NGINX Ingress with ClusterIP or NodePort
Now, reinstall it without an NLB.

Option A: Use ClusterIP (Recommended for ALB)

helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx --create-namespace \
  --set controller.service.type=ClusterIP \
  --set controller.ingressClass=nginx



------------
ingress:
  enabled: true
  ingressClassName: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-buffer-size: "128k"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
    # Critical for proper header forwarding through multiple layers
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header X-Forwarded-Proto $scheme;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header Host $host;
  rules:
    - host: test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com
      paths:
        - path: /auth/
          pathType: Prefix
  tls:
    - hosts:
        - test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com
      secretName: keycloak-tls

----
replicaCount: 1

# Database Configuration
database:
  existingSecret: "keycloak-db-secret"
  vendor: "postgres"
  hostname: "test-keycloak-rds-cluster-instance-02.ctqmi8iak6gd.us-east-1.rds.amazonaws.com"
  port: 5432
  database: "keycloak"
  username: "keycloak"
  passwordSecret: "keycloak-db-secret"
  passwordSecretKey: "password"

# Keycloak Configuration
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
extraEnv:
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless
  - name: KC_HTTPS_CERTIFICATE_FILE
    value: "/opt/keycloak/certs/tls.crt"
  - name: KC_HTTPS_CERTIFICATE_KEY_FILE
    value: "/opt/keycloak/certs/tls.key"
  - name: KC_HOSTNAME
    value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
  - name: KC_HOSTNAME_URL
    value: "https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com/auth"
  - name: KC_HOSTNAME_ADMIN
    value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
  - name: KC_HOSTNAME_ADMIN_URL
    value: "https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com/auth"
  - name: KC_HOSTNAME_STRICT
    value: "false"
  - name: KC_HOSTNAME_STRICT_HTTPS
    value: "true"
  - name: KC_PROXY
    value: "edge"
  - name: KC_HTTP_RELATIVE_PATH
    value: "/auth"
  - name: KC_HTTP_ENABLED
    value: "true"  # Enable HTTP for internal communication
  - name: KC_HTTPS_PORT
    value: "8443"
  - name: KC_HTTP_PORT
    value: "8080"
  - name: KC_HEALTH_ENABLED
    value: "true"
  - name: KC_LOG_LEVEL
    value: "INFO"
  - name: KC_PROXY_ADDRESS_FORWARDING
    value: "true"
  - name: KC_DEFAULT_HTTP_HEADERS
    value: >-
      {
        "Content-Security-Policy": "default-src 'self' https:; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data: https:; frame-src 'self' https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com",
        "X-Frame-Options": "SAMEORIGIN",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains"
      }

# Service Configuration
service:
  type: ClusterIP
  port: 8443
  targetPort: 8443
  # Add HTTP port for internal communication
  httpPort: 8080
  httpTargetPort: 8080

# Ingress Configuration
ingress:
  enabled: true
  ingressClassName: "nginx"
  rules:
    - host: test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com
      paths:
        - path: /auth/
          pathType: ImplementationSpecific
  tls:
    - hosts:
        - test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com
      secretName: keycloak-tls

# Volume Configuration for TLS Certificates
extraVolumes:
  - name: keycloak-tls
    secret:
      secretName: keycloak-tls
extraVolumeMounts:
  - mountPath: /opt/keycloak/certs
    name: keycloak-tls
    readOnly: true

# Probes
startupProbe:
  httpGet:
    path: "/auth/health"
    port: 8443
    scheme: HTTPS
  initialDelaySeconds: 15
  timeoutSeconds: 1
  failureThreshold: 60
  periodSeconds: 5
livenessProbe:
  httpGet:
    path: "/auth/health/live"
    port: 8443
    scheme: HTTPS
  initialDelaySeconds: 0
  timeoutSeconds: 5
readinessProbe:
  httpGet:
    path: "/auth/health/ready"
    port: 8443
    scheme: HTTPS
  initialDelaySeconds: 10
  timeoutSeconds: 1

# Resource Allocation
resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

# Persistence Storage (if needed)
persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi





--------------
We need an IAM user with read-only access to AWS Account XYZ to allow our Python application to retrieve metadata and configuration details from AWS services programmatically.

Request Details:
IAM User Name: aws-readonly-user (or any naming convention as per your policy)
Permissions Required:
AWS Managed Policy: ReadOnlyAccess (grants read-only access to most AWS services)
AWS Managed Policy: IAMReadOnlyAccess (optional, required if we need read access to IAM resources such as users, roles, and policies)
Authentication Method:
Programmatic access (Access Key & Secret Key) for API calls from Python code
Intended Use Case:
The user will be used in our Python application running outside AWS to fetch AWS resource details securely.
Access Key Rotation Policy:
If an internal policy requires periodic key rotation, please share the standard rotation frequency and process.
Security Considerations:
The credentials will be securely stored using best practices (e.g., AWS Secrets Manager or environment variables).
No write or modification access is neededâ€”strictly read-only access.


------------
Hi Team,

Thank you for your call yesterday. I have reviewed AWS Account XYZ, and currently, we do not have the necessary permissions to create new users or modify existing user permissions.

To proceed with your request, you can raise a ServiceNow (SNOW) ticket using the following reference ticket. Alternatively, if you prefer, we are happy to raise the ticket on your behalf and support you throughout the process.

Please let us know your preferred approach so we can move forward accordingly.

Looking forward to your response.

Thank you.

----------------


extraEnv: |
    # Hostname and HTTPS Configuration
    - name: KC_HOSTNAME
      value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    - name: KC_HOSTNAME_URL
      value: "https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    - name: KC_HOSTNAME_ADMIN
      value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    
    # Strict HTTPS Enforcement
    - name: KC_HOSTNAME_STRICT
      value: "false"
    - name: KC_HOSTNAME_STRICT_HTTPS
      value: "true"
    
    # Proxy and Forwarding Configuration
    - name: KC_PROXY
      value: "edge"
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"
    - name: X_FORWARDED_PROTO
      value: "https"
    
    # Enhanced Content Security Policy
    - name: KC_HTTP_RELATIVE_PATH
      value: "/auth"
    - name: KC_DEFAULT_HTTP_HEADERS
      value: |
        {
          "Content-Security-Policy": "default-src 'self' https:; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' https:; frame-src 'self' https:",
          "X-Frame-Options": "SAMEORIGIN",
          "Strict-Transport-Security": "max-age=31536000; includeSubDomains"
        }
    
    # Logging and Monitoring
    - name: KC_LOG_LEVEL
      value: "INFO"
    - name: KC_METRICS_ENABLED
      value: "true"
    - name: KC_HEALTH_ENABLED
      value: "true"


----------
keycloak:
  extraEnv: |
    # âœ… Set the correct ALB hostname
    - name: KC_HOSTNAME
      value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    - name: KC_HOSTNAME_URL
      value: "https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    - name: KC_HOSTNAME_STRICT
      value: "false"  # Allow Keycloak to be accessed via different hostnames
    - name: KC_HOSTNAME_STRICT_HTTPS
      value: "true"   # Ensure HTTPS is always used
    - name: KC_HOSTNAME_ADMIN
      value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"

    # âœ… Ensure Keycloak understands it's behind an HTTPS proxy
    - name: KC_PROXY
      value: "edge"
    - name: X_FORWARDED_PROTO
      value: "https"
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"

    # âœ… Enable HTTP so NLB can reach Keycloak
    - name: KC_HTTP_ENABLED
      value: "true"

    # âœ… Content Security Policy Fix
    - name: KC_DEFAULT_HTTP_HEADERS
      value: '{"Content-Security-Policy": "frame-ancestors self https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com", "X-Frame-Options": "ALLOW-FROM https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"}'

    # âœ… Enable additional logs for debugging
    - name: KC_LOG_LEVEL
      value: "DEBUG"
    - name: KC_METRICS_ENABLED
      value: "true"
    - name: KC_HEALTH_ENABLED
      value: "true"



------------
Possible Causes and Debugging Steps

1. Check if the ALB Exists
Run the following command to verify if the ALB is properly created in AWS:

aws elbv2 describe-load-balancers --region us-east-1 --query "LoadBalancers[?DNSName=='test-keycloak-ext-app-lb-1446264669.us-east-1.elb.amazonaws.com']"
If the output is empty, the ALB does not exist.
If the output contains details, then the ALB is available.
âž¡ Fix: If the ALB does not exist, double-check your AWS configuration.

2. Verify the ALB's DNS Name
Run:

nslookup test-keycloak-ext-app-lb-1446264669.us-east-1.elb.amazonaws.com
If it fails, then AWS Route 53 or your ALB setup might be incorrect.
If it resolves to an IP, then the issue is likely with your local network or firewall.
âž¡ Fix: If DNS does not resolve, try replacing the ALB (delete and recreate it).

3. Check ALBâ€™s Scheme
Your ALB must be internet-facing. Run:

aws elbv2 describe-load-balancers --region us-east-1 --query "LoadBalancers[?DNSName=='test-keycloak-ext-app-lb-1446264669.us-east-1.elb.amazonaws.com'].Scheme"
Expected output:

"internet-facing"
If you see "internal", then your ALB is private and not reachable from the internet.
âž¡ Fix: You must recreate the ALB as an internet-facing ALB.

4. Verify ALB Security Group
Your ALBâ€™s security group must allow inbound traffic on port 443 (HTTPS).

Run:

aws ec2 describe-security-groups --region us-east-1 --group-ids <ALB_SECURITY_GROUP_ID>
Look for:

"Ingress": [
    {
        "IpProtocol": "tcp",
        "FromPort": 443,
        "ToPort": 443,
        "IpRanges": [{"CidrIp": "0.0.0.0/0"}]
    }
]
âž¡ Fix: If port 443 is not open to the internet, update the security group.

5. Check If ALB Has a Listener on Port 443
Run:

aws elbv2 describe-listeners --load-balancer-arn <ALB_ARN>
You should see:

"Protocol": "HTTPS",
"Port": 443
âž¡ Fix: If no listener exists for port 443, create one:

aws elbv2 create-listener --load-balancer-arn <ALB_ARN> \
  --protocol HTTPS --port 443 \
  --certificates CertificateArn=<CERTIFICATE_ARN> \
  --default-actions Type=forward,TargetGroupArn=<TARGET_GROUP_ARN>
6. Test from a Different Network
Try running curl from a different internet connection (e.g., mobile hotspot).
Some corporate networks block AWS ELB DNS resolution.
âž¡ Fix: If it works on a different network, your ISP or firewall is blocking access.

Final Summary

Check if the ALB exists: aws elbv2 describe-load-balancers
Verify DNS resolution: nslookup test-keycloak-ext-app-lb-1446264669.us-east-1.elb.amazonaws.com
Ensure ALB is internet-facing
Check ALB security group (allow inbound on port 443)
Ensure ALB has a listener on port 443
Try from a different network
Once the ALB resolves correctly, your curl request should work. ðŸš€




---------
Detailed Step-by-Step Guide:

Step 1: Create Service Account and Token in EKS
Create a Service Account:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: harness-delegate-sa
  namespace: kube-system
Save this as sa-harness-delegate.yaml and apply:

kubectl apply -f sa-harness-delegate.yaml
Create a ClusterRole with Necessary Permissions:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: harness-delegate-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints", "persistentvolumeclaims", "secrets", "configmaps"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["rolebindings", "roles"]
  verbs: ["create", "update", "patch", "delete"]
Save this as clusterrole-harness-delegate.yaml and apply:

kubectl apply -f clusterrole-harness-delegate.yaml
Bind the ClusterRole to the Service Account:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: harness-delegate-binding
subjects:
- kind: ServiceAccount
  name: harness-delegate-sa
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: harness-delegate-role
  apiGroup: rbac.authorization.k8s.io
Save this as rolebinding-harness-delegate.yaml and apply:

kubectl apply -f rolebinding-harness-delegate.yaml
Step 2: Generate the Service Account Token
Since you are using Kubernetes version 1.24 or later, the token isn't automatically generated. You need to manually create a bound token.

Create the token:
kubectl create token harness-delegate-sa --namespace kube-system
Alternatively, using a manifest:
apiVersion: v1
kind: Secret
type: kubernetes.io/service-account-token
metadata:
  name: harness-delegate-token
  annotations:
    kubernetes.io/service-account.name: "harness-delegate-sa"
  namespace: kube-system
Save this as secret-harness-delegate.yaml and apply:

kubectl apply -f secret-harness-delegate.yaml
Retrieve the token:
kubectl get secret harness-delegate-token -n kube-system -o jsonpath='{.data.token}' | base64 --decode
Step 3: Encode the Token for Harness
Encode the token to base64:
echo "<TOKEN_VALUE>" | base64
Copy the output. This will be used as a secret inside Harness.
Step 4: Configure Harness Encrypted Secret
Go to Harness UI:
Navigate to Account Settings â†’ Secrets Management â†’ Secrets.
Create a New Secret:
Choose Encrypted Text.
Name it, e.g., k8s-sa-token.
Paste the base64 encoded token from the previous step.
Save the secret.
Step 5: Create Kubernetes Connector in Harness
Navigate to Harness UI:
Go to Setup â†’ Connectors â†’ New Connector â†’ Kubernetes.
Configure the Kubernetes Connector:
Cluster Details:
Cluster URL: https://<PRIVATE_API_ENDPOINT>
Example: https://eks-vpc-endpoint.us-west-2.eks.amazonaws.com
Authentication Method:
Select Service Account Token.
Choose the secret you created (k8s-sa-token).
Network Settings:
No Proxy needed since it's within the same VPC.
Set Delegate Selection:
Select the Delegate running inside the ECS cluster in the same VPC.
This ensures that the network communication is private and secure.
Test and Save:
Click on Test Connection.
If successful, save the connector.
Step 6: Test the Setup
Deploy a test application or execute a Kubernetes command using the Harness pipeline.
Validate that the Harness Delegate can:
Create a deployment.
Update or Patch resources.
List existing resources.
Delete a deployment or resource.
You can check the permissions by running:
kubectl auth can-i list pods --as=system:serviceaccount:kube-system:harness-delegate-sa





----------------------
SELECT grantee, table_catalog, table_schema, table_name, privilege_type
FROM information_schema.role_table_grants
WHERE grantee = '<DB_USERNAME>';


SHOW default_transaction_read_only;
SELECT * FROM pg_roles WHERE rolname = '<DB_USERNAME>';

ALTER DATABASE <DB_NAME> SET default_transaction_read_only = off;


--------
CREATE TABLE test_table (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


---------
kubectl create secret generic keycloak-db-secret \
  --namespace keycloak \
  --from-literal=username=<DB_USERNAME> \
  --from-literal=password=<DB_PASSWORD>


-----------------------

database:
  # Don't create a secret for the DB password, use the existing K8s secret
  existingSecret: "keycloak-db-secret"
  existingSecretKey: "password"
  # Database vendor for Keycloak
  vendor: postgres
  # AWS RDS Endpoint (replace with your actual endpoint)
  hostname: <RDS_ENDPOINT>
  # Port for PostgreSQL (default is 5432)
  port: 5432
  # Name of the database in RDS
  database: <DB_NAME>
  # Database username (this is securely pulled from the secret)
  username: <DB_USERNAME>
  # Password is pulled from the existing K8s secret
  password: ""

# Enable Proxy Address Forwarding for correct URL redirects
keycloak:
  extraEnv: |
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"
    - name: KC_DB
      value: "postgres"
    - name: KC_DB_URL_HOST
      value: "<RDS_ENDPOINT>"
    - name: KC_DB_URL_PORT
      value: "5432"
    - name: KC_DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: keycloak-db-secret
          key: username
    - name: KC_DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: keycloak-db-secret
          key: password
    - name: KC_DB_URL_DATABASE
      value: "<DB_NAME>"
    - name: KC_DB_SCHEMA
      value: "public"

------


helm upgrade keycloak codecentric-internal/keycloakx \
  --namespace keycloak \
  -f keycloak-values.yaml


--------



----------
# Explicitly set the relative path and URL to include a trailing slash
http:
  relativePath: "/auth/"
  internalPort: http
  internalScheme: HTTP

keycloak:
  hostname: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
  hostnameStrict: false
  hostnameStrictBackchannel: false
  extraEnv: |
    - name: KEYCLOAK_FRONTEND_URL
      value: http://adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com/auth/
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"




--------
replicaCount: 1

# Override the container startup command to run Keycloak with custom options.
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
  - "--http-port=8080"
  - "--hostname-strict=false"

# Set extra environment variables.
extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless
  - name: KEYCLOAK_FRONTEND_URL
    value: http://adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
  - name: PROXY_ADDRESS_FORWARDING
    value: "true"

# Keycloak-specific configuration.
keycloak:
  hostname: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
  hostnameStrict: false
  hostnameStrictBackchannel: false

# Expose the Keycloak service on port 80 while targeting container port 8080.
service:
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  ingressClassName: nginx  # Use the new field instead of deprecated annotation
  servicePort: http
  rules:
    - host: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []  # No TLS for now
  annotations: {}

# Override fullname to ensure Helm regenerates the Ingress correctly.
fullnameOverride: keycloak
nameOverride: keycloak

resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

# Persistence configuration. For production, enable persistence and adjust storageClass/size.
persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi

# External database configuration.
postgresql:
  enabled: false




---------------
replicaCount: 1

# Override the container startup command to run Keycloak with custom options.
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
  - "--http-port=8080"
  - "--hostname-strict=false"

# Set extra environment variables.
extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless
  - name: KEYCLOAK_FRONTEND_URL
    value: http://adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com

# Keycloak-specific configuration.
keycloak:
  username: admin
  password: admin
  hostname: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
  hostnameStrict: false
  hostnameStrictBackchannel: false

# Expose the Keycloak service on port 80 while targeting container port 8080.
service:
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  ingressClassName: nginx  # Use the new field instead of deprecated annotation
  hosts:
    - host: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []  # No TLS for now
  annotations: {}

# Override fullname to ensure Helm regenerates the Ingress correctly.
fullnameOverride: keycloak

resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

# Persistence configuration. For production, enable persistence and adjust storageClass/size.
persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi

# External database configuration.
postgresql:
  enabled: false




-----------
replicaCount: 1

# Override the container startup command to run Keycloak with custom options.
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
  - "--http-port=8080"
  - "--hostname-strict=false"

# Set extra environment variables.
extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless

# Keycloak-specific configuration.
keycloak:
  username: admin
  password: admin

# Expose the Keycloak service on port 80 while targeting container port 8080.
service:
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  # Use your NLB DNS name here.
  hosts:
    - host: your-nlb-dns-name.aws-region.elb.amazonaws.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  annotations:
    kubernetes.io/ingress.class: "nginx"
  tls: []  # For now, TLS is not configured for testing.

resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi

postgresql:
  enabled: false






--------------------
--------------------
replicaCount: 1

# Override the container startup command to run Keycloak with custom options.
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
  - "--http-port=8080"
  - "--hostname-strict=false"

# Set extra environment variables.
extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless

# Keycloak-specific configuration.
keycloak:
  # Although we are also setting admin credentials via extraEnv,
  # this block may be used internally (for secret creation or other logic).
  username: admin
  password: admin

ingress:
  enabled: true
  # Replace the host value with the DNS name of your AWS NLB or custom domain.
  hosts:
    - host: your-nlb-dns-name.aws-region.elb.amazonaws.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  annotations:
    # This tells ingress-nginx to handle the routing.
    kubernetes.io/ingress.class: "nginx"
  tls: []  # For production, configure TLS as needed.

resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

# Persistence configuration. For production, enable persistence and adjust storageClass/size.
persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi

# External database configuration.
postgresql:
  enabled: false



---------
stage('Reindex Artifactory Helm Repo') {
    container('docker') {
        withCredentials([
            usernamePassword(
                credentialsId: 'jfrog-service-account',
                usernameVariable: 'ARTIFACTORY_USER',
                passwordVariable: 'ARTIFACTORY_PASSWORD'
            )
        ]) {
            script {
                // POST to the reindex endpoint for your Helm repo
                def reindexCmd = """
                    curl -X POST \\
                         -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} \\
                         "${env.ARTIFACTORY_URL}/artifactory/api/helm/${env.ARTIFACTORY_REPO}/reindex"
                """
                echo "Triggering reindex with:\n${reindexCmd}"
                def exitCode = sh(script: reindexCmd, returnStatus: true)
                
                if (exitCode == 0) {
                    echo "SUCCESS: Reindex triggered. Artifactory should update index.yaml."
                } else {
                    error "FAILURE: Reindex call failed with exit code ${exitCode}"
                }
            }
        }
    }
}




-----------
stage('Upload to Artifactory') {
    container('docker') {
        withCredentials([
            usernamePassword(
                credentialsId: 'jfrog-service-account',
                usernameVariable: 'ARTIFACTORY_USER',
                passwordVariable: 'ARTIFACTORY_PASSWORD'
            )
        ]) {
            script {
                // Identify the packaged chart file (assuming only one .tgz file exists)
                def chartFile = sh(script: "ls *.tgz", returnStdout: true).trim()
                echo "Uploading Helm chart: ${chartFile}"
                
                // Build the curl command (using -v for verbose logs)
                def curlCmd = """
                    curl -v -X PUT -H 'Content-Type: application/tar+gzip' \\
                         -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} \\
                         -T ${chartFile} \\
                         "${env.ARTIFACTORY_URL}/artifactory/${env.ARTIFACTORY_REPO}/${chartFile}"
                """
                echo "Running curl command:\n${curlCmd}"
                
                // Execute the curl command and capture the exit code
                def exitCode = sh(script: curlCmd, returnStatus: true)
                
                // Check the exit code to determine success/failure
                if (exitCode == 0) {
                    echo "SUCCESS: The Helm chart was successfully uploaded to Artifactory!"
                } else {
                    error "FAILURE: Upload to Artifactory failed with exit code ${exitCode}"
                }
            }
        }
    }
}



-------
@Library('framework-shared-libs@v1.0.0') _

// Set up pipeline parameters (similar to "parameters { ... }" in declarative)
properties([
    parameters([
        string(name: 'SOURCE_HELM_REPO_URL',    defaultValue: 'https://codecentric.github.io/helm-charts', description: 'Source Helm repository URL'),
        string(name: 'SOURCE_HELM_REPO_NAME',   defaultValue: 'codecentric',                               description: 'Name of the source Helm repository'),
        string(name: 'SOURCE_HELM_CHART_NAME',  defaultValue: 'keycloakx',                                description: 'Source Helm chart name'),
        string(name: 'SOURCE_HELM_CHART_VERSION', defaultValue: '',                                       description: 'Source Helm chart version (leave empty for latest)')
    ])
])

// Define a podTemplate that references your 'cicd.yaml'
podTemplate(
    yamlFile: 'cicd.yaml', 
    label: "docker-build-${env.BUILD_NUMBER}"
) {
    node("docker-build-${env.BUILD_NUMBER}") {
        // Set environment variables inside the node
        env.ARTIFACTORY_URL   = 'https://your-artifactory.example.com'
        env.ARTIFACTORY_REPO  = 'helm-local'
        env.INTERNAL_VERSION  = '1.0.0-internal'
        // Example of other environment variables you might use
        env.DOCKER_REGISTRY   = "docker-registry.default.svc:5000"
        // etc.

        stage('Checkout') {
            checkout scm
        }

        stage('Pull Helm Chart') {
            // Use the container name from your cicd.yaml that has Helm installed
            container('docker') {
                script {
                    // Add the source Helm repository and update
                    sh "helm repo add ${params.SOURCE_HELM_REPO_NAME} ${params.SOURCE_HELM_REPO_URL}"
                    sh "helm repo update"

                    // Build the helm pull command
                    def pullCmd = "helm pull ${params.SOURCE_HELM_REPO_NAME}/${params.SOURCE_HELM_CHART_NAME} --untar"
                    if (params.SOURCE_HELM_CHART_VERSION?.trim()) {
                        pullCmd += " --version ${params.SOURCE_HELM_CHART_VERSION}"
                    }
                    sh pullCmd
                }
            }
        }

        stage('Update Internal Version') {
            container('docker') {
                script {
                    // Update the version in Chart.yaml to your internal version
                    sh """
                        sed -i 's/^version:.*/version: ${env.INTERNAL_VERSION}/' ${params.SOURCE_HELM_CHART_NAME}/Chart.yaml
                    """
                }
            }
        }

        stage('Package Helm Chart') {
            container('docker') {
                script {
                    // Package the chart into a .tgz file
                    sh "helm package ${params.SOURCE_HELM_CHART_NAME}"
                }
            }
        }

        stage('Upload to Artifactory') {
            container('docker') {
                // Use Jenkins credentials for the Artifactory service account
                withCredentials([
                    usernamePassword(
                        credentialsId: 'jfrog-service-account',
                        usernameVariable: 'ARTIFACTORY_USER',
                        passwordVariable: 'ARTIFACTORY_PASSWORD'
                    )
                ]) {
                    script {
                        // Identify the packaged chart file (assuming only one .tgz)
                        def chartFile = sh(script: "ls *.tgz", returnStdout: true).trim()

                        // Upload via curl with basic auth
                        sh """
                            curl -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} -T ${chartFile} \\
                            "${env.ARTIFACTORY_URL}/artifactory/${env.ARTIFACTORY_REPO}/${chartFile}"
                        """
                    }
                }
            }
        }

        // EXAMPLE: If you have Docker build/push steps, add them as additional stages
        /*
        stage('Build Docker Image') {
            container('docker') {
                sh "docker build -t ${DOCKER_REGISTRY}/myimage:${BUILD_NUMBER} ."
            }
        }

        stage('Push Docker Image') {
            container('docker') {
                sh "docker push ${DOCKER_REGISTRY}/myimage:${BUILD_NUMBER}"
            }
        }
        */
    }
}







---------------
pipeline {
    agent any

    parameters {
        string(name: 'SOURCE_HELM_REPO_URL', defaultValue: 'https://codecentric.github.io/helm-charts', description: 'Source Helm repository URL')
        string(name: 'SOURCE_HELM_REPO_NAME', defaultValue: 'codecentric', description: 'Name of the source Helm repository')
        string(name: 'SOURCE_HELM_CHART_NAME', defaultValue: 'keycloakx', description: 'Source Helm chart name')
        string(name: 'SOURCE_HELM_CHART_VERSION', defaultValue: '', description: 'Source Helm chart version (leave empty for latest)')
    }

    environment {
        // Artifactory configuration (adjust these values for your environment)
        ARTIFACTORY_URL = 'https://your-artifactory.example.com'
        ARTIFACTORY_REPO = 'helm-local'
        // Internal version to be applied to the chart
        INTERNAL_VERSION = '1.0.0-internal'
    }

    stages {
        stage('Pull Helm Chart') {
            steps {
                script {
                    // Add the source Helm repository using parameters
                    sh "helm repo add ${params.SOURCE_HELM_REPO_NAME} ${params.SOURCE_HELM_REPO_URL}"
                    sh "helm repo update"
                    // Construct the helm pull command; include --version if provided
                    def pullCmd = "helm pull ${params.SOURCE_HELM_REPO_NAME}/${params.SOURCE_HELM_CHART_NAME} --untar"
                    if (params.SOURCE_HELM_CHART_VERSION?.trim()) {
                        pullCmd += " --version ${params.SOURCE_HELM_CHART_VERSION}"
                    }
                    sh pullCmd
                }
            }
        }
        stage('Update Internal Version') {
            steps {
                script {
                    // Update the version in Chart.yaml to use the internal version
                    // This example uses GNU sed; adjust for macOS if needed.
                    sh "sed -i 's/^version:.*/version: ${INTERNAL_VERSION}/' ${params.SOURCE_HELM_CHART_NAME}/Chart.yaml"
                }
            }
        }
        stage('Package Helm Chart') {
            steps {
                script {
                    // Package the chart, which will create a .tgz file (e.g., keycloakx-1.0.0-internal.tgz)
                    sh "helm package ${params.SOURCE_HELM_CHART_NAME}"
                }
            }
        }
        stage('Upload to Artifactory') {
            steps {
                // Use the Artifactory service account credentials stored in Jenkins (ID: jfrog-service-account)
                withCredentials([usernamePassword(credentialsId: 'jfrog-service-account', 
                                                  usernameVariable: 'ARTIFACTORY_USER', 
                                                  passwordVariable: 'ARTIFACTORY_PASSWORD')]) {
                    script {
                        // Identify the packaged chart file (assuming only one .tgz file exists)
                        def chartFile = sh(script: "ls *.tgz", returnStdout: true).trim()
                        // Use curl with basic authentication to upload the chart to Artifactory
                        sh """
                            curl -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} -T ${chartFile} \
                                 "${ARTIFACTORY_URL}/artifactory/${ARTIFACTORY_REPO}/${chartFile}"
                        """
                    }
                }
            }
        }
    }
}




-----------------------
pipeline {
  agent { label 'helm-enabled' } // Use a specific agent label with Git, Helm, and curl installed.
  
  parameters {
    string(name: 'ARTIFACTORY_URL', defaultValue: 'https://your-artifactory-domain/artifactory/helm', description: 'Artifactory Helm Repository URL')
    string(name: 'CHART_REPO', defaultValue: 'https://github.com/bitnami/charts.git', description: 'Git repository containing the helm charts')
    string(name: 'CHART_DIR', defaultValue: 'bitnami/keycloak', description: 'Relative path to the Keycloak chart within the repository')
  }
  
  environment {
    // Credentials can be reused across stages.
    ARTIFACTORY_CREDENTIALS = credentials('artifactory-credentials')
  }
  
  stages {
    stage('Test Artifactory Connectivity') {
      steps {
        script {
          // Test connectivity using curl and validate response code.
          sh '''
            RESPONSE=$(curl -u ${ARTIFACTORY_CREDENTIALS_USR}:${ARTIFACTORY_CREDENTIALS_PSW} -s -o /dev/null -w "%{http_code}" ${ARTIFACTORY_URL}/index.yaml)
            if [ "$RESPONSE" -ne 200 ]; then
              echo "Failed to connect to Artifactory. HTTP response code: $RESPONSE"
              exit 1
            fi
          '''
        }
      }
    }
    
    stage('Clone Helm Chart Repository') {
      steps {
        // Clone the repository. Using a shallow clone to speed up the process.
        sh "git clone --depth 1 ${params.CHART_REPO}"
      }
    }
    
    stage('Package Helm Chart') {
      steps {
        dir(params.CHART_DIR) {
          // Package the chart. This creates a .tgz file in the current directory.
          sh "helm package ."
        }
      }
    }
    
    stage('Push Helm Chart to Artifactory') {
      steps {
        script {
          withCredentials([usernamePassword(credentialsId: 'artifactory-credentials', 
                                               usernameVariable: 'ARTIFACTORY_USER', 
                                               passwordVariable: 'ARTIFACTORY_PASSWORD')]) {
            // Find the packaged chart file (assuming only one .tgz exists)
            def chartFile = sh(script: "ls ${params.CHART_DIR}/*.tgz", returnStdout: true).trim()
            echo "Chart package file: ${chartFile}"
            
            // Upload the chart using curl.
            sh """
              curl -u $ARTIFACTORY_USER:$ARTIFACTORY_PASSWORD -T ${chartFile} "${params.ARTIFACTORY_URL}/${chartFile.tokenize('/')[-1]}"
            """
            
            // Archive the chart artifact for traceability.
            archiveArtifacts artifacts: "${params.CHART_DIR}/*.tgz", allowEmptyArchive: false
          }
        }
      }
    }
  }
  
  post {
    success {
      echo 'Helm chart successfully pushed to Artifactory!'
    }
    failure {
      echo 'Pipeline failed. Please check the logs for details.'
    }
    cleanup {
      cleanWs()
    }
  }
}
