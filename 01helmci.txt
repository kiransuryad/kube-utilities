Hi Kailash,

Could you please suggest the next steps or the standard process for the following?

As you know, WP has currently provided us access to the DTS and EDP organizations on github.worldpay.com. Looking ahead, can we assume that you will be migrating these organizations into our FIS-managed GitHub, and subsequently share the new URLs and access with us so we can integrate them into our pipelines and related workflows?

Or, is it expected from our side to handle the migration of these repositories?

Would appreciate your input and guidance on this.

----
pipeline:
  name: Deploy-cert-manager-test-kc-helm-eks
  identifier: Deploycertmanagertestkchelmeks
  projectIdentifier: caaskeycloak
  orgIdentifier: keycloak
  tags: {}

  stages:
    - stage:
        name: Deploy cert-manager to test
        identifier: deploy_cert_manager_test
        type: Deployment
        spec:
          service:
            serviceRef: certmanagerservice02
          environment:
            environmentRef: testeksenv
            deployToAll: false
            infrastructureDefinitions:
              - identifier: testkchelmeksinfra
                inputs:
                  identifier: testkchelmeksinfra
                  type: KubernetesDirect
                  spec:
                    namespace: <+input>  # Runtime input during deployment

          execution:
            steps:

              # Step 1: Namespace Preparation
              - step:
                  type: ShellScript
                  name: Create Namespace If Missing
                  identifier: create_namespace
                  spec:
                    shell: Bash
                    onDelegate: true
                    executionTarget: {}
                    delegateSelectors:
                      - helm-delegate
                    environmentVariables:
                      - name: NAMESPACE
                        type: String
                        value: <+infra.namespace>
                    script: |
                      echo "Checking if namespace ${NAMESPACE} exists..."
                      if ! kubectl get ns ${NAMESPACE}; then
                        echo "Namespace does not exist. Creating..."
                        kubectl create ns ${NAMESPACE}
                      else
                        echo "Namespace ${NAMESPACE} already exists."
                      fi
                  timeout: 5m

              # Step 2: Helm Chart Deployment
              - step:
                  type: HelmDeploy
                  name: Helm Deploy
                  identifier: helm_deploy
                  spec:
                    skipDryRun: false
                    chart:
                      chartVersion: <+input>  # Runtime input
                    valuesPaths:
                      - values/cert-manager/test-inline-values.yaml
                  timeout: 10m

              # Step 3: Post-Deployment Readiness Check
              - step:
                  type: ShellScript
                  name: Verify Pod Readiness
                  identifier: verify_pods
                  spec:
                    shell: Bash
                    onDelegate: true
                    executionTarget: {}
                    delegateSelectors:
                      - helm-delegate
                    environmentVariables:
                      - name: NAMESPACE
                        type: String
                        value: <+infra.namespace>
                    script: |
                      echo "Checking pod readiness in ${NAMESPACE}..."
                      kubectl get pods -n ${NAMESPACE}
                      kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=cert-manager -n ${NAMESPACE} --timeout=60s
                  timeout: 5m


-----
echo "Checking pod status in ${NAMESPACE}..."
                      kubectl get pods -n ${NAMESPACE}
                      echo "Waiting for all pods to be Ready..."
                      kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=order-service -n ${NAMESPACE} --timeout=60s


----
Step-by-Step Setup

🟢 Step 1: Open VS Code Workspace

Open your Jenkins project/repo folder in VS Code.
Ensure your Jenkinsfile (or .groovy file) is inside the workspace (e.g., in root or under a pipelines/ directory).
🟢 Step 2: Install & Enable Extensions

You mentioned you already installed:

✅ Groovy Lint (by nvuillam)
✅ Format and Fix
Verify:

Open Extensions sidebar (Ctrl + Shift + X) → Confirm both are enabled.
🟢 Step 3: Create the .groovylintrc.json Configuration File

In VS Code Explorer sidebar, right-click your workspace root folder.
Choose "New File".
Name the file:
.groovylintrc.json
Paste this config inside:
{
  "rulesets": ["rulesets/basic.xml", "rulesets/braces.xml", "rulesets/naming.xml"],
  "rules": {
    "BracesForClass": { "enabled": true },
    "BracesForMethod": { "enabled": true },
    "Indentation": { "spacesPerIndentLevel": 4 },
    "LineLength": { "enabled": true, "length": 120 },
    "TrailingWhitespace": { "enabled": true },
    "UnnecessarySemicolon": { "enabled": true }
  }
}
✅ This sets indentation, brace rules, and line length guidelines.

🟢 Step 4: Edit VS Code User Settings

Open VS Code Command Palette:
Ctrl + Shift + P
Search for:
Preferences: Open Settings (JSON)
In the JSON file, add this block:
{
    "groovyLint.enable": true,
    "groovyLint.fixOnSave": true,
    "groovyLint.config": "${workspaceFolder}/.groovylintrc.json",
    "groovyLint.reportLevel": "info",
    "editor.formatOnSave": true,
    "[groovy]": {
        "editor.defaultFormatter": "nvuillam.vscode-groovy-lint"
    }
}
✅ This will:

Enable Groovy Lint globally.
Apply auto-fix on every save for Groovy/Jenkins files.
🟢 Step 5: Format & Fix Command (Manual Trigger)

Two ways to format manually:
Option A: Right-click Jenkinsfile → Format Document with...
Open your Jenkinsfile or .groovy.
Right-click inside the editor →
“Format Document with...” → select Groovy Lint Formatter.
Option B: Run command directly
Ctrl + Shift + P
Search:
"Groovy Lint: Format and Fix"
✅ Click to execute.
🟢 Step 6: Format-on-save (automatic)

Open your Jenkinsfile.
Make any edit (add/remove space).
Save file (Ctrl + S)
🔄 Groovy Lint autoformats based on your .groovylintrc.json.
🟢 Step 7: Validate

✅ After formatting, your Jenkinsfile will:

Apply 4-space indentation.
Enforce braces on classes/methods.
Auto-remove trailing whitespaces.
Flag long lines (>120 chars) as lint warnings.
💡 Optional: Keyboard Shortcut

Open Command Palette:
Ctrl + Shift + P
Search: Keyboard Shortcuts (JSON)
Add this binding:
{
  "key": "ctrl+alt+l",
  "command": "groovyLint.formatAndFix",
  "when": "editorTextFocus && editorLangId == 'groovy'"
}
Now you can press Ctrl + Alt + L to trigger format on demand!

Would you like me to apply this on a sample Jenkinsfile and show you the before/after formatting for visual confirmation? 🚀


----
Hello all,

All the repositories listed below, along with several others, reside under the Worldspace domain specific to AWS account 771019480822. Many of you may have already lost access to them.

Please review and confirm:

Whether you still have access to these repositories.
Which of these repositories are relevant for your team’s work.
Based on your assessment, please raise a migration request for the required repositories to FIS GitHub.
This will help ensure continuity and avoid any potential disruption.



-----
Subject: Guidance on Keycloak Migration & LDAP Configuration in CIO AWS Account

Good Afternoon Casey & Rajesh,

We have successfully deployed Keycloak into our test environment in the CIO AWS Account. As part of our testing, we are working on migrating data from the current non-prod environment to the CIO AWS test account.

We would appreciate your guidance on the following points:

Data Migration:
Can we proceed with fetching all the data from the non-prod environment and pushing it into our test environment for testing purposes?
Are there any considerations or constraints we should be aware of?
LDAP Configuration:
The current non-prod account is configured to use worldpay.local:3268 LDAP for the master realm, which will not be accessible from the CIO AWS Account.
Could you suggest an alternative LDAP that we can use from our CIO AWS Account?
If an alternative is not available, what process or next steps do you recommend?
FIS Data Migration:
We understand that only FIS data needs to be migrated.
Are there any specific steps, processes, or considerations we should follow for this migration?
Internal URL for IdP Configuration:
In the current setup, an internal URL is used for configuring with other IdPs.
Could you confirm if a similar internal URL is required in the CIO AWS Account setup?
Our understanding is that we need to reach out to Akamai to obtain a proper URL and whitelist the required IPs.
Could you confirm if this is the correct approach, or suggest any additional steps we should take?
Looking forward to your guidance and suggestions on these points.




-----------
Step 3: Reset the Admin Password

Keycloak hashes passwords using PBKDF2-SHA256, so you need to manually update it.

Generate a new hashed password using Keycloak CLI: If you have a working Keycloak installation somewhere, run:
/opt/keycloak/bin/kcadm.sh config credentials --server http://localhost:8080 --realm master --user admin --password 'NewSecurePassword'
If you cannot use Keycloak CLI, use an online PBKDF2-SHA256 hash generator, or use Python:

from hashlib import pbkdf2_hmac
import base64

password = "NewSecurePassword"
salt = b'12345678'  # You can change this

hash_bytes = pbkdf2_hmac('sha256', password.encode(), salt, 27500)
hash_b64 = base64.b64encode(hash_bytes).decode()

print(f"PBKDF2_SHA256 Hash: {hash_b64}")
Update the password in the PostgreSQL database:
UPDATE credential SET secret_data = '{"value":"<HASHED_PASSWORD>", "salt": "<SALT>"}'
WHERE user_id = '<ADMIN_USER_ID>';
Example:

UPDATE credential SET secret_data = '{"value":"ABC123hashedvalue", "salt": "12345678"}'
WHERE user_id = '1234abcd-5678-efgh-ijkl-9012mnop3456';
Step 4: Restart Keycloak

Restart Keycloak to ensure changes take effect:

systemctl restart keycloak
or if running in a Docker container:

docker restart <keycloak-container>
Step 5: Login with the New Password

Try logging in with:

Username: admin
Password: NewSecurePassword


Option 1: Generate Using Python

On any machine with Python, run this script:

from hashlib import pbkdf2_hmac
import base64
import os

password = "NewSecurePassword"
salt = os.urandom(16)  # Generate a random salt

hash_bytes = pbkdf2_hmac('sha256', password.encode(), salt, 27500)
hash_b64 = base64.b64encode(hash_bytes).decode()
salt_b64 = base64.b64encode(salt).decode()

print(f'Hashed Password: {hash_b64}')
print(f'Salt: {salt_b64}')
Replace "NewSecurePassword" with your desired password.
The script will output a hashed password and a salt.
Example Output:

Hashed Password: W3JLS0lZMUpzbm1PV0ZaVExWWmE=
Salt: Rm9vQmFyMTIzNDU2Nzg5MA==
Step 2: Insert a New Admin User in PostgreSQL
Connect to PostgreSQL:
psql -h <db-host> -U <db-user> -d <db-name>
Example:

psql -h my-keycloak-db.us-east-1.rds.amazonaws.com -U keycloak -d keycloak
Create the New User:
INSERT INTO user_entity (id, username, enabled, email, email_verified, created_timestamp)
VALUES ('new-admin-id-123', 'newadmin', true, 'newadmin@example.com', true, extract(epoch from now()) * 1000);
Set the Password for the New User (Replace <HASHED_PASSWORD> and <SALT> with values from the Python script):
INSERT INTO credential (id, user_id, type, created_date, secret_data, credential_data)
VALUES (
    'new-cred-id-456',
    'new-admin-id-123',
    'password',
    extract(epoch from now()) * 1000,
    '{"value":"<HASHED_PASSWORD>", "salt": "<SALT>", "algorithm":"pbkdf2-sha256", "iterations": 27500}',
    '{"hashIterations":27500, "algorithm":"pbkdf2-sha256"}'
);
Example using generated values:

INSERT INTO credential (id, user_id, type, created_date, secret_data, credential_data)
VALUES (
    'new-cred-id-456',
    'new-admin-id-123',
    'password',
    extract(epoch from now()) * 1000,
    '{"value":"W3JLS0lZMUpzbm1PV0ZaVExWWmE=", "salt": "Rm9vQmFyMTIzNDU2Nzg5MA==", "algorithm":"pbkdf2-sha256", "iterations": 27500}',
    '{"hashIterations":27500, "algorithm":"pbkdf2-sha256"}'
);
Assign the Admin Role to the New User:
INSERT INTO user_role_mapping (user_id, role_id, realm_id)
SELECT 'new-admin-id-123', id, realm_id FROM role WHERE name = 'admin';
Step 3: Restart Keycloak
To apply the changes, restart Keycloak:

systemctl restart keycloak
or if running in Docker:

docker restart <keycloak-container-name>



------
Solution: Reset Keycloak Admin OTP via CLI

Since you cannot log in via the UI, you can use Keycloak's admin CLI (kcadm.sh) to disable OTP for the admin user.

Step 1: Login to Keycloak Admin CLI
SSH into the Keycloak host machine and run:

cd /opt/keycloak/bin
./kcadm.sh config credentials --server http://localhost:8080 --realm master --user admin --password <admin-password>
Replace <admin-password> with the actual password.
If Keycloak is running behind a reverse proxy or a different hostname, change http://localhost:8080 accordingly.
Step 2: Check the Admin User Details
Run:

./kcadm.sh get users -r master --query username=admin
This will return details including the user ID.

Step 3: Disable OTP for the Admin User
Once you have the admin user ID, disable OTP:

./kcadm.sh update users/<user-id> -r master -s "totp=false"
Example:

./kcadm.sh update users/1234abcd-5678-efgh-ijkl-9012mnop3456 -r master -s "totp=false"
Alternatively, you can remove OTP credentials:

./kcadm.sh delete users/1234abcd-5678-efgh-ijkl-9012mnop3456/credentials -r master
Step 4: Restart Keycloak (If Needed)
If the changes don't take effect immediately, restart Keycloak:

systemctl restart keycloak
or if running as a container:

docker restart <keycloak-container-name>
Step 5: Login Again
Now, try logging in without OTP. If successful, you can set up a new OTP.

Reset OTP via UI (Optional)

Navigate to Keycloak Admin Console.
Go to Users → Admin User → Credentials.
Set a new password and enforce a new OTP.
Alternative: Create a Temporary Super Admin User

If disabling OTP doesn’t work, create a new admin user with full privileges:

./kcadm.sh create users -r master -s username=newadmin -s enabled=true
./kcadm.sh set-password -r master --username newadmin --new-password NewPassword123
./kcadm.sh add-roles -r master --uusername newadmin --rolename admin --cclientid realm-management
Then, log in with this new user, reset OTP for the old 

-----
Step 1: Zip the Dump File Inside the Pod

Since you're inside a Kubernetes pod, use tar or zip to compress the dump file.

Option 1: Using tar (Recommended)
Inside the pod, run:

tar -czvf backup.dump.tar.gz backup.dump
This compresses backup.dump into backup.dump.tar.gz.

Option 2: Using zip
If your pod has zip installed, run:

zip backup.dump.zip backup.dump
Step 2: Encrypt the Dump File

Use openssl to encrypt the file inside the pod.

Encrypt with AES-256
Inside the pod:

openssl enc -aes-256-cbc -salt -in backup.dump.tar.gz -out backup.dump.tar.gz.enc -pass pass:"YourSecurePassword"
-aes-256-cbc → Strong encryption
-salt → Adds randomness for security
-pass pass:"YourSecurePassword" → Replace with a strong password
Step 3: Copy the Encrypted File from Pod to Local

Now, get the encrypted file from the pod to your local machine.

Option 1: Use kubectl cp (Easiest)
From your local machine, run:

kubectl cp <pod-name>:/path/to/backup.dump.tar.gz.enc ./backup.dump.tar.gz.enc
Example:

kubectl cp my-pod:/tmp/backup.dump.tar.gz.enc ./backup.dump.tar.gz.enc
Step 4: Decrypt the File on Your Local Machine

On your local machine, run:

openssl enc -d -aes-256-cbc -in backup.dump.tar.gz.enc -out backup.dump.tar.gz -pass pass:"YourSecurePassword"
Then, extract it:

tar -xzvf backup.dump.tar.gz



-------
Hi [Recipient's Name],

Thank you for setting up the meeting. I appreciate your time.

This discussion is regarding the AWS account EDP-mgmt (1212212122). To ensure our call is as productive as possible, please let me know if there are any specific aspects of this account you’d like to focus on. Alternatively, I can walk you through the key services running, such as EC2, EKS clusters, and others.

Looking forward to your thoughts.

Best regards,



-------
Hey Komi,

Just wanted to take a moment to appreciate the help you provided in resolving the EKS → NLB → Ingress-Nginx → Keycloak issue. Your insights on traffic flow, HTTPS handling, and NGINX configurations made a huge difference in getting things sorted.

Debugging this wasn’t straightforward, but your clear approach and patience really helped in breaking it down step by step. Looking forward to wrapping up the remaining bits with your continued support!

Thanks again for the solid guidance—much appreciated!

Best,




------
helm uninstall ingress-nginx -n ingress-nginx
Step 2: Reinstall NGINX Ingress with ClusterIP or NodePort
Now, reinstall it without an NLB.

Option A: Use ClusterIP (Recommended for ALB)

helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx --create-namespace \
  --set controller.service.type=ClusterIP \
  --set controller.ingressClass=nginx



------------
ingress:
  enabled: true
  ingressClassName: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-buffer-size: "128k"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
    # Critical for proper header forwarding through multiple layers
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header X-Forwarded-Proto $scheme;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header Host $host;
  rules:
    - host: test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com
      paths:
        - path: /auth/
          pathType: Prefix
  tls:
    - hosts:
        - test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com
      secretName: keycloak-tls

----
replicaCount: 1

# Database Configuration
database:
  existingSecret: "keycloak-db-secret"
  vendor: "postgres"
  hostname: "test-keycloak-rds-cluster-instance-02.ctqmi8iak6gd.us-east-1.rds.amazonaws.com"
  port: 5432
  database: "keycloak"
  username: "keycloak"
  passwordSecret: "keycloak-db-secret"
  passwordSecretKey: "password"

# Keycloak Configuration
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
extraEnv:
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless
  - name: KC_HTTPS_CERTIFICATE_FILE
    value: "/opt/keycloak/certs/tls.crt"
  - name: KC_HTTPS_CERTIFICATE_KEY_FILE
    value: "/opt/keycloak/certs/tls.key"
  - name: KC_HOSTNAME
    value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
  - name: KC_HOSTNAME_URL
    value: "https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com/auth"
  - name: KC_HOSTNAME_ADMIN
    value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
  - name: KC_HOSTNAME_ADMIN_URL
    value: "https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com/auth"
  - name: KC_HOSTNAME_STRICT
    value: "false"
  - name: KC_HOSTNAME_STRICT_HTTPS
    value: "true"
  - name: KC_PROXY
    value: "edge"
  - name: KC_HTTP_RELATIVE_PATH
    value: "/auth"
  - name: KC_HTTP_ENABLED
    value: "true"  # Enable HTTP for internal communication
  - name: KC_HTTPS_PORT
    value: "8443"
  - name: KC_HTTP_PORT
    value: "8080"
  - name: KC_HEALTH_ENABLED
    value: "true"
  - name: KC_LOG_LEVEL
    value: "INFO"
  - name: KC_PROXY_ADDRESS_FORWARDING
    value: "true"
  - name: KC_DEFAULT_HTTP_HEADERS
    value: >-
      {
        "Content-Security-Policy": "default-src 'self' https:; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data: https:; frame-src 'self' https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com",
        "X-Frame-Options": "SAMEORIGIN",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains"
      }

# Service Configuration
service:
  type: ClusterIP
  port: 8443
  targetPort: 8443
  # Add HTTP port for internal communication
  httpPort: 8080
  httpTargetPort: 8080

# Ingress Configuration
ingress:
  enabled: true
  ingressClassName: "nginx"
  rules:
    - host: test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com
      paths:
        - path: /auth/
          pathType: ImplementationSpecific
  tls:
    - hosts:
        - test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com
      secretName: keycloak-tls

# Volume Configuration for TLS Certificates
extraVolumes:
  - name: keycloak-tls
    secret:
      secretName: keycloak-tls
extraVolumeMounts:
  - mountPath: /opt/keycloak/certs
    name: keycloak-tls
    readOnly: true

# Probes
startupProbe:
  httpGet:
    path: "/auth/health"
    port: 8443
    scheme: HTTPS
  initialDelaySeconds: 15
  timeoutSeconds: 1
  failureThreshold: 60
  periodSeconds: 5
livenessProbe:
  httpGet:
    path: "/auth/health/live"
    port: 8443
    scheme: HTTPS
  initialDelaySeconds: 0
  timeoutSeconds: 5
readinessProbe:
  httpGet:
    path: "/auth/health/ready"
    port: 8443
    scheme: HTTPS
  initialDelaySeconds: 10
  timeoutSeconds: 1

# Resource Allocation
resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

# Persistence Storage (if needed)
persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi





--------------
We need an IAM user with read-only access to AWS Account XYZ to allow our Python application to retrieve metadata and configuration details from AWS services programmatically.

Request Details:
IAM User Name: aws-readonly-user (or any naming convention as per your policy)
Permissions Required:
AWS Managed Policy: ReadOnlyAccess (grants read-only access to most AWS services)
AWS Managed Policy: IAMReadOnlyAccess (optional, required if we need read access to IAM resources such as users, roles, and policies)
Authentication Method:
Programmatic access (Access Key & Secret Key) for API calls from Python code
Intended Use Case:
The user will be used in our Python application running outside AWS to fetch AWS resource details securely.
Access Key Rotation Policy:
If an internal policy requires periodic key rotation, please share the standard rotation frequency and process.
Security Considerations:
The credentials will be securely stored using best practices (e.g., AWS Secrets Manager or environment variables).
No write or modification access is needed—strictly read-only access.


------------
Hi Team,

Thank you for your call yesterday. I have reviewed AWS Account XYZ, and currently, we do not have the necessary permissions to create new users or modify existing user permissions.

To proceed with your request, you can raise a ServiceNow (SNOW) ticket using the following reference ticket. Alternatively, if you prefer, we are happy to raise the ticket on your behalf and support you throughout the process.

Please let us know your preferred approach so we can move forward accordingly.

Looking forward to your response.

Thank you.

----------------


extraEnv: |
    # Hostname and HTTPS Configuration
    - name: KC_HOSTNAME
      value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    - name: KC_HOSTNAME_URL
      value: "https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    - name: KC_HOSTNAME_ADMIN
      value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    
    # Strict HTTPS Enforcement
    - name: KC_HOSTNAME_STRICT
      value: "false"
    - name: KC_HOSTNAME_STRICT_HTTPS
      value: "true"
    
    # Proxy and Forwarding Configuration
    - name: KC_PROXY
      value: "edge"
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"
    - name: X_FORWARDED_PROTO
      value: "https"
    
    # Enhanced Content Security Policy
    - name: KC_HTTP_RELATIVE_PATH
      value: "/auth"
    - name: KC_DEFAULT_HTTP_HEADERS
      value: |
        {
          "Content-Security-Policy": "default-src 'self' https:; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' https:; frame-src 'self' https:",
          "X-Frame-Options": "SAMEORIGIN",
          "Strict-Transport-Security": "max-age=31536000; includeSubDomains"
        }
    
    # Logging and Monitoring
    - name: KC_LOG_LEVEL
      value: "INFO"
    - name: KC_METRICS_ENABLED
      value: "true"
    - name: KC_HEALTH_ENABLED
      value: "true"


----------
keycloak:
  extraEnv: |
    # ✅ Set the correct ALB hostname
    - name: KC_HOSTNAME
      value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    - name: KC_HOSTNAME_URL
      value: "https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"
    - name: KC_HOSTNAME_STRICT
      value: "false"  # Allow Keycloak to be accessed via different hostnames
    - name: KC_HOSTNAME_STRICT_HTTPS
      value: "true"   # Ensure HTTPS is always used
    - name: KC_HOSTNAME_ADMIN
      value: "test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"

    # ✅ Ensure Keycloak understands it's behind an HTTPS proxy
    - name: KC_PROXY
      value: "edge"
    - name: X_FORWARDED_PROTO
      value: "https"
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"

    # ✅ Enable HTTP so NLB can reach Keycloak
    - name: KC_HTTP_ENABLED
      value: "true"

    # ✅ Content Security Policy Fix
    - name: KC_DEFAULT_HTTP_HEADERS
      value: '{"Content-Security-Policy": "frame-ancestors self https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com", "X-Frame-Options": "ALLOW-FROM https://test-keycloak-ext-app-lb-1296767681.us-east-1.elb.amazonaws.com"}'

    # ✅ Enable additional logs for debugging
    - name: KC_LOG_LEVEL
      value: "DEBUG"
    - name: KC_METRICS_ENABLED
      value: "true"
    - name: KC_HEALTH_ENABLED
      value: "true"



------------
Possible Causes and Debugging Steps

1. Check if the ALB Exists
Run the following command to verify if the ALB is properly created in AWS:

aws elbv2 describe-load-balancers --region us-east-1 --query "LoadBalancers[?DNSName=='test-keycloak-ext-app-lb-1446264669.us-east-1.elb.amazonaws.com']"
If the output is empty, the ALB does not exist.
If the output contains details, then the ALB is available.
➡ Fix: If the ALB does not exist, double-check your AWS configuration.

2. Verify the ALB's DNS Name
Run:

nslookup test-keycloak-ext-app-lb-1446264669.us-east-1.elb.amazonaws.com
If it fails, then AWS Route 53 or your ALB setup might be incorrect.
If it resolves to an IP, then the issue is likely with your local network or firewall.
➡ Fix: If DNS does not resolve, try replacing the ALB (delete and recreate it).

3. Check ALB’s Scheme
Your ALB must be internet-facing. Run:

aws elbv2 describe-load-balancers --region us-east-1 --query "LoadBalancers[?DNSName=='test-keycloak-ext-app-lb-1446264669.us-east-1.elb.amazonaws.com'].Scheme"
Expected output:

"internet-facing"
If you see "internal", then your ALB is private and not reachable from the internet.
➡ Fix: You must recreate the ALB as an internet-facing ALB.

4. Verify ALB Security Group
Your ALB’s security group must allow inbound traffic on port 443 (HTTPS).

Run:

aws ec2 describe-security-groups --region us-east-1 --group-ids <ALB_SECURITY_GROUP_ID>
Look for:

"Ingress": [
    {
        "IpProtocol": "tcp",
        "FromPort": 443,
        "ToPort": 443,
        "IpRanges": [{"CidrIp": "0.0.0.0/0"}]
    }
]
➡ Fix: If port 443 is not open to the internet, update the security group.

5. Check If ALB Has a Listener on Port 443
Run:

aws elbv2 describe-listeners --load-balancer-arn <ALB_ARN>
You should see:

"Protocol": "HTTPS",
"Port": 443
➡ Fix: If no listener exists for port 443, create one:

aws elbv2 create-listener --load-balancer-arn <ALB_ARN> \
  --protocol HTTPS --port 443 \
  --certificates CertificateArn=<CERTIFICATE_ARN> \
  --default-actions Type=forward,TargetGroupArn=<TARGET_GROUP_ARN>
6. Test from a Different Network
Try running curl from a different internet connection (e.g., mobile hotspot).
Some corporate networks block AWS ELB DNS resolution.
➡ Fix: If it works on a different network, your ISP or firewall is blocking access.

Final Summary

Check if the ALB exists: aws elbv2 describe-load-balancers
Verify DNS resolution: nslookup test-keycloak-ext-app-lb-1446264669.us-east-1.elb.amazonaws.com
Ensure ALB is internet-facing
Check ALB security group (allow inbound on port 443)
Ensure ALB has a listener on port 443
Try from a different network
Once the ALB resolves correctly, your curl request should work. 🚀




---------
Detailed Step-by-Step Guide:

Step 1: Create Service Account and Token in EKS
Create a Service Account:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: harness-delegate-sa
  namespace: kube-system
Save this as sa-harness-delegate.yaml and apply:

kubectl apply -f sa-harness-delegate.yaml
Create a ClusterRole with Necessary Permissions:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: harness-delegate-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints", "persistentvolumeclaims", "secrets", "configmaps"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["rolebindings", "roles"]
  verbs: ["create", "update", "patch", "delete"]
Save this as clusterrole-harness-delegate.yaml and apply:

kubectl apply -f clusterrole-harness-delegate.yaml
Bind the ClusterRole to the Service Account:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: harness-delegate-binding
subjects:
- kind: ServiceAccount
  name: harness-delegate-sa
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: harness-delegate-role
  apiGroup: rbac.authorization.k8s.io
Save this as rolebinding-harness-delegate.yaml and apply:

kubectl apply -f rolebinding-harness-delegate.yaml
Step 2: Generate the Service Account Token
Since you are using Kubernetes version 1.24 or later, the token isn't automatically generated. You need to manually create a bound token.

Create the token:
kubectl create token harness-delegate-sa --namespace kube-system
Alternatively, using a manifest:
apiVersion: v1
kind: Secret
type: kubernetes.io/service-account-token
metadata:
  name: harness-delegate-token
  annotations:
    kubernetes.io/service-account.name: "harness-delegate-sa"
  namespace: kube-system
Save this as secret-harness-delegate.yaml and apply:

kubectl apply -f secret-harness-delegate.yaml
Retrieve the token:
kubectl get secret harness-delegate-token -n kube-system -o jsonpath='{.data.token}' | base64 --decode
Step 3: Encode the Token for Harness
Encode the token to base64:
echo "<TOKEN_VALUE>" | base64
Copy the output. This will be used as a secret inside Harness.
Step 4: Configure Harness Encrypted Secret
Go to Harness UI:
Navigate to Account Settings → Secrets Management → Secrets.
Create a New Secret:
Choose Encrypted Text.
Name it, e.g., k8s-sa-token.
Paste the base64 encoded token from the previous step.
Save the secret.
Step 5: Create Kubernetes Connector in Harness
Navigate to Harness UI:
Go to Setup → Connectors → New Connector → Kubernetes.
Configure the Kubernetes Connector:
Cluster Details:
Cluster URL: https://<PRIVATE_API_ENDPOINT>
Example: https://eks-vpc-endpoint.us-west-2.eks.amazonaws.com
Authentication Method:
Select Service Account Token.
Choose the secret you created (k8s-sa-token).
Network Settings:
No Proxy needed since it's within the same VPC.
Set Delegate Selection:
Select the Delegate running inside the ECS cluster in the same VPC.
This ensures that the network communication is private and secure.
Test and Save:
Click on Test Connection.
If successful, save the connector.
Step 6: Test the Setup
Deploy a test application or execute a Kubernetes command using the Harness pipeline.
Validate that the Harness Delegate can:
Create a deployment.
Update or Patch resources.
List existing resources.
Delete a deployment or resource.
You can check the permissions by running:
kubectl auth can-i list pods --as=system:serviceaccount:kube-system:harness-delegate-sa





----------------------
SELECT grantee, table_catalog, table_schema, table_name, privilege_type
FROM information_schema.role_table_grants
WHERE grantee = '<DB_USERNAME>';


SHOW default_transaction_read_only;
SELECT * FROM pg_roles WHERE rolname = '<DB_USERNAME>';

ALTER DATABASE <DB_NAME> SET default_transaction_read_only = off;


--------
CREATE TABLE test_table (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


---------
kubectl create secret generic keycloak-db-secret \
  --namespace keycloak \
  --from-literal=username=<DB_USERNAME> \
  --from-literal=password=<DB_PASSWORD>


-----------------------

database:
  # Don't create a secret for the DB password, use the existing K8s secret
  existingSecret: "keycloak-db-secret"
  existingSecretKey: "password"
  # Database vendor for Keycloak
  vendor: postgres
  # AWS RDS Endpoint (replace with your actual endpoint)
  hostname: <RDS_ENDPOINT>
  # Port for PostgreSQL (default is 5432)
  port: 5432
  # Name of the database in RDS
  database: <DB_NAME>
  # Database username (this is securely pulled from the secret)
  username: <DB_USERNAME>
  # Password is pulled from the existing K8s secret
  password: ""

# Enable Proxy Address Forwarding for correct URL redirects
keycloak:
  extraEnv: |
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"
    - name: KC_DB
      value: "postgres"
    - name: KC_DB_URL_HOST
      value: "<RDS_ENDPOINT>"
    - name: KC_DB_URL_PORT
      value: "5432"
    - name: KC_DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: keycloak-db-secret
          key: username
    - name: KC_DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: keycloak-db-secret
          key: password
    - name: KC_DB_URL_DATABASE
      value: "<DB_NAME>"
    - name: KC_DB_SCHEMA
      value: "public"

------


helm upgrade keycloak codecentric-internal/keycloakx \
  --namespace keycloak \
  -f keycloak-values.yaml


--------



----------
# Explicitly set the relative path and URL to include a trailing slash
http:
  relativePath: "/auth/"
  internalPort: http
  internalScheme: HTTP

keycloak:
  hostname: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
  hostnameStrict: false
  hostnameStrictBackchannel: false
  extraEnv: |
    - name: KEYCLOAK_FRONTEND_URL
      value: http://adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com/auth/
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"




--------
replicaCount: 1

# Override the container startup command to run Keycloak with custom options.
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
  - "--http-port=8080"
  - "--hostname-strict=false"

# Set extra environment variables.
extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless
  - name: KEYCLOAK_FRONTEND_URL
    value: http://adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
  - name: PROXY_ADDRESS_FORWARDING
    value: "true"

# Keycloak-specific configuration.
keycloak:
  hostname: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
  hostnameStrict: false
  hostnameStrictBackchannel: false

# Expose the Keycloak service on port 80 while targeting container port 8080.
service:
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  ingressClassName: nginx  # Use the new field instead of deprecated annotation
  servicePort: http
  rules:
    - host: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []  # No TLS for now
  annotations: {}

# Override fullname to ensure Helm regenerates the Ingress correctly.
fullnameOverride: keycloak
nameOverride: keycloak

resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

# Persistence configuration. For production, enable persistence and adjust storageClass/size.
persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi

# External database configuration.
postgresql:
  enabled: false




---------------
replicaCount: 1

# Override the container startup command to run Keycloak with custom options.
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
  - "--http-port=8080"
  - "--hostname-strict=false"

# Set extra environment variables.
extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless
  - name: KEYCLOAK_FRONTEND_URL
    value: http://adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com

# Keycloak-specific configuration.
keycloak:
  username: admin
  password: admin
  hostname: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
  hostnameStrict: false
  hostnameStrictBackchannel: false

# Expose the Keycloak service on port 80 while targeting container port 8080.
service:
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  ingressClassName: nginx  # Use the new field instead of deprecated annotation
  hosts:
    - host: adebf18ca644b40fb80c46e542a0c6c5-74a543961b5155cb.elb.us-east-1.amazonaws.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []  # No TLS for now
  annotations: {}

# Override fullname to ensure Helm regenerates the Ingress correctly.
fullnameOverride: keycloak

resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

# Persistence configuration. For production, enable persistence and adjust storageClass/size.
persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi

# External database configuration.
postgresql:
  enabled: false




-----------
replicaCount: 1

# Override the container startup command to run Keycloak with custom options.
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
  - "--http-port=8080"
  - "--hostname-strict=false"

# Set extra environment variables.
extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless

# Keycloak-specific configuration.
keycloak:
  username: admin
  password: admin

# Expose the Keycloak service on port 80 while targeting container port 8080.
service:
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  # Use your NLB DNS name here.
  hosts:
    - host: your-nlb-dns-name.aws-region.elb.amazonaws.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  annotations:
    kubernetes.io/ingress.class: "nginx"
  tls: []  # For now, TLS is not configured for testing.

resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi

postgresql:
  enabled: false






--------------------
--------------------
replicaCount: 1

# Override the container startup command to run Keycloak with custom options.
command:
  - "/opt/keycloak/bin/kc.sh"
  - "start"
  - "--http-port=8080"
  - "--hostname-strict=false"

# Set extra environment variables.
extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: admin
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: admin
  - name: JAVA_OPTS_APPEND
    value: >-
      -Djgroups.dns.query={{ include "keycloak.fullname" . }}-headless

# Keycloak-specific configuration.
keycloak:
  # Although we are also setting admin credentials via extraEnv,
  # this block may be used internally (for secret creation or other logic).
  username: admin
  password: admin

ingress:
  enabled: true
  # Replace the host value with the DNS name of your AWS NLB or custom domain.
  hosts:
    - host: your-nlb-dns-name.aws-region.elb.amazonaws.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  annotations:
    # This tells ingress-nginx to handle the routing.
    kubernetes.io/ingress.class: "nginx"
  tls: []  # For production, configure TLS as needed.

resources:
  limits:
    cpu: "500m"
    memory: "1024Mi"
  requests:
    cpu: "250m"
    memory: "512Mi"

# Persistence configuration. For production, enable persistence and adjust storageClass/size.
persistence:
  enabled: false
  storageClass: "gp2"
  size: 8Gi

# External database configuration.
postgresql:
  enabled: false



---------
stage('Reindex Artifactory Helm Repo') {
    container('docker') {
        withCredentials([
            usernamePassword(
                credentialsId: 'jfrog-service-account',
                usernameVariable: 'ARTIFACTORY_USER',
                passwordVariable: 'ARTIFACTORY_PASSWORD'
            )
        ]) {
            script {
                // POST to the reindex endpoint for your Helm repo
                def reindexCmd = """
                    curl -X POST \\
                         -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} \\
                         "${env.ARTIFACTORY_URL}/artifactory/api/helm/${env.ARTIFACTORY_REPO}/reindex"
                """
                echo "Triggering reindex with:\n${reindexCmd}"
                def exitCode = sh(script: reindexCmd, returnStatus: true)
                
                if (exitCode == 0) {
                    echo "SUCCESS: Reindex triggered. Artifactory should update index.yaml."
                } else {
                    error "FAILURE: Reindex call failed with exit code ${exitCode}"
                }
            }
        }
    }
}




-----------
stage('Upload to Artifactory') {
    container('docker') {
        withCredentials([
            usernamePassword(
                credentialsId: 'jfrog-service-account',
                usernameVariable: 'ARTIFACTORY_USER',
                passwordVariable: 'ARTIFACTORY_PASSWORD'
            )
        ]) {
            script {
                // Identify the packaged chart file (assuming only one .tgz file exists)
                def chartFile = sh(script: "ls *.tgz", returnStdout: true).trim()
                echo "Uploading Helm chart: ${chartFile}"
                
                // Build the curl command (using -v for verbose logs)
                def curlCmd = """
                    curl -v -X PUT -H 'Content-Type: application/tar+gzip' \\
                         -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} \\
                         -T ${chartFile} \\
                         "${env.ARTIFACTORY_URL}/artifactory/${env.ARTIFACTORY_REPO}/${chartFile}"
                """
                echo "Running curl command:\n${curlCmd}"
                
                // Execute the curl command and capture the exit code
                def exitCode = sh(script: curlCmd, returnStatus: true)
                
                // Check the exit code to determine success/failure
                if (exitCode == 0) {
                    echo "SUCCESS: The Helm chart was successfully uploaded to Artifactory!"
                } else {
                    error "FAILURE: Upload to Artifactory failed with exit code ${exitCode}"
                }
            }
        }
    }
}



-------
@Library('framework-shared-libs@v1.0.0') _

// Set up pipeline parameters (similar to "parameters { ... }" in declarative)
properties([
    parameters([
        string(name: 'SOURCE_HELM_REPO_URL',    defaultValue: 'https://codecentric.github.io/helm-charts', description: 'Source Helm repository URL'),
        string(name: 'SOURCE_HELM_REPO_NAME',   defaultValue: 'codecentric',                               description: 'Name of the source Helm repository'),
        string(name: 'SOURCE_HELM_CHART_NAME',  defaultValue: 'keycloakx',                                description: 'Source Helm chart name'),
        string(name: 'SOURCE_HELM_CHART_VERSION', defaultValue: '',                                       description: 'Source Helm chart version (leave empty for latest)')
    ])
])

// Define a podTemplate that references your 'cicd.yaml'
podTemplate(
    yamlFile: 'cicd.yaml', 
    label: "docker-build-${env.BUILD_NUMBER}"
) {
    node("docker-build-${env.BUILD_NUMBER}") {
        // Set environment variables inside the node
        env.ARTIFACTORY_URL   = 'https://your-artifactory.example.com'
        env.ARTIFACTORY_REPO  = 'helm-local'
        env.INTERNAL_VERSION  = '1.0.0-internal'
        // Example of other environment variables you might use
        env.DOCKER_REGISTRY   = "docker-registry.default.svc:5000"
        // etc.

        stage('Checkout') {
            checkout scm
        }

        stage('Pull Helm Chart') {
            // Use the container name from your cicd.yaml that has Helm installed
            container('docker') {
                script {
                    // Add the source Helm repository and update
                    sh "helm repo add ${params.SOURCE_HELM_REPO_NAME} ${params.SOURCE_HELM_REPO_URL}"
                    sh "helm repo update"

                    // Build the helm pull command
                    def pullCmd = "helm pull ${params.SOURCE_HELM_REPO_NAME}/${params.SOURCE_HELM_CHART_NAME} --untar"
                    if (params.SOURCE_HELM_CHART_VERSION?.trim()) {
                        pullCmd += " --version ${params.SOURCE_HELM_CHART_VERSION}"
                    }
                    sh pullCmd
                }
            }
        }

        stage('Update Internal Version') {
            container('docker') {
                script {
                    // Update the version in Chart.yaml to your internal version
                    sh """
                        sed -i 's/^version:.*/version: ${env.INTERNAL_VERSION}/' ${params.SOURCE_HELM_CHART_NAME}/Chart.yaml
                    """
                }
            }
        }

        stage('Package Helm Chart') {
            container('docker') {
                script {
                    // Package the chart into a .tgz file
                    sh "helm package ${params.SOURCE_HELM_CHART_NAME}"
                }
            }
        }

        stage('Upload to Artifactory') {
            container('docker') {
                // Use Jenkins credentials for the Artifactory service account
                withCredentials([
                    usernamePassword(
                        credentialsId: 'jfrog-service-account',
                        usernameVariable: 'ARTIFACTORY_USER',
                        passwordVariable: 'ARTIFACTORY_PASSWORD'
                    )
                ]) {
                    script {
                        // Identify the packaged chart file (assuming only one .tgz)
                        def chartFile = sh(script: "ls *.tgz", returnStdout: true).trim()

                        // Upload via curl with basic auth
                        sh """
                            curl -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} -T ${chartFile} \\
                            "${env.ARTIFACTORY_URL}/artifactory/${env.ARTIFACTORY_REPO}/${chartFile}"
                        """
                    }
                }
            }
        }

        // EXAMPLE: If you have Docker build/push steps, add them as additional stages
        /*
        stage('Build Docker Image') {
            container('docker') {
                sh "docker build -t ${DOCKER_REGISTRY}/myimage:${BUILD_NUMBER} ."
            }
        }

        stage('Push Docker Image') {
            container('docker') {
                sh "docker push ${DOCKER_REGISTRY}/myimage:${BUILD_NUMBER}"
            }
        }
        */
    }
}







---------------
pipeline {
    agent any

    parameters {
        string(name: 'SOURCE_HELM_REPO_URL', defaultValue: 'https://codecentric.github.io/helm-charts', description: 'Source Helm repository URL')
        string(name: 'SOURCE_HELM_REPO_NAME', defaultValue: 'codecentric', description: 'Name of the source Helm repository')
        string(name: 'SOURCE_HELM_CHART_NAME', defaultValue: 'keycloakx', description: 'Source Helm chart name')
        string(name: 'SOURCE_HELM_CHART_VERSION', defaultValue: '', description: 'Source Helm chart version (leave empty for latest)')
    }

    environment {
        // Artifactory configuration (adjust these values for your environment)
        ARTIFACTORY_URL = 'https://your-artifactory.example.com'
        ARTIFACTORY_REPO = 'helm-local'
        // Internal version to be applied to the chart
        INTERNAL_VERSION = '1.0.0-internal'
    }

    stages {
        stage('Pull Helm Chart') {
            steps {
                script {
                    // Add the source Helm repository using parameters
                    sh "helm repo add ${params.SOURCE_HELM_REPO_NAME} ${params.SOURCE_HELM_REPO_URL}"
                    sh "helm repo update"
                    // Construct the helm pull command; include --version if provided
                    def pullCmd = "helm pull ${params.SOURCE_HELM_REPO_NAME}/${params.SOURCE_HELM_CHART_NAME} --untar"
                    if (params.SOURCE_HELM_CHART_VERSION?.trim()) {
                        pullCmd += " --version ${params.SOURCE_HELM_CHART_VERSION}"
                    }
                    sh pullCmd
                }
            }
        }
        stage('Update Internal Version') {
            steps {
                script {
                    // Update the version in Chart.yaml to use the internal version
                    // This example uses GNU sed; adjust for macOS if needed.
                    sh "sed -i 's/^version:.*/version: ${INTERNAL_VERSION}/' ${params.SOURCE_HELM_CHART_NAME}/Chart.yaml"
                }
            }
        }
        stage('Package Helm Chart') {
            steps {
                script {
                    // Package the chart, which will create a .tgz file (e.g., keycloakx-1.0.0-internal.tgz)
                    sh "helm package ${params.SOURCE_HELM_CHART_NAME}"
                }
            }
        }
        stage('Upload to Artifactory') {
            steps {
                // Use the Artifactory service account credentials stored in Jenkins (ID: jfrog-service-account)
                withCredentials([usernamePassword(credentialsId: 'jfrog-service-account', 
                                                  usernameVariable: 'ARTIFACTORY_USER', 
                                                  passwordVariable: 'ARTIFACTORY_PASSWORD')]) {
                    script {
                        // Identify the packaged chart file (assuming only one .tgz file exists)
                        def chartFile = sh(script: "ls *.tgz", returnStdout: true).trim()
                        // Use curl with basic authentication to upload the chart to Artifactory
                        sh """
                            curl -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} -T ${chartFile} \
                                 "${ARTIFACTORY_URL}/artifactory/${ARTIFACTORY_REPO}/${chartFile}"
                        """
                    }
                }
            }
        }
    }
}




-----------------------
pipeline {
  agent { label 'helm-enabled' } // Use a specific agent label with Git, Helm, and curl installed.
  
  parameters {
    string(name: 'ARTIFACTORY_URL', defaultValue: 'https://your-artifactory-domain/artifactory/helm', description: 'Artifactory Helm Repository URL')
    string(name: 'CHART_REPO', defaultValue: 'https://github.com/bitnami/charts.git', description: 'Git repository containing the helm charts')
    string(name: 'CHART_DIR', defaultValue: 'bitnami/keycloak', description: 'Relative path to the Keycloak chart within the repository')
  }
  
  environment {
    // Credentials can be reused across stages.
    ARTIFACTORY_CREDENTIALS = credentials('artifactory-credentials')
  }
  
  stages {
    stage('Test Artifactory Connectivity') {
      steps {
        script {
          // Test connectivity using curl and validate response code.
          sh '''
            RESPONSE=$(curl -u ${ARTIFACTORY_CREDENTIALS_USR}:${ARTIFACTORY_CREDENTIALS_PSW} -s -o /dev/null -w "%{http_code}" ${ARTIFACTORY_URL}/index.yaml)
            if [ "$RESPONSE" -ne 200 ]; then
              echo "Failed to connect to Artifactory. HTTP response code: $RESPONSE"
              exit 1
            fi
          '''
        }
      }
    }
    
    stage('Clone Helm Chart Repository') {
      steps {
        // Clone the repository. Using a shallow clone to speed up the process.
        sh "git clone --depth 1 ${params.CHART_REPO}"
      }
    }
    
    stage('Package Helm Chart') {
      steps {
        dir(params.CHART_DIR) {
          // Package the chart. This creates a .tgz file in the current directory.
          sh "helm package ."
        }
      }
    }
    
    stage('Push Helm Chart to Artifactory') {
      steps {
        script {
          withCredentials([usernamePassword(credentialsId: 'artifactory-credentials', 
                                               usernameVariable: 'ARTIFACTORY_USER', 
                                               passwordVariable: 'ARTIFACTORY_PASSWORD')]) {
            // Find the packaged chart file (assuming only one .tgz exists)
            def chartFile = sh(script: "ls ${params.CHART_DIR}/*.tgz", returnStdout: true).trim()
            echo "Chart package file: ${chartFile}"
            
            // Upload the chart using curl.
            sh """
              curl -u $ARTIFACTORY_USER:$ARTIFACTORY_PASSWORD -T ${chartFile} "${params.ARTIFACTORY_URL}/${chartFile.tokenize('/')[-1]}"
            """
            
            // Archive the chart artifact for traceability.
            archiveArtifacts artifacts: "${params.CHART_DIR}/*.tgz", allowEmptyArchive: false
          }
        }
      }
    }
  }
  
  post {
    success {
      echo 'Helm chart successfully pushed to Artifactory!'
    }
    failure {
      echo 'Pipeline failed. Please check the logs for details.'
    }
    cleanup {
      cleanWs()
    }
  }
}
