env:
    GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
    GF_AUTH_GENERIC_OAUTH_NAME: "Azure AD"
    GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "your-azure-client-id"
    GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: "your-azure-secret"
    GF_AUTH_GENERIC_OAUTH_SCOPES: "openid email profile"
    GF_AUTH_GENERIC_OAUTH_AUTH_URL: "https://login.microsoftonline.com/your-tenant-id/oauth2/v2.0/authorize"
    GF_AUTH_GENERIC_OAUTH_TOKEN_URL: "https://login.microsoftonline.com/your-tenant-id/oauth2/v2.0/token"
    GF_AUTH_GENERIC_OAUTH_API_URL: "https://graph.microsoft.com/oidc/userinfo"
    GF_AUTH_SIGNOUT_REDIRECT_URL: "https://login.microsoftonline.com/common/oauth2/v2.0/logout"
    GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
    GF_USERS_ALLOW_SIGN_UP: "false"
    GF_AUTH_DISABLE_LOGIN_FORM: "true"
    GF_SERVER_ROOT_URL: "https://dev-ethos-idp.monitor.fiscloudservices.com/grafana/"
    GF_SERVER_SERVE_FROM_SUB_PATH: "true"

---------------------
message: |
  {{- if eq .CommonLabels.alertname "KeycloakDown" -}}
  [Critical] Keycloak is DOWN in {{ .Labels.namespace }}. No instance detected for 5m. Instance: {{ .Labels.pod }}
  {{- else if eq .CommonLabels.alertname "KeycloakHigh5xxRate" -}}
  [Warning] High 5xx error rate for Keycloak in {{ .Labels.namespace }}: > {{ .Value }} errors/min. Instance: {{ .Labels.pod }}
  {{- else if eq .CommonLabels.alertname "KeycloakHighResponseLatency" -}}
  [Warning] High response latency for Keycloak in {{ .Labels.namespace }}: 99th percentile > {{ .Value }}s. Instance: {{ .Labels.pod }}
  {{- else if eq .CommonLabels.alertname "KeycloakHighLoginFailures" -}}
  [Warning] Excessive login failures for Keycloak in {{ .Labels.namespace }}: > {{ .Value }}/min. Instance: {{ .Labels.pod }}
  {{- else if eq .CommonLabels.alertname "KeycloakJvmHighMemory" -}}
  [Warning] JVM heap usage > 80% for Keycloak in {{ .Labels.namespace }}. Instance: {{ .Labels.pod }}
  {{- else if eq .CommonLabels.alertname "KeycloakPodRestartingFrequently" -}}
  [Warning] Keycloak pod restarted > 3 times in 10m in {{ .Labels.namespace }}. Pod: {{ .Labels.pod }}
  {{- else if eq .CommonLabels.alertname "KeycloakJavaNonHeapThresholdExceeded" -}}
  [Warning] JVM nonheap usage > 90% for Keycloak in {{ .Labels.namespace }}. Instance: {{ .Labels.pod }}
  {{- else -}}
  [Prometheus] Alert: {{ .CommonLabels.alertname }} ({{ .CommonLabels.severity }})
  Summary: {{ with .Annotations.summary }}{{ . }}{{ else }}No summary provided{{ end }}
  Description: {{ with .Annotations.description }}{{ . }}{{ else }}No description provided{{ end }}
  {{- end -}}


-------------------
# ------------------------------
# Variables (define these as needed)
# ------------------------------
variable "cluster_name" {
  type        = string
  description = "EKS Cluster name"
}
variable "alertmanager_namespace" {
  type        = string
  default     = "kube-prometheus-stack"
}
variable "alertmanager_serviceaccount" {
  type        = string
  default     = "alertmanager"
}
variable "sns_topic_arn" {
  type        = string
  description = "SNS Topic ARN to publish alerts"
}

# ------------------------------
# Get EKS cluster OIDC provider
# ------------------------------
data "aws_eks_cluster" "this" {
  name = var.cluster_name
}

data "aws_iam_openid_connect_provider" "this" {
  url = replace(data.aws_eks_cluster.this.identity[0].oidc[0].issuer, "https://", "")
}

# ------------------------------
# IAM Role for IRSA
# ------------------------------
resource "aws_iam_role" "alertmanager_irsa" {
  name = "eks-irsa-alertmanager"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = {
        Federated = data.aws_iam_openid_connect_provider.this.arn
      },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          # NOTE: The SA must match the one created by the Helm chart!
          "${replace(data.aws_eks_cluster.this.identity[0].oidc[0].issuer, "https://", "")}:sub" = "system:serviceaccount:${var.alertmanager_namespace}:${var.alertmanager_serviceaccount}"
        }
      }
    }]
  })
}

# ------------------------------
# IAM Policy to allow SNS:Publish
# ------------------------------
resource "aws_iam_policy" "alertmanager_sns_publish" {
  name        = "alertmanager-sns-publish"
  description = "Allow Alertmanager to publish to SNS topic"
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Action = [
        "sns:Publish"
      ],
      Resource = var.sns_topic_arn
    }]
  })
}

# ------------------------------
# Attach Policy to Role
# ------------------------------
resource "aws_iam_role_policy_attachment" "attach_alertmanager_policy" {
  role       = aws_iam_role.alertmanager_irsa.name
  policy_arn = aws_iam_policy.alertmanager_sns_publish.arn
}

# ------------------------------
# Outputs for use in Helm values.yaml
# ------------------------------
output "alertmanager_irsa_role_arn" {
  value = aws_iam_role.alertmanager_irsa.arn
  description = "IAM Role ARN for Alertmanager IRSA"
}


---

alertmanager:
  serviceAccount:
    create: true
    name: alertmanager  # <--- must match Terraform var.alertmanager_serviceaccount
    annotations:
      eks.amazonaws.com/role-arn: <output_from_terraform_alertmanager_irsa_role_arn>



--------------
alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m

    receivers:
      - name: 'sns-email'
        sns_configs:
          - sigv4:
              region: eu-west-1
            topic_arn: arn:aws:sns:eu-west-1:123456789012:prometheus-alerts
            subject: '{{`[Prometheus Alert] {{ .CommonLabels.alertname }}: {{ .CommonLabels.severity }}`}}'
            message: |
              {{`{{- if eq .CommonLabels.alertname "KeycloakDown" -}}
[Critical] Keycloak is DOWN in {{ .Labels.namespace }}. No instance detected for 5m. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakHigh5xxRate" -}}
[Warning] High 5xx error rate for Keycloak in {{ .Labels.namespace }}: > {{ .Value }} errors/min. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakHighResponseLatency" -}}
[Warning] High response latency for Keycloak in {{ .Labels.namespace }}: 99th percentile > {{ .Value }}s. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakHighLoginFailures" -}}
[Warning] Excessive login failures for Keycloak in {{ .Labels.namespace }}: > {{ .Value }}/min. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakJvmHighMemory" -}}
[Warning] JVM heap usage > 80% for Keycloak in {{ .Labels.namespace }}. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakPodRestartingFrequently" -}}
[Warning] Keycloak pod restarted > 3 times in 10m in {{ .Labels.namespace }}. Pod: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakJavaNonHeapThresholdExceeded" -}}
[Warning] JVM nonheap usage > 90% for Keycloak in {{ .Labels.namespace }}. Instance: {{ .Labels.pod }}
{{- else -}}
[Prometheus] Alert: {{ .CommonLabels.alertname }} ({{ .CommonLabels.severity }})\nSummary: {{ .Annotations.summary }}\nDescription: {{ .Annotations.description }}
{{- end -}}`}}
    route:
      receiver: 'sns-email'
      group_by: ['alertname']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 1h
      routes:
        - match:
            severity: 'critical'
          receiver: 'sns-email'
        - match:
            severity: 'warning'
          receiver: 'sns-email'



-------------
apiVersion: v1
kind: Secret
alertmanager:
  sns:
    region: eu-west-1
    topicArn: arn:aws:sns:eu-west-1:123456789012:prometheus-alerts


--------
metadata:
  name: kube-prometheus-stack-alertmanager
  namespace: {{ .Values.prometheusRules.namespace | default "kube-prometheus-stack" }}
  labels:
    app: kube-prometheus-stack-alertmanager
    {{- include "kube-prometheus-stack.labels" . | nindent 4 }}
type: Opaque
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m

    receivers:
      - name: 'sns-email'
        sns_configs:
          - sigv4:
              region: {{ .Values.alertmanager.sns.region | quote }}
            topic_arn: {{ .Values.alertmanager.sns.topicArn | quote }}
            subject: '{{`[Prometheus Alert] {{ .CommonLabels.alertname }}: {{ .CommonLabels.severity }}`}}'
            message: |
              {{`{{- if eq .CommonLabels.alertname "KeycloakDown" -}}
[Critical] Keycloak is DOWN in {{ .Labels.namespace }}. No instance detected for 5m. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakHigh5xxRate" -}}
[Warning] High 5xx error rate for Keycloak in {{ .Labels.namespace }}: > {{ .Value }} errors/min. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakHighResponseLatency" -}}
[Warning] High response latency for Keycloak in {{ .Labels.namespace }}: 99th percentile > {{ .Value }}s. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakHighLoginFailures" -}}
[Warning] Excessive login failures for Keycloak in {{ .Labels.namespace }}: > {{ .Value }}/min. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakJvmHighMemory" -}}
[Warning] JVM heap usage > 80% for Keycloak in {{ .Labels.namespace }}. Instance: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakPodRestartingFrequently" -}}
[Warning] Keycloak pod restarted > 3 times in 10m in {{ .Labels.namespace }}. Pod: {{ .Labels.pod }}
{{- else if eq .CommonLabels.alertname "KeycloakJavaNonHeapThresholdExceeded" -}}
[Warning] JVM nonheap usage > 90% for Keycloak in {{ .Labels.namespace }}. Instance: {{ .Labels.pod }}
{{- else -}}
[Prometheus] Alert: {{ .CommonLabels.alertname }} ({{ .CommonLabels.severity }})\nSummary: {{ .Annotations.summary }}\nDescription: {{ .Annotations.description }}
{{- end -}}`}}
    route:
      receiver: 'sns-email'
      group_by: ['alertname']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 1h
      routes:
        - match:
            severity: 'critical'
          receiver: 'sns-email'
        - match:
            severity: 'warning'
          receiver: 'sns-email'




----------------
{{- if .Values.prometheusRules.create | default true }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: keycloak-prometheus-alerts
  namespace: {{ .Values.prometheusRules.namespace | default "kube-prometheus-stack" }}
  labels:
    {{- include "kube-prometheus-stack.labels" . | nindent 4 }}
spec:
  groups:
    - name: keycloak.rules
      rules:

        {{- if .Values.keycloakAlerts.keycloakDown.enabled }}
        - alert: KeycloakDown
          expr: absent(up{job="keycloak", namespace="keycloak-eda"} == 1)
          for: 5m
          labels:
            severity: "{{ .Values.keycloakAlerts.keycloakDown.severity }}"
          annotations:
            summary: "Keycloak service is down"
            description: "No running Keycloak instance detected in namespace keycloak-eda for more than 5 minutes."
        {{- end }}

        {{- if .Values.keycloakAlerts.high5xxRate.enabled }}
        - alert: KeycloakHigh5xxRate
          expr: sum(rate(http_server_requests_seconds_count{job="keycloak", namespace="keycloak-eda", status=~"5.."}[5m])) > {{ .Values.keycloakAlerts.high5xxRate.threshold | default 5 }}
          for: 2m
          labels:
            severity: "{{ .Values.keycloakAlerts.high5xxRate.severity }}"
          annotations:
            summary: "High 5xx error rate"
            description: "Keycloak in keycloak-eda is returning 5xx errors at a high rate."
        {{- end }}

        {{- if .Values.keycloakAlerts.highResponseLatency.enabled }}
        - alert: KeycloakHighResponseLatency
          expr: histogram_quantile({{ .Values.keycloakAlerts.highResponseLatency.quantile | default 0.99 }}, sum(rate(http_server_requests_seconds_bucket{job="keycloak", namespace="keycloak-eda"}[5m])) by (le)) > {{ .Values.keycloakAlerts.highResponseLatency.latencySeconds | default 2 }}
          for: 5m
          labels:
            severity: "{{ .Values.keycloakAlerts.highResponseLatency.severity }}"
          annotations:
            summary: "Keycloak high response latency"
            description: "Keycloak {{ .Values.keycloakAlerts.highResponseLatency.quantile | default 0.99 | mul 100 | int }}th percentile response latency > {{ .Values.keycloakAlerts.highResponseLatency.latencySeconds | default 2 }}s for 5m in keycloak-eda."
        {{- end }}

        {{- if .Values.keycloakAlerts.highLoginFailures.enabled }}
        - alert: KeycloakHighLoginFailures
          expr: sum(rate(keycloak_logins_failed_total{namespace="keycloak-eda"}[5m])) > {{ .Values.keycloakAlerts.highLoginFailures.threshold | default 10 }}
          for: 2m
          labels:
            severity: "{{ .Values.keycloakAlerts.highLoginFailures.severity }}"
          annotations:
            summary: "High login failure rate"
            description: "Keycloak login failures > {{ .Values.keycloakAlerts.highLoginFailures.threshold | default 10 }}/min in keycloak-eda."
        {{- end }}

        {{- if .Values.keycloakAlerts.jvmHeapHighMemory.enabled }}
        - alert: KeycloakJvmHighMemory
          expr: |
            (
              (jvm_memory_used_bytes{namespace="keycloak-eda", area="heap"} / ignoring(id) jvm_memory_max_bytes{namespace="keycloak-eda", area="heap"})
            ) > {{ .Values.keycloakAlerts.jvmHeapHighMemory.percent | default 0.8 }}
          for: 10m
          labels:
            severity: "{{ .Values.keycloakAlerts.jvmHeapHighMemory.severity }}"
          annotations:
            summary: "Keycloak JVM high memory usage"
            description: "JVM heap usage > {{ .Values.keycloakAlerts.jvmHeapHighMemory.percent | default 0.8 | mul 100 | int }}% for 10m in keycloak-eda."
        {{- end }}

        {{- if .Values.keycloakAlerts.podRestartingFrequently.enabled }}
        - alert: KeycloakPodRestartingFrequently
          expr: increase(kube_pod_container_status_restarts_total{namespace="keycloak-eda", pod=~"keycloak.*"}[{{ .Values.keycloakAlerts.podRestartingFrequently.window | default "10m" }}]) > {{ .Values.keycloakAlerts.podRestartingFrequently.threshold | default 3 }}
          for: 5m
          labels:
            severity: "{{ .Values.keycloakAlerts.podRestartingFrequently.severity }}"
          annotations:
            summary: "Keycloak pod restart loop"
            description: "Keycloak pod in keycloak-eda restarted more than {{ .Values.keycloakAlerts.podRestartingFrequently.threshold | default 3 }} times in {{ .Values.keycloakAlerts.podRestartingFrequently.window | default "10m" }}."
        {{- end }}

        {{- if .Values.keycloakAlerts.javaNonHeapThresholdExceeded.enabled }}
        - alert: KeycloakJavaNonHeapThresholdExceeded
          expr: |
            100 * jvm_memory_bytes_used{area="nonheap",namespace="keycloak-eda"}
            / jvm_memory_bytes_max{area="nonheap",namespace="keycloak-eda"} > {{ .Values.keycloakAlerts.javaNonHeapThresholdExceeded.percent | default 0.9 }}
          for: 1m
          labels:
            severity: "{{ .Values.keycloakAlerts.javaNonHeapThresholdExceeded.severity }}"
          annotations:
            summary: Keycloak Java NonHeap Threshold Exceeded
            description: "JVM nonheap usage > {{ .Values.keycloakAlerts.javaNonHeapThresholdExceeded.percent | default 0.9 | mul 100 | int }}% in keycloak-eda."
        {{- end }}

{{- end }}



------
prometheusRules:
  create: true
  namespace: kube-prometheus-stack

keycloakAlerts:
  keycloakDown:
    enabled: true
    severity: critical
  high5xxRate:
    enabled: true
    severity: warning
    threshold: 5
  highResponseLatency:
    enabled: true
    severity: warning
    quantile: 0.99
    latencySeconds: 2
  highLoginFailures:
    enabled: true
    severity: warning
    threshold: 10
  jvmHeapHighMemory:
    enabled: true
    severity: warning
    percent: 0.8
  podRestartingFrequently:
    enabled: true
    severity: warning
    threshold: 3
    window: 10m
  javaNonHeapThresholdExceeded:
    enabled: true
    severity: warning
    percent: 0.9







-------------------
{{- if .Values.prometheusRules.create | default true }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: keycloak-prometheus-alerts
  namespace: {{ .Values.prometheusRules.namespace | default "kube-prometheus-stack" }}
  labels:
    {{- include "kube-prometheus-stack.labels" . | nindent 4 }}
spec:
  groups:
    - name: keycloak.rules
      rules:
        # 1. Keycloak Instance Down
        {{- if .Values.keycloakAlerts.keycloakDown.enabled | default true }}
        - alert: KeycloakDown
          expr: absent(up{job="keycloak", namespace="keycloak-eda"} == 1)
          for: 5m
          labels:
            severity: {{ .Values.keycloakAlerts.keycloakDown.severity | default "critical" }}
          annotations:
            summary: "Keycloak service is down"
            description: "No running Keycloak instance detected in namespace keycloak-eda for more than 5 minutes."
        {{- end }}

        # 2. High 5xx Error Rate
        {{- if .Values.keycloakAlerts.high5xxRate.enabled | default true }}
        - alert: KeycloakHigh5xxRate
          expr: sum(rate(http_server_requests_seconds_count{job="keycloak", namespace="keycloak-eda", status=~"5.."}[5m])) > 5
          for: 2m
          labels:
            severity: {{ .Values.keycloakAlerts.high5xxRate.severity | default "warning" }}
          annotations:
            summary: "High 5xx error rate"
            description: "Keycloak in keycloak-eda is returning 5xx errors at a high rate."
        {{- end }}

        # 3. High Response Latency
        {{- if .Values.keycloakAlerts.highResponseLatency.enabled | default true }}
        - alert: KeycloakHighResponseLatency
          expr: histogram_quantile(0.99, sum(rate(http_server_requests_seconds_bucket{job="keycloak", namespace="keycloak-eda"}[5m])) by (le)) > 2
          for: 5m
          labels:
            severity: {{ .Values.keycloakAlerts.highResponseLatency.severity | default "warning" }}
          annotations:
            summary: "Keycloak high response latency"
            description: "Keycloak 99th percentile response latency > 2s for 5m in keycloak-eda."
        {{- end }}

        # 4. High Auth Failure Rate
        {{- if .Values.keycloakAlerts.highLoginFailures.enabled | default true }}
        - alert: KeycloakHighLoginFailures
          expr: sum(rate(keycloak_logins_failed_total{namespace="keycloak-eda"}[5m])) > 10
          for: 2m
          labels:
            severity: {{ .Values.keycloakAlerts.highLoginFailures.severity | default "warning" }}
          annotations:
            summary: "High login failure rate"
            description: "Keycloak login failures > 10/min in keycloak-eda."
        {{- end }}

        # 5. High JVM Heap Memory Usage
        {{- if .Values.keycloakAlerts.jvmHeapHighMemory.enabled | default true }}
        - alert: KeycloakJvmHighMemory
          expr: |
            (
              (jvm_memory_used_bytes{namespace="keycloak-eda", area="heap"} / ignoring(id) jvm_memory_max_bytes{namespace="keycloak-eda", area="heap"})
            ) > 0.8
          for: 10m
          labels:
            severity: {{ .Values.keycloakAlerts.jvmHeapHighMemory.severity | default "warning" }}
          annotations:
            summary: "Keycloak JVM high memory usage"
            description: "JVM heap usage > 80% for 10m in keycloak-eda."
        {{- end }}

        # 6. Pod Restart Loops
        {{- if .Values.keycloakAlerts.podRestartingFrequently.enabled | default true }}
        - alert: KeycloakPodRestartingFrequently
          expr: increase(kube_pod_container_status_restarts_total{namespace="keycloak-eda", pod=~"keycloak.*"}[10m]) > 3
          for: 5m
          labels:
            severity: {{ .Values.keycloakAlerts.podRestartingFrequently.severity | default "warning" }}
          annotations:
            summary: "Keycloak pod restart loop"
            description: "Keycloak pod in keycloak-eda restarted more than 3 times in 10m."
        {{- end }}

{{- end }}


-----

prometheusRules:
  create: true
  namespace: kube-prometheus-stack

keycloakAlerts:
  keycloakDown:
    enabled: true
    severity: critical
  high5xxRate:
    enabled: true
    severity: warning
  highResponseLatency:
    enabled: true
    severity: warning
  highLoginFailures:
    enabled: true
    severity: warning
  jvmHeapHighMemory:
    enabled: true
    severity: warning
  podRestartingFrequently:
    enabled: true
    severity: warning





--------------------
helm template kube-prometheus-stack prometheus-community/kube-prometheus-stack -f dev-cio-eks-custom-values.yaml --namespace kube-prometheus-stack > out.yaml


-----
prometheus:
  prometheusSpec:
    additionalPrometheusRulesMap:
      keycloak-alerts:
        groups:
          - name: keycloak.rules
            rules:
              # A. Liveness & Health

              # 1. Keycloak Instance Down
              - alert: KeycloakDown
                expr: absent(up{job="keycloak", namespace="keycloak-eda"} == 1)
                for: 5m
                labels:
                  severity: critical
                annotations:
                  summary: "Keycloak service is down"
                  description: "No running Keycloak instance detected in namespace keycloak-eda for more than 5 minutes."

              # 2. High 5xx Error Rate (adjust metric name as per your setup)
              - alert: KeycloakHigh5xxRate
                expr: sum(rate(http_server_requests_seconds_count{job="keycloak", namespace="keycloak-eda", status=~"5.."}[5m])) > 5
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: "High 5xx error rate"
                  description: "Keycloak in keycloak-eda is returning 5xx errors at a high rate."

              # 3. High Response Latency (e.g., 99th percentile > 2s over 5m)
              - alert: KeycloakHighResponseLatency
                expr: histogram_quantile(0.99, sum(rate(http_server_requests_seconds_bucket{job="keycloak", namespace="keycloak-eda"}[5m])) by (le)) > 2
                for: 5m
                labels:
                  severity: warning
                annotations:
                  summary: "Keycloak high response latency"
                  description: "Keycloak 99th percentile response latency > 2s for 5m in keycloak-eda."

              # B. Auth/Security Events

              # 4. High Auth Failure Rate (adjust metric if needed)
              - alert: KeycloakHighLoginFailures
                expr: sum(rate(keycloak_logins_failed_total{namespace="keycloak-eda"}[5m])) > 10
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: "High login failure rate"
                  description: "Keycloak login failures > 10/min in keycloak-eda."

              # C. JVM/Container Health

              # 5. High Memory Usage (JVM heap used > 80% of max)
              - alert: KeycloakJvmHighMemory
                expr: |
                  (
                    (jvm_memory_used_bytes{namespace="keycloak-eda", area="heap"} / ignoring(id) jvm_memory_max_bytes{namespace="keycloak-eda", area="heap"})
                  ) > 0.8
                for: 10m
                labels:
                  severity: warning
                annotations:
                  summary: "Keycloak JVM high memory usage"
                  description: "JVM heap usage > 80% for 10m in keycloak-eda."

              # 6. Restart Loops (pod restarts > 3 in 10m)
              - alert: KeycloakPodRestartingFrequently
                expr: |
                  increase(kube_pod_container_status_restarts_total{namespace="keycloak-eda", pod=~"keycloak.*"}[10m]) > 3
                for: 5m
                labels:
                  severity: warning
                annotations:
                  summary: "Keycloak pod restart loop"
                  description: "Keycloak pod in keycloak-eda restarted more than 3 times in 10m."



--------------
# Use your private image and secret
image:
  repository: <YOUR_PRIVATE_REGISTRY>/fluent-bit
  tag: <YOUR_TAG>
  pullPolicy: IfNotPresent

imagePullSecrets:
  - name: <YOUR_IMAGE_PULL_SECRET_NAME>

# Namespace override
global:
  namespaceOverride: kube-prometheus-stack

serviceAccount:
  create: true
  name: fluent-bit
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_ID>:role/<YOUR-FLUENT-BIT-ROLE>

# Enable the default tail input to collect kubernetes pod logs
input:
  enabled: true
  tag: kube.*
  path: /var/log/containers/*.log   # Chart only allows a glob here, so we filter by grep below
  db: /var/log/flb_kube.db
  multilineParser: docker, cri
  memBufLimit: 50MB
  skipLongLines: On
  refreshInterval: 5

# Optionally add extraInputs/filters if needed
# extraInputs: ""
# additionalInputs: ""

# Kubernetes filter for enrichment
filter:
  kubernetes:
    enabled: true
    match: kube.*

  # Filter to only logs from keycloak-eda namespace
  grep_namespace:
    enabled: true
    match: kube.*
    regex: $kubernetes['namespace_name'] ^keycloak-eda$

  # Filter to only pods whose name starts with keycloak-
  grep_pod:
    enabled: true
    match: kube.*
    regex: $kubernetes['pod_name'] ^keycloak-.*

  # If you want additional filter logic, use extraFilters or additionalFilters
  # extraFilters: ""
  # additionalFilters: ""

# Use the high-performance C plugin for CloudWatch Logs (recommended by AWS)
cloudWatchLogs:
  enabled: true
  match: "*"
  region: eu-west-1
  logGroupName: /fsi_logs_eks_keycloak
  logStreamPrefix: keycloak-
  autoCreateGroup: true
  logRetentionDays: 30

# Optionally: disable other outputs
cloudWatch:
  enabled: false

firehose:
  enabled: false

kinesis:
  enabled: false

kinesis_streams:
  enabled: false

elasticsearch:
  enabled: false

opensearch:
  enabled: false

s3:
  enabled: false

# Resources/tolerations as needed for your cluster
resources:
  requests:
    cpu: 50m
    memory: 100Mi
  limits:
    cpu: 200m
    memory: 256Mi

tolerations:
  - operator: Exists





---------
serviceAccount:
  create: true
  name: fluent-bit
  annotations:
    # IRSA: replace with your fluent-bit IAM role ARN
    eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_ID>:role/<YOUR-FLUENT-BIT-ROLE>

cloudWatch:
  enabled: true
  region: eu-west-1   # Change as needed
  logGroupName: /fsi_logs_eks_keycloak
  logStreamPrefix: keycloak-
  autoCreateGroup: true

input:
  tail:
    enabled: true
    path: /var/log/containers/*_keycloak-eda_*.log
    parser: docker
    tag: kube.*
    refresh_interval: 5
    rotate_wait: 30
    mem_buf_limit: 50MB
    skip_long_lines: On

filterKubernetes:
  enabled: true
  match: kube.*
  # You may add further filtering if you want, e.g., by app label

filters:
  - name: grep
    match: kube.*
    regex: $kubernetes['namespace_name'] ^keycloak-eda$
  - name: grep
    match: kube.*
    regex: $kubernetes['pod_name'] ^keycloak-.*

outputs:
  cloudwatch:
    enabled: true
    region: eu-west-1
    logGroupName: /fsi_logs_eks_keycloak
    logStreamPrefix: keycloak-
    autoCreateGroup: true

resources:
  requests:
    cpu: 50m
    memory: 100Mi
  limits:
    cpu: 200m
    memory: 256Mi

tolerations:
  - operator: Exists

namespaceOverride: kube-prometheus-stack

# RBAC and securityContext as default





-------------------------------------
module "thirdparty_healthcheck" {
  source            = "./module/route53-healthchecks"
  fqdn              = "thirdparty.example.com"
  port              = 443
  protocol          = "HTTPS"
  resource_path     = "/"
  alert_emails      = ["admin@example.com", "devops@example.com"]
  enable_sse        = true    # Set false if you do not want encryption
  alarm_name        = "ThirdPartyApp-HealthCheck-Alarm"
  alarm_description = "Alert if third-party app is down (Route53 health check failed)."
  tags              = { Env = "prod" }
}

--

variable "fqdn" {
  type        = string
  description = "FQDN of the external endpoint to check"
}

variable "port" {
  type        = number
  description = "Port to check (80, 443, or custom)"
  default     = 443
}

variable "protocol" {
  type        = string
  description = "Protocol: HTTP, HTTPS, or TCP"
  default     = "HTTPS"
}

variable "resource_path" {
  type        = string
  description = "Resource path for HTTP/S checks (e.g., /health)"
  default     = "/"
}

variable "alert_emails" {
  type        = list(string)
  description = "List of emails to notify"
}

variable "enable_sse" {
  type        = bool
  description = "If true, enable SNS encryption with a customer KMS key"
  default     = false
}

variable "alarm_name" {
  type        = string
  description = "CloudWatch Alarm Name"
}

variable "alarm_description" {
  type        = string
  description = "Alarm description"
  default     = "Route53 Health Check failure"
}

variable "tags" {
  type        = map(string)
  default     = {}
  description = "Resource tags"
}


---

locals {
  sns_topic_name = "thirdparty-url-health-alerts"
}


---

# KMS Key (only if SSE enabled)
resource "aws_kms_key" "sns" {
  count                   = var.enable_sse ? 1 : 0
  description             = "KMS key for SNS topic encryption"
  deletion_window_in_days = 7
  enable_key_rotation     = true
  tags                    = var.tags
}

# Route53 Health Check
resource "aws_route53_health_check" "this" {
  fqdn              = var.fqdn
  port              = var.port
  type              = var.protocol
  resource_path     = var.resource_path
  request_interval  = 30
  failure_threshold = 3
  tags              = var.tags
}

# SNS Topic (with or without encryption)
resource "aws_sns_topic" "alerts" {
  name              = local.sns_topic_name
  kms_master_key_id = var.enable_sse ? aws_kms_key.sns[0].arn : null
  tags              = var.tags
}

resource "aws_sns_topic_subscription" "email" {
  count     = length(var.alert_emails)
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = var.alert_emails[count.index]
}

resource "aws_cloudwatch_metric_alarm" "this" {
  alarm_name          = var.alarm_name
  comparison_operator = "LessThanThreshold"
  evaluation_periods  = 1
  metric_name         = "HealthCheckStatus"
  namespace           = "AWS/Route53"
  period              = 60
  statistic           = "Minimum"
  threshold           = 1
  alarm_description   = var.alarm_description
  actions_enabled     = true
  alarm_actions       = [aws_sns_topic.alerts.arn]
  dimensions = {
    HealthCheckId = aws_route53_health_check.this.id
  }
  tags = var.tags
}


---

output "health_check_id" {
  value       = aws_route53_health_check.this.id
  description = "ID of the Route53 health check"
}

output "sns_topic_arn" {
  value       = aws_sns_topic.alerts.arn
  description = "ARN of SNS topic for alerts"
}

output "alarm_arn" {
  value       = aws_cloudwatch_metric_alarm.this.arn
  description = "ARN of the CloudWatch alarm"
}








--------------------------
-------------------------
##############################################
# Fluent Bit IRSA & CloudWatch Setup Module  #
##############################################

# 1. Variables: Set from parent/root module

variable "cluster_name" {
  description = "EKS cluster name"
  type        = string
}

variable "namespace" {
  description = "Namespace where Fluent Bit is deployed"
  type        = string
}

variable "service_account" {
  description = "Service account name for Fluent Bit"
  type        = string
  default     = "fluent-bit"
}

variable "log_group_name" {
  description = "Name of the CloudWatch log group"
  type        = string
  default     = "/eks/pmcs-eks-logging"
}

variable "log_retention_days" {
  description = "Log group retention in days"
  type        = number
  default     = 30
}

variable "tags" {
  description = "Common tags"
  type        = map(string)
  default     = {}
}

# 2. Data sources for EKS cluster and OIDC provider

data "aws_eks_cluster" "this" {
  name = var.cluster_name
}

data "aws_eks_cluster_auth" "this" {
  name = var.cluster_name
}

data "aws_iam_openid_connect_provider" "this" {
  url = data.aws_eks_cluster.this.identity[0].oidc[0].issuer
}

# 3. CloudWatch Log Group

resource "aws_cloudwatch_log_group" "fluent_bit" {
  name              = var.log_group_name
  retention_in_days = var.log_retention_days
  tags              = merge(var.tags, { "Name" = var.log_group_name })
}

# 4. IAM Policy for Fluent Bit

data "aws_iam_policy_document" "fluent_bit_cw_logs" {
  statement {
    actions = [
      "logs:CreateLogStream",
      "logs:CreateLogGroup",
      "logs:PutLogEvents",
      "logs:DescribeLogStreams"
    ]
    resources = [
      aws_cloudwatch_log_group.fluent_bit.arn,
      "${aws_cloudwatch_log_group.fluent_bit.arn}:*"
    ]
  }
}

# 5. IRSA Role for Fluent Bit

resource "aws_iam_role" "fluent_bit_irsa" {
  name = "eks-fluent-bit-irsa-${var.namespace}"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Federated = data.aws_iam_openid_connect_provider.this.arn
      }
      Action = "sts:AssumeRoleWithWebIdentity"
      Condition = {
        StringEquals = {
          "${replace(data.aws_iam_openid_connect_provider.this.url, "https://", "")}:sub" = "system:serviceaccount:${var.namespace}:${var.service_account}"
        }
      }
    }]
  })
  tags = merge(var.tags, { "Name" = "eks-fluent-bit-irsa-${var.namespace}" })
}

# 6. Attach Policy to IRSA Role

resource "aws_iam_role_policy" "fluent_bit_attach" {
  name   = "eks-fluent-bit-irsa-policy-${var.namespace}"
  role   = aws_iam_role.fluent_bit_irsa.id
  policy = data.aws_iam_policy_document.fluent_bit_cw_logs.json
}

# 7. Outputs for Helm chart integration

output "fluent_bit_irsa_role_arn" {
  description = "IAM role ARN for Fluent Bit IRSA"
  value       = aws_iam_role.fluent_bit_irsa.arn
}

output "cloudwatch_log_group_name" {
  description = "CloudWatch log group name for Fluent Bit"
  value       = aws_cloudwatch_log_group.fluent_bit.name
}

----

module "fluent_bit_irsa" {
  source          = "./modules/fluent-bit-irsa" # Path to your module
  cluster_name    = "your-eks-cluster"
  namespace       = "kube-prometheus-stack"
  service_account = "fluent-bit"
  log_group_name  = "/eks/pmcs-eks-logging"
  log_retention_days = 30

  tags = {
    Project = "your-project"
    Owner   = "your-team"
  }
}




--------------------
scrapeConfigs:
  - job_name: keycloak
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      # Keep only keycloak-eda namespace
      - source_labels: [__meta_kubernetes_namespace]
        regex: keycloak-eda
        action: keep

      # Keep pods whose name starts with keycloak
      - source_labels: [__meta_kubernetes_pod_name]
        regex: keycloak-.*
        action: keep

      # Relabel app name for metrics filtering
      - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
        target_label: app

      # Map pod labels to promtail labels
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
      - source_labels: [__meta_kubernetes_pod_container_name]
        target_label: container
      - source_labels: [__meta_kubernetes_pod_node_name]
        target_label: node

      # Set __path__ to pod container logs location
      - action: replace
        replacement: /var/log/pods/*/*/*.log
        target_label: __path__






------------------
scrapeConfigs:
  - job_name: keycloak
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      # Keep only pods in keycloak namespace
      - source_labels: [__meta_kubernetes_namespace]
        regex: keycloak-eda
        action: keep

      # Keep only pods with label app=keycloak (adjust if label differs)
      - source_labels: [__meta_kubernetes_pod_label_app]
        regex: keycloak
        action: keep

      # Keep only container logs from the keycloak container
      - source_labels: [__meta_kubernetes_pod_container_name]
        regex: keycloak
        action: keep

      # Map Kubernetes labels to promtail labels
      - source_labels: [__meta_kubernetes_pod_label_app]
        target_label: app
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
      - source_labels: [__meta_kubernetes_pod_container_name]
        target_label: container

      # Set __path__ to point promtail to pod logs on the node
      - action: replace
        replacement: /var/log/pods/*/*/*.log
        target_label: __path__




--------------
loki:
  serviceName: release-d5739a-loki-gateway
  servicePort: 80

config:
  clients:
    - url: http://release-d5739a-loki-gateway.kube-prometheus-stack.svc.cluster.local/loki/api/v1/push

  snippets:
    pipelineStages:
      - docker: {}

  scrapeConfigs:
    - job_name: keycloak
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        # Keep logs only from namespace keycloak-eda
        - source_labels: [__meta_kubernetes_namespace]
          regex: keycloak-eda
          action: keep

        # Keep pods labeled with app.kubernetes.io/name=keycloak
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
          regex: keycloak
          action: keep

        # Keep only logs from the keycloak container
        - source_labels: [__meta_kubernetes_pod_container_name]
          regex: keycloak
          action: keep

        # Relabel standard metadata labels to promtail labels
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
          target_label: app
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_container_name]
          target_label: container

        # Set __path__ to pod log files path for promtail
        - action: replace
          replacement: /var/log/pods/*/*/*.log
          target_label: __path__

extraServerConfigs: |
  health_check_target: false

rbac:
  create: true

serviceAccount:
  create: true
  name: promtail

readinessProbe:
  failureThreshold: 5
  httpGet:
    path: "{{ printf '%s/ready' .Values.httpPathPrefix }}"
    port: 3101
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1





-----------------------
loki:
  serviceName: release-d5739a-loki-gateway
  servicePort: 80

config:
  clients:
    - url: http://release-d5739a-loki-gateway.kube-prometheus-stack.svc.cluster.local/loki/api/v1/push

  snippets:
    pipelineStages:
      - docker: {}

    scrapeConfigs:
      - job_name: keycloak
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            target_label: app
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_node_name
            target_label: node
          - action: keep
            source_labels: [__meta_kubernetes_namespace]
            regex: keycloak-eda
          - action: replace
            replacement: /var/log/pods/*/*/*.log
            target_label: __path__

extraServerConfigs: 
  health_check_target: false

rbac:
  create: true

serviceAccount:
  create: true
  name: promtail

readinessProbe:
  failureThreshold: 5
  httpGet:
    path: "{{ printf '%s/ready' .Values.httpPathPrefix }}"
    port: 3101
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1






-----------------
loki:
  serviceName: release-01234-loki-gateway
  servicePort: 80

config:
  clients:
    - url: http://release-01234-loki-gateway.kube-prometheus-stack.svc.cluster.local/loki/api/v1/push

  positions:
    filename: /run/promtail/positions.yaml

  scrape_configs:
    - job_name: kubernetes-pods
      pipeline_stages:
        - docker: {}
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
          target_label: app
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_container_name]
          target_label: container
        - action: replace
          source_labels:
            - __meta_kubernetes_pod_node_name
          target_label: node
        - action: keep
          source_labels: [__meta_kubernetes_pod_container_name]
          regex: .+


----------
loki:
  auth_enabled: false

  server:
    http_listen_port: 3100
    grpc_listen_port: 9095
    http_server_read_timeout: 600s
    http_server_write_timeout: 600s

  commonConfig:
    path_prefix: /var/loki
    replication_factor: 3

  # S3 storage configuration for AWS IRSA
  storage:
    type: s3
    s3:
      bucketnames: "observability-loki-dev-eda-keycloak-cluster"
      region: "us-east-1"
      endpoint: "s3.us-east-1.amazonaws.com"
      s3ForcePathStyle: true
      insecure: false

  # Required! This is what the Helm chart expects for schema
  schemaConfig:
    configs:
      - from: "2024-01-01"
        store: boltdb-shipper
        object_store: s3
        schema: v12
        index:
          prefix: index_
          period: 24h

  # Enable/disable serviceAccount for IRSA (make sure this name matches your IRSA role binding)
  serviceAccount:
    create: true
    name: loki
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::537124937025:role/loki-irsa-dev"

  replicaCount: 3

  persistence:
    enabled: true
    size: 50Gi
    storageClassName: "gp3-encrypted"

  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 30
    timeoutSeconds: 1

  podSecurityContext:
    fsGroup: 10001
    runAsGroup: 10001
    runAsNonRoot: true
    runAsUser: 10001

  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL
    allowPrivilegeEscalation: false

  rbac:
    create: true

  # Optional: Enable Loki UI if desired
  ui:
    enabled: true

# Prometheus ServiceMonitor (for kube-prometheus-stack integration)
metrics:
  enabled: true
  serviceMonitor:
    enabled: true




--------------------
loki:
  config: |
    auth_enabled: false
    server:
      http_listen_port: 3100
    common:
      replication_factor: 3
    storage_config:
      aws:
        s3: s3://observability-loki-dev-eda-keycloak-cluster
        region: us-east-1
        endpoint: s3.us-east-1.amazonaws.com
        s3forcepathstyle: true
        insecure: false
    schema_config:
      configs:
        - from: 2024-01-01
          store: boltdb-shipper
          object_store: aws
          schema: v12
          index:
            prefix: index_
            period: 24h
    compactor:
      working_directory: /var/loki/compactor
      shared_store: aws
      compaction_interval: 10m
      retention_enabled: true
    limits_config:
      retention_period: 365d
      max_streams_per_user: 0
      max_entries_limit_per_query: 5000
      max_query_length: 168h

  serviceAccount:
    create: true
    name: loki
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::537124937025:role/loki-irsa-dev"

  replicaCount: 3

  persistence:
    enabled: true
    size: 50Gi
    storageClassName: "gp3-encrypted"

  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  readinessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 5
    periodSeconds: 10

  livenessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 15
    periodSeconds: 10

  rbac:
    create: true





-------------
loki:
  enabled: true

  # The ONLY correct S3 storage config for Helm 6.x+ is under loki.config.storage_config
  config:
    auth_enabled: false
    server:
      http_listen_port: 3100
    common:
      replication_factor: 3
    storage_config:
      aws:
        s3: s3://observability-loki-dev-eda-keycloak-cluster
        region: us-east-1
        endpoint: s3.us-east-1.amazonaws.com
        s3forcepathstyle: true
        insecure: false
    schema_config:
      configs:
        - from: 2024-01-01
          store: boltdb-shipper
          object_store: aws
          schema: v12
          index:
            prefix: index_
            period: 24h
    compactor:
      working_directory: /var/loki/compactor
      shared_store: aws
      compaction_interval: 10m
      retention_enabled: true
    limits_config:
      retention_period: 365d
      max_streams_per_user: 0
      max_entries_limit_per_query: 5000
      max_query_length: 168h

  # IRSA ServiceAccount configuration
  serviceAccount:
    create: true
    name: loki
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::537124937025:role/loki-irsa-dev"

  # High Availability (recommended)
  replicaCount: 3

  # Persistent storage for index cache, WAL, etc.
  persistence:
    enabled: true
    size: 50Gi
    storageClassName: "gp3-encrypted"

  # Resources (tune as needed for your scale)
  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

  # ServiceMonitor for Prometheus metrics
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  readinessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 5
    periodSeconds: 10

  livenessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 15
    periodSeconds: 10

  rbac:
    create: true







-----------
loki:
  enabled: true

  # The ONLY correct S3 storage config for Helm 6.x+ is under loki.config.storage_config
  config:
    auth_enabled: false
    server:
      http_listen_port: 3100
    common:
      replication_factor: 3
    storage_config:
      aws:
        s3: s3://observability-loki-dev-eda-keycloak-cluster
        region: us-east-1
        endpoint: s3.us-east-1.amazonaws.com
        s3forcepathstyle: true
        insecure: false
    schema_config:
      configs:
        - from: 2024-01-01
          store: boltdb-shipper
          object_store: aws
          schema: v12
          index:
            prefix: index_
            period: 24h
    compactor:
      working_directory: /var/loki/compactor
      shared_store: aws
      compaction_interval: 10m
      retention_enabled: true
    limits_config:
      retention_period: 365d
      max_streams_per_user: 0
      max_entries_limit_per_query: 5000
      max_query_length: 168h

  # IRSA ServiceAccount configuration
  serviceAccount:
    create: true
    name: loki
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::537124937025:role/loki-irsa-dev"

  # High Availability (recommended)
  replicaCount: 3

  # Persistent storage for index cache, WAL, etc.
  persistence:
    enabled: true
    size: 50Gi
    storageClassName: "gp3-encrypted"

  # Resources (tune as needed for your scale)
  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

  # ServiceMonitor for Prometheus metrics
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  readinessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 5
    periodSeconds: 10

  livenessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 15
    periodSeconds: 10

  rbac:
    create: true









--------------------------------------------------------------
replicaCount: 3   # Horizontal scaling for high availability

# S3 Backend Configuration (Object Storage)
loki:
  commonConfig:
    replication_factor: 3  # Must match replicaCount for multi-zone durability
    ring:
      kvstore:
        store: memberlist

  storage:
    type: 's3'
    s3:
      bucketnames: "observability-loki-${ENVIRONMENT}"   # Fill in via env or hardcode if needed
      endpoint: "s3.${AWS_REGION}.amazonaws.com"
      region: "${AWS_REGION}"   # Set via values or secret
      s3forcepathstyle: true
      insecure: false
      access_key_id: ""        # Empty to use IAM Role (IRSA)
      secret_access_key: ""    # Empty to use IAM Role (IRSA)
    # (No static creds—uses IRSA)

  schemaConfig:
    configs:
      - from: 2020-10-15
        store: boltdb-shipper
        object_store: s3
        schema: v12
        index:
          prefix: index_
          period: 24h

  compactor:
    shared_store: s3
    compaction_interval: 10m
    retention_enabled: true

  limits_config:
    retention_period: 365d    # Matches your S3 lifecycle
    max_streams_per_user: 0
    max_entries_limit_per_query: 5000
    max_query_length: 168h   # 1 week for a single query

  table_manager:
    retention_deletes_enabled: true
    retention_period: 365d

# ServiceAccount and IRSA configuration
serviceAccount:
  create: true
  name: loki
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::${AWS_ACCOUNT_ID}:role/loki-irsa-${ENVIRONMENT}"

# Security
securityContext:
  runAsUser: 10001
  fsGroup: 10001
podSecurityContext:
  runAsUser: 10001
  fsGroup: 10001

# Resource Requests & Limits
resources:
  requests:
    cpu: "500m"
    memory: "2Gi"
  limits:
    cpu: "2"
    memory: "8Gi"

# Network
service:
  type: ClusterIP
  port: 3100

# Ingress (if external access is needed)
ingress:
  enabled: false  # Or true if you expose externally
  # ...configure rules/TLS as needed

# Persistence for BoltDB Shipper cache (recommended)
persistence:
  enabled: true
  accessModes:
    - ReadWriteOnce
  size: 50Gi
  storageClassName: "gp3"    # Or your preferred EBS SC

# Readiness & Liveness Probes
readinessProbe:
  httpGet:
    path: /ready
    port: 3100
  initialDelaySeconds: 5
  periodSeconds: 10

livenessProbe:
  httpGet:
    path: /ready
    port: 3100
  initialDelaySeconds: 15
  periodSeconds: 10

# Monitoring: Enable /metrics for ServiceMonitor
metrics:
  enabled: true
  serviceMonitor:
    enabled: true
    additionalLabels:
      release: "kube-prometheus-stack"   # Match your prometheus deployment's selector
    interval: 30s

# Extra environment variables for flexibility
extraEnv:
  - name: AWS_REGION
    value: "${AWS_REGION}"

# Tracing, if needed
tracing:
  enabled: false

# Alerts (customize as needed)
alerting:
  enabled: false

# RBAC
rbac:
  create: true






---------------------------------------------
extraEnv:
  - name: KC_FEATURES
    value: "user-event-metrics"
  - name: KC_USER_EVENT_METRICS_ENABLED
    value: "true"
  - name: KC_EVENT_METRICS_USER_EVENTS
    value: "login,logout,register,reset_password,update_profile,update_email,refresh_token,remove_credential,delete_account"
  - name: KC_EVENT_METRICS_USER_TAGS
    value: "realm,clientId"



--------------
prometheus:
  prometheusSpec:
    serviceMonitorNamespaceSelector: {}    # empty means all namespaces, or set matchNames if restricting
    serviceMonitorSelector: {}             # default, will pick up all ServiceMonitors
    # (optional) If you have network segmentation, set podMonitorNamespaceSelector similarly
    # (optional) To restrict, use:
    # serviceMonitorNamespaceSelector:
    #   matchNames:
    #     - "keycloak-eda"
  # Expose Prometheus UI via ingress
  ingress:
    enabled: true
    hosts:
      - prometheus.example.com   # change to your DNS
    paths:
      - /
    # Add cert, annotations, etc. as needed

grafana:
  ingress:
    enabled: true
    hosts:
      - grafana.example.com
    paths:
      - /
    # Add cert, annotations, etc. as needed



-----------------
for (def img in images) {
  def scanPassed = false  // default to false
  
  stage("Pull ${img}") {
    container('buildah') {
      sh "buildah pull ${img}"
    }
  }

  stage("Code Analysis - CxOne ${img}") {
    container('buildah') {
      try {
        sh """
          echo "🔎 Scanning image ${img} with CheckmarxOne CLI..."
          checkmarx one scan container \\
            --image ${img} \\
            --project-name ThirdParty-${imageName} \\
            --output-path ./scan-results-${imageName}-${imageTag} \\
            --threshold high=1,medium=5,low=10 \\
            --base-uri https://ast.fis.cxone.cloud \\
            --tenant fis \\
            --agent Jenkins \\
            --apikey ${CHECKMARX_ONE_API_KEY}
        """
        scanPassed = true
      } catch (err) {
        echo "❌ Scan failed for image ${img}, skipping push. Error: ${err}"
        scanPassed = false
      }
    }
  }

  if (scanPassed) {
    stage("Tag & Push ${img} to Artifactory") {
      withCredentials([usernamePassword(credentialsId: 'svacct-hydra-cicd', usernameVariable: 'ART_USER', passwordVariable: 'ART_PASS')]) {
        container('buildah') {
          sh "buildah tag ${img} ${fullArtifactoryPath}"
          echo "🚀 Pushing image ${fullArtifactoryPath}"
          sh """
            echo ${ART_PASS} | buildah login -u ${ART_USER} --password-stdin ${ARTIFACTORY_REGISTRY}
            buildah push ${fullArtifactoryPath}
          """
        }
      }
    }
  } else {
    echo "⚠️ Skipped pushing ${img} due to failed scan."
  }
}






--------------------------------------------------
--------------------------------------------------
images:
  - quay.io/prometheus/prometheus:v3.2.1
  - docker.io/busybox:latest
  - registry.k8s.io/kubectl:latest
  - quay.io/prometheus/alertmanager:v0.28.1
  - quay.io/prometheus-operator/admission-webhook:latest
  - registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.2
  - quay.io/prometheus-operator/prometheus-operator:latest
  - quay.io/prometheus-operator/prometheus-config-reloader:latest
  - quay.io/thanos/thanos:v0.37.2




-------------------
properties([
  parameters([
    string(name: 'IMAGE_LIST_FILE', description: 'Path to YAML file with list of images to process', defaultValue: 'images.yaml')
  ])
])

podTemplate(
  label: 'third-party-image-scan-${UUID.randomUUID().toString()}',
  containers: [
    containerTemplate(
      name: 'buildah',
      image: 'caas-docker-release-local.docker.fis.dev/mirror/buildah/stable:v1.23.1',
      command: 'cat',
      ttyEnabled: true,
      imagePullPolicy: 'IfNotPresent',
      resourceRequestCpu: '100m',
      resourceRequestMemory: '128Mi'
    )
  ],
  imagePullSecrets: [
    'edas-docker-release-local-ro',
    'edas-docker-snapshot-svaccct-hydra-cicd'
  ],
  serviceAccount: 'jenkins'
) {
  node(POD_LABEL) {
    stage('Checkout Image List') {
      checkout scm
    }

    script {
      // Read YAML image list like your existing pipeline reads values.yaml
      def imagesYaml = readYaml file: "${params.IMAGE_LIST_FILE}"
      def images = imagesYaml.images

      for (def img in images) {
        img = img.trim()
        def imgParts = img.split(':')
        def imageName = imgParts[0].split('/').last()
        def imageTag = imgParts.length > 1 ? imgParts[1] : 'latest'
        def artifactoryImage = "${imageName}:${imageTag}"
        def fullArtifactoryPath = "${ARTIFACTORY_REGISTRY}/${artifactoryImage}"

        stage("Pull ${img}") {
          container('buildah') {
            sh "buildah pull ${img}"
          }
        }

        stage("Code Analysis - CxOne ${img}") {
          checkmarxASTScanner(
            additionalOptions: '--threshold sca-high=1;sca-medium=1;sca-low=1',
            checkmarxInstallation: 'checkmarx-one-cli-latest',
            credentialsId: 'cxone_scan_creds',
            projectName: "CxOne_ThirdParty_${imageName}",
            serverUrl: 'https://ast.fis.cxcone.cloud',
            tenantName: 'fis',
            useOnDemandInstallations: true,
            useOnPremServerCredentials: true
          )
        }

        stage("Tag & Push ${img} to Artifactory") {
          withCredentials([usernamePassword(credentialsId: 'svacct-hydra-cicd', usernameVariable: 'ART_USER', passwordVariable: 'ART_PASS')]) {
            container('buildah') {
              echo "🔹 Tagging image ${img} → ${fullArtifactoryPath}"
              sh "buildah tag ${img} ${fullArtifactoryPath}"

              echo "🚀 Pushing image ${fullArtifactoryPath}"
              sh """
                echo ${ART_PASS} | buildah login -u ${ART_USER} --password-stdin ${ARTIFACTORY_REGISTRY}
                buildah push ${fullArtifactoryPath}
              """
            }
          }
        }
      }
    }
  }
}







--------------------------------------------------------
resource "aws_kms_key" "sns_key" {
  description = "KMS key for encrypting observability SNS topic"
  enable_key_rotation = true

  policy = jsonencode({
    Version = "2012-10-17"
    Id = "sns-kms-key-policy"
    Statement = [
      {
        Sid = "Enable IAM User Permissions"
        Effect = "Allow"
        Principal = { AWS = "*" }
        Action = "kms:*"
        Resource = "*"
      }
    ]
  })

  tags = merge(
    {
      Name = "sns-kms-${var.environment}"
    },
    local.fis_tags
  )
}

resource "aws_kms_alias" "sns_key_alias" {
  name          = "alias/observability-sns-${var.environment}"
  target_key_id = aws_kms_key.sns_key.id
}


resource "aws_sns_topic" "alerts" {
  name              = "observability-alerts-${var.environment}"
  kms_master_key_id = aws_kms_key.sns_key.arn
  tags = merge(
    {
      Name = "observability-alerts-${var.environment}"
    },
    local.fis_tags
  )
}


output "sns_kms_key_arn" {
  description = "ARN of the SNS encryption KMS key."
  value       = aws_kms_key.sns_key.arn
}



-----------
#######################################
# Loki S3 Bucket
#######################################

resource "aws_s3_bucket" "loki_bucket" {
  bucket = "observability-loki-${var.environment}"
  force_destroy = true
  tags = local.common_tags
}

resource "aws_s3_bucket_versioning" "loki_versioning" {
  bucket = aws_s3_bucket.loki_bucket.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "loki_encryption" {
  bucket = aws_s3_bucket.loki_bucket.id
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_public_access_block" "loki_block" {
  bucket = aws_s3_bucket.loki_bucket.id
  block_public_acls   = true
  block_public_policy = true
  ignore_public_acls  = true
  restrict_public_buckets = true
}

resource "aws_s3_bucket_lifecycle_configuration" "loki_lifecycle" {
  bucket = aws_s3_bucket.loki_bucket.id
  rule {
    id     = "expire-logs-365d"
    status = "Enabled"
    expiration { days = 365 }
  }
}

#######################################
# Thanos S3 Bucket
#######################################

resource "aws_s3_bucket" "thanos_bucket" {
  bucket = "observability-thanos-${var.environment}"
  force_destroy = true
  tags = local.common_tags
}

resource "aws_s3_bucket_versioning" "thanos_versioning" {
  bucket = aws_s3_bucket.thanos_bucket.id
  versioning_configuration { status = "Enabled" }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "thanos_encryption" {
  bucket = aws_s3_bucket.thanos_bucket.id
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_public_access_block" "thanos_block" {
  bucket = aws_s3_bucket.thanos_bucket.id
  block_public_acls   = true
  block_public_policy = true
  ignore_public_acls  = true
  restrict_public_buckets = true
}

resource "aws_s3_bucket_lifecycle_configuration" "thanos_lifecycle" {
  bucket = aws_s3_bucket.thanos_bucket.id
  rule {
    id     = "expire-metrics-730d"
    status = "Enabled"
    expiration { days = 730 }
  }
}

#######################################
# SNS Topic & Multiple Email Subscriptions
#######################################

resource "aws_sns_topic" "alerts" {
  name = "observability-alerts-${var.environment}"
  tags = local.common_tags
}

resource "aws_sns_topic_subscription" "alerts_emails" {
  for_each = toset(var.alert_emails)
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = each.value
}

#######################################
# IAM Policy for S3 & SNS
#######################################

data "aws_iam_policy_document" "observability_policy" {
  statement {
    actions = ["s3:GetObject", "s3:PutObject", "s3:DeleteObject", "s3:ListBucket"]
    resources = [
      aws_s3_bucket.loki_bucket.arn, "${aws_s3_bucket.loki_bucket.arn}/*",
      aws_s3_bucket.thanos_bucket.arn, "${aws_s3_bucket.thanos_bucket.arn}/*"
    ]
  }
  statement {
    actions = ["sns:Publish"]
    resources = [aws_sns_topic.alerts.arn]
  }
}

resource "aws_iam_policy" "observability_policy" {
  name        = "observability-${var.environment}"
  description = "Access to Loki/Thanos buckets and SNS alerts"
  policy      = data.aws_iam_policy_document.observability_policy.json
}

#######################################
# OIDC Provider & IRSA Roles
#######################################

data "aws_eks_cluster" "cluster" {
  name = var.eks_cluster_name
}

data "aws_iam_openid_connect_provider" "oidc" {
  url = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
}

locals {
  oidc_provider_arn = data.aws_iam_openid_connect_provider.oidc.arn
  oidc_provider_url = replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, "https://", "")
}

resource "aws_iam_role" "loki_irsa" {
  name = "loki-irsa-${var.environment}"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = { Federated = local.oidc_provider_arn },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${local.oidc_provider_url}:sub" = "system:serviceaccount:${var.namespace}:loki"
        }
      }
    }]
  })
  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "loki_attach" {
  role       = aws_iam_role.loki_irsa.name
  policy_arn = aws_iam_policy.observability_policy.arn
}

resource "aws_iam_role" "thanos_irsa" {
  name = "thanos-irsa-${var.environment}"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = { Federated = local.oidc_provider_arn },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${local.oidc_provider_url}:sub" = "system:serviceaccount:${var.namespace}:thanos"
        }
      }
    }]
  })
  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "thanos_attach" {
  role       = aws_iam_role.thanos_irsa.name
  policy_arn = aws_iam_policy.observability_policy.arn
}

resource "aws_iam_role" "prometheus_irsa" {
  name = "prometheus-irsa-${var.environment}"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = { Federated = local.oidc_provider_arn },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${local.oidc_provider_url}:sub" = "system:serviceaccount:${var.namespace}:prometheus"
        }
      }
    }]
  })
  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "prometheus_attach" {
  role       = aws_iam_role.prometheus_irsa.name
  policy_arn = aws_iam_policy.observability_policy.arn
}


------

variables.tf
--

variable "environment" {
  description = "Environment, e.g., dev, prod"
  type        = string
}

variable "alert_emails" {
  description = "List of email addresses for SNS alerts."
  type        = list(string)
}

variable "eks_cluster_name" {
  description = "EKS cluster name."
  type        = string
}

variable "namespace" {
  description = "K8s namespace for observability components."
  type        = string
}

variable "tags" {
  description = "Extra tags to apply."
  type        = map(string)
  default     = {}
}



local.tf
--

locals {
  common_tags = merge({
    Environment = var.environment,
    Project     = "Observability"
  }, var.tags)
}

---


outputs.tf
--

output "loki_bucket_name" {
  description = "Loki bucket name."
  value       = aws_s3_bucket.loki_bucket.bucket
}

output "thanos_bucket_name" {
  description = "Thanos bucket name."
  value       = aws_s3_bucket.thanos_bucket.bucket
}

output "sns_topic_arn" {
  description = "SNS topic ARN."
  value       = aws_sns_topic.alerts.arn
}

output "observability_policy_arn" {
  description = "Observability IAM policy ARN."
  value       = aws_iam_policy.observability_policy.arn
}

output "loki_irsa_role_arn" {
  description = "ARN for Loki IRSA role."
  value       = aws_iam_role.loki_irsa.arn
}

output "thanos_irsa_role_arn" {
  description = "ARN for Thanos IRSA role."
  value       = aws_iam_role.thanos_irsa.arn
}

output "prometheus_irsa_role_arn" {
  description = "ARN for Prometheus IRSA role."
  value       = aws_iam_role.prometheus_irsa.arn
}


----

provider.tf
--

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 4.0"
    }
  }
}

provider "aws" {
  region = var.region
}



-------

main-- main.tf
--

module "observability" {
  source           = "./observability"
  environment      = "prod"
  alert_emails     = ["alerts@example.com", "oncall@example.com"]
  eks_cluster_name = "my-eks-cluster"
  namespace        = "monitoring"
  tags = {
    Owner = "DevOps"
  }
}

