loki:
  auth_enabled: false

  server:
    http_listen_port: 3100
    grpc_listen_port: 9095
    http_server_read_timeout: 600s
    http_server_write_timeout: 600s

  commonConfig:
    path_prefix: /var/loki
    replication_factor: 3

  # S3 storage configuration for AWS IRSA
  storage:
    type: s3
    s3:
      bucketnames: "observability-loki-dev-eda-keycloak-cluster"
      region: "us-east-1"
      endpoint: "s3.us-east-1.amazonaws.com"
      s3ForcePathStyle: true
      insecure: false

  # Required! This is what the Helm chart expects for schema
  schemaConfig:
    configs:
      - from: "2024-01-01"
        store: boltdb-shipper
        object_store: s3
        schema: v12
        index:
          prefix: index_
          period: 24h

  # Enable/disable serviceAccount for IRSA (make sure this name matches your IRSA role binding)
  serviceAccount:
    create: true
    name: loki
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::537124937025:role/loki-irsa-dev"

  replicaCount: 3

  persistence:
    enabled: true
    size: 50Gi
    storageClassName: "gp3-encrypted"

  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 30
    timeoutSeconds: 1

  podSecurityContext:
    fsGroup: 10001
    runAsGroup: 10001
    runAsNonRoot: true
    runAsUser: 10001

  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL
    allowPrivilegeEscalation: false

  rbac:
    create: true

  # Optional: Enable Loki UI if desired
  ui:
    enabled: true

# Prometheus ServiceMonitor (for kube-prometheus-stack integration)
metrics:
  enabled: true
  serviceMonitor:
    enabled: true




--------------------
loki:
  config: |
    auth_enabled: false
    server:
      http_listen_port: 3100
    common:
      replication_factor: 3
    storage_config:
      aws:
        s3: s3://observability-loki-dev-eda-keycloak-cluster
        region: us-east-1
        endpoint: s3.us-east-1.amazonaws.com
        s3forcepathstyle: true
        insecure: false
    schema_config:
      configs:
        - from: 2024-01-01
          store: boltdb-shipper
          object_store: aws
          schema: v12
          index:
            prefix: index_
            period: 24h
    compactor:
      working_directory: /var/loki/compactor
      shared_store: aws
      compaction_interval: 10m
      retention_enabled: true
    limits_config:
      retention_period: 365d
      max_streams_per_user: 0
      max_entries_limit_per_query: 5000
      max_query_length: 168h

  serviceAccount:
    create: true
    name: loki
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::537124937025:role/loki-irsa-dev"

  replicaCount: 3

  persistence:
    enabled: true
    size: 50Gi
    storageClassName: "gp3-encrypted"

  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  readinessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 5
    periodSeconds: 10

  livenessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 15
    periodSeconds: 10

  rbac:
    create: true





-------------
loki:
  enabled: true

  # The ONLY correct S3 storage config for Helm 6.x+ is under loki.config.storage_config
  config:
    auth_enabled: false
    server:
      http_listen_port: 3100
    common:
      replication_factor: 3
    storage_config:
      aws:
        s3: s3://observability-loki-dev-eda-keycloak-cluster
        region: us-east-1
        endpoint: s3.us-east-1.amazonaws.com
        s3forcepathstyle: true
        insecure: false
    schema_config:
      configs:
        - from: 2024-01-01
          store: boltdb-shipper
          object_store: aws
          schema: v12
          index:
            prefix: index_
            period: 24h
    compactor:
      working_directory: /var/loki/compactor
      shared_store: aws
      compaction_interval: 10m
      retention_enabled: true
    limits_config:
      retention_period: 365d
      max_streams_per_user: 0
      max_entries_limit_per_query: 5000
      max_query_length: 168h

  # IRSA ServiceAccount configuration
  serviceAccount:
    create: true
    name: loki
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::537124937025:role/loki-irsa-dev"

  # High Availability (recommended)
  replicaCount: 3

  # Persistent storage for index cache, WAL, etc.
  persistence:
    enabled: true
    size: 50Gi
    storageClassName: "gp3-encrypted"

  # Resources (tune as needed for your scale)
  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

  # ServiceMonitor for Prometheus metrics
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  readinessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 5
    periodSeconds: 10

  livenessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 15
    periodSeconds: 10

  rbac:
    create: true







-----------
loki:
  enabled: true

  # The ONLY correct S3 storage config for Helm 6.x+ is under loki.config.storage_config
  config:
    auth_enabled: false
    server:
      http_listen_port: 3100
    common:
      replication_factor: 3
    storage_config:
      aws:
        s3: s3://observability-loki-dev-eda-keycloak-cluster
        region: us-east-1
        endpoint: s3.us-east-1.amazonaws.com
        s3forcepathstyle: true
        insecure: false
    schema_config:
      configs:
        - from: 2024-01-01
          store: boltdb-shipper
          object_store: aws
          schema: v12
          index:
            prefix: index_
            period: 24h
    compactor:
      working_directory: /var/loki/compactor
      shared_store: aws
      compaction_interval: 10m
      retention_enabled: true
    limits_config:
      retention_period: 365d
      max_streams_per_user: 0
      max_entries_limit_per_query: 5000
      max_query_length: 168h

  # IRSA ServiceAccount configuration
  serviceAccount:
    create: true
    name: loki
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::537124937025:role/loki-irsa-dev"

  # High Availability (recommended)
  replicaCount: 3

  # Persistent storage for index cache, WAL, etc.
  persistence:
    enabled: true
    size: 50Gi
    storageClassName: "gp3-encrypted"

  # Resources (tune as needed for your scale)
  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

  # ServiceMonitor for Prometheus metrics
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  readinessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 5
    periodSeconds: 10

  livenessProbe:
    httpGet:
      path: /ready
      port: 3100
    initialDelaySeconds: 15
    periodSeconds: 10

  rbac:
    create: true









--------------------------------------------------------------
replicaCount: 3   # Horizontal scaling for high availability

# S3 Backend Configuration (Object Storage)
loki:
  commonConfig:
    replication_factor: 3  # Must match replicaCount for multi-zone durability
    ring:
      kvstore:
        store: memberlist

  storage:
    type: 's3'
    s3:
      bucketnames: "observability-loki-${ENVIRONMENT}"   # Fill in via env or hardcode if needed
      endpoint: "s3.${AWS_REGION}.amazonaws.com"
      region: "${AWS_REGION}"   # Set via values or secret
      s3forcepathstyle: true
      insecure: false
      access_key_id: ""        # Empty to use IAM Role (IRSA)
      secret_access_key: ""    # Empty to use IAM Role (IRSA)
    # (No static credsâ€”uses IRSA)

  schemaConfig:
    configs:
      - from: 2020-10-15
        store: boltdb-shipper
        object_store: s3
        schema: v12
        index:
          prefix: index_
          period: 24h

  compactor:
    shared_store: s3
    compaction_interval: 10m
    retention_enabled: true

  limits_config:
    retention_period: 365d    # Matches your S3 lifecycle
    max_streams_per_user: 0
    max_entries_limit_per_query: 5000
    max_query_length: 168h   # 1 week for a single query

  table_manager:
    retention_deletes_enabled: true
    retention_period: 365d

# ServiceAccount and IRSA configuration
serviceAccount:
  create: true
  name: loki
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::${AWS_ACCOUNT_ID}:role/loki-irsa-${ENVIRONMENT}"

# Security
securityContext:
  runAsUser: 10001
  fsGroup: 10001
podSecurityContext:
  runAsUser: 10001
  fsGroup: 10001

# Resource Requests & Limits
resources:
  requests:
    cpu: "500m"
    memory: "2Gi"
  limits:
    cpu: "2"
    memory: "8Gi"

# Network
service:
  type: ClusterIP
  port: 3100

# Ingress (if external access is needed)
ingress:
  enabled: false  # Or true if you expose externally
  # ...configure rules/TLS as needed

# Persistence for BoltDB Shipper cache (recommended)
persistence:
  enabled: true
  accessModes:
    - ReadWriteOnce
  size: 50Gi
  storageClassName: "gp3"    # Or your preferred EBS SC

# Readiness & Liveness Probes
readinessProbe:
  httpGet:
    path: /ready
    port: 3100
  initialDelaySeconds: 5
  periodSeconds: 10

livenessProbe:
  httpGet:
    path: /ready
    port: 3100
  initialDelaySeconds: 15
  periodSeconds: 10

# Monitoring: Enable /metrics for ServiceMonitor
metrics:
  enabled: true
  serviceMonitor:
    enabled: true
    additionalLabels:
      release: "kube-prometheus-stack"   # Match your prometheus deployment's selector
    interval: 30s

# Extra environment variables for flexibility
extraEnv:
  - name: AWS_REGION
    value: "${AWS_REGION}"

# Tracing, if needed
tracing:
  enabled: false

# Alerts (customize as needed)
alerting:
  enabled: false

# RBAC
rbac:
  create: true






---------------------------------------------
extraEnv:
  - name: KC_FEATURES
    value: "user-event-metrics"
  - name: KC_USER_EVENT_METRICS_ENABLED
    value: "true"
  - name: KC_EVENT_METRICS_USER_EVENTS
    value: "login,logout,register,reset_password,update_profile,update_email,refresh_token,remove_credential,delete_account"
  - name: KC_EVENT_METRICS_USER_TAGS
    value: "realm,clientId"



--------------
prometheus:
  prometheusSpec:
    serviceMonitorNamespaceSelector: {}    # empty means all namespaces, or set matchNames if restricting
    serviceMonitorSelector: {}             # default, will pick up all ServiceMonitors
    # (optional) If you have network segmentation, set podMonitorNamespaceSelector similarly
    # (optional) To restrict, use:
    # serviceMonitorNamespaceSelector:
    #   matchNames:
    #     - "keycloak-eda"
  # Expose Prometheus UI via ingress
  ingress:
    enabled: true
    hosts:
      - prometheus.example.com   # change to your DNS
    paths:
      - /
    # Add cert, annotations, etc. as needed

grafana:
  ingress:
    enabled: true
    hosts:
      - grafana.example.com
    paths:
      - /
    # Add cert, annotations, etc. as needed



-----------------
for (def img in images) {
  def scanPassed = false  // default to false
  
  stage("Pull ${img}") {
    container('buildah') {
      sh "buildah pull ${img}"
    }
  }

  stage("Code Analysis - CxOne ${img}") {
    container('buildah') {
      try {
        sh """
          echo "ðŸ”Ž Scanning image ${img} with CheckmarxOne CLI..."
          checkmarx one scan container \\
            --image ${img} \\
            --project-name ThirdParty-${imageName} \\
            --output-path ./scan-results-${imageName}-${imageTag} \\
            --threshold high=1,medium=5,low=10 \\
            --base-uri https://ast.fis.cxone.cloud \\
            --tenant fis \\
            --agent Jenkins \\
            --apikey ${CHECKMARX_ONE_API_KEY}
        """
        scanPassed = true
      } catch (err) {
        echo "âŒ Scan failed for image ${img}, skipping push. Error: ${err}"
        scanPassed = false
      }
    }
  }

  if (scanPassed) {
    stage("Tag & Push ${img} to Artifactory") {
      withCredentials([usernamePassword(credentialsId: 'svacct-hydra-cicd', usernameVariable: 'ART_USER', passwordVariable: 'ART_PASS')]) {
        container('buildah') {
          sh "buildah tag ${img} ${fullArtifactoryPath}"
          echo "ðŸš€ Pushing image ${fullArtifactoryPath}"
          sh """
            echo ${ART_PASS} | buildah login -u ${ART_USER} --password-stdin ${ARTIFACTORY_REGISTRY}
            buildah push ${fullArtifactoryPath}
          """
        }
      }
    }
  } else {
    echo "âš ï¸ Skipped pushing ${img} due to failed scan."
  }
}






--------------------------------------------------
--------------------------------------------------
images:
  - quay.io/prometheus/prometheus:v3.2.1
  - docker.io/busybox:latest
  - registry.k8s.io/kubectl:latest
  - quay.io/prometheus/alertmanager:v0.28.1
  - quay.io/prometheus-operator/admission-webhook:latest
  - registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.2
  - quay.io/prometheus-operator/prometheus-operator:latest
  - quay.io/prometheus-operator/prometheus-config-reloader:latest
  - quay.io/thanos/thanos:v0.37.2




-------------------
properties([
  parameters([
    string(name: 'IMAGE_LIST_FILE', description: 'Path to YAML file with list of images to process', defaultValue: 'images.yaml')
  ])
])

podTemplate(
  label: 'third-party-image-scan-${UUID.randomUUID().toString()}',
  containers: [
    containerTemplate(
      name: 'buildah',
      image: 'caas-docker-release-local.docker.fis.dev/mirror/buildah/stable:v1.23.1',
      command: 'cat',
      ttyEnabled: true,
      imagePullPolicy: 'IfNotPresent',
      resourceRequestCpu: '100m',
      resourceRequestMemory: '128Mi'
    )
  ],
  imagePullSecrets: [
    'edas-docker-release-local-ro',
    'edas-docker-snapshot-svaccct-hydra-cicd'
  ],
  serviceAccount: 'jenkins'
) {
  node(POD_LABEL) {
    stage('Checkout Image List') {
      checkout scm
    }

    script {
      // Read YAML image list like your existing pipeline reads values.yaml
      def imagesYaml = readYaml file: "${params.IMAGE_LIST_FILE}"
      def images = imagesYaml.images

      for (def img in images) {
        img = img.trim()
        def imgParts = img.split(':')
        def imageName = imgParts[0].split('/').last()
        def imageTag = imgParts.length > 1 ? imgParts[1] : 'latest'
        def artifactoryImage = "${imageName}:${imageTag}"
        def fullArtifactoryPath = "${ARTIFACTORY_REGISTRY}/${artifactoryImage}"

        stage("Pull ${img}") {
          container('buildah') {
            sh "buildah pull ${img}"
          }
        }

        stage("Code Analysis - CxOne ${img}") {
          checkmarxASTScanner(
            additionalOptions: '--threshold sca-high=1;sca-medium=1;sca-low=1',
            checkmarxInstallation: 'checkmarx-one-cli-latest',
            credentialsId: 'cxone_scan_creds',
            projectName: "CxOne_ThirdParty_${imageName}",
            serverUrl: 'https://ast.fis.cxcone.cloud',
            tenantName: 'fis',
            useOnDemandInstallations: true,
            useOnPremServerCredentials: true
          )
        }

        stage("Tag & Push ${img} to Artifactory") {
          withCredentials([usernamePassword(credentialsId: 'svacct-hydra-cicd', usernameVariable: 'ART_USER', passwordVariable: 'ART_PASS')]) {
            container('buildah') {
              echo "ðŸ”¹ Tagging image ${img} â†’ ${fullArtifactoryPath}"
              sh "buildah tag ${img} ${fullArtifactoryPath}"

              echo "ðŸš€ Pushing image ${fullArtifactoryPath}"
              sh """
                echo ${ART_PASS} | buildah login -u ${ART_USER} --password-stdin ${ARTIFACTORY_REGISTRY}
                buildah push ${fullArtifactoryPath}
              """
            }
          }
        }
      }
    }
  }
}







--------------------------------------------------------
resource "aws_kms_key" "sns_key" {
  description = "KMS key for encrypting observability SNS topic"
  enable_key_rotation = true

  policy = jsonencode({
    Version = "2012-10-17"
    Id = "sns-kms-key-policy"
    Statement = [
      {
        Sid = "Enable IAM User Permissions"
        Effect = "Allow"
        Principal = { AWS = "*" }
        Action = "kms:*"
        Resource = "*"
      }
    ]
  })

  tags = merge(
    {
      Name = "sns-kms-${var.environment}"
    },
    local.fis_tags
  )
}

resource "aws_kms_alias" "sns_key_alias" {
  name          = "alias/observability-sns-${var.environment}"
  target_key_id = aws_kms_key.sns_key.id
}


resource "aws_sns_topic" "alerts" {
  name              = "observability-alerts-${var.environment}"
  kms_master_key_id = aws_kms_key.sns_key.arn
  tags = merge(
    {
      Name = "observability-alerts-${var.environment}"
    },
    local.fis_tags
  )
}


output "sns_kms_key_arn" {
  description = "ARN of the SNS encryption KMS key."
  value       = aws_kms_key.sns_key.arn
}



-----------
#######################################
# Loki S3 Bucket
#######################################

resource "aws_s3_bucket" "loki_bucket" {
  bucket = "observability-loki-${var.environment}"
  force_destroy = true
  tags = local.common_tags
}

resource "aws_s3_bucket_versioning" "loki_versioning" {
  bucket = aws_s3_bucket.loki_bucket.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "loki_encryption" {
  bucket = aws_s3_bucket.loki_bucket.id
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_public_access_block" "loki_block" {
  bucket = aws_s3_bucket.loki_bucket.id
  block_public_acls   = true
  block_public_policy = true
  ignore_public_acls  = true
  restrict_public_buckets = true
}

resource "aws_s3_bucket_lifecycle_configuration" "loki_lifecycle" {
  bucket = aws_s3_bucket.loki_bucket.id
  rule {
    id     = "expire-logs-365d"
    status = "Enabled"
    expiration { days = 365 }
  }
}

#######################################
# Thanos S3 Bucket
#######################################

resource "aws_s3_bucket" "thanos_bucket" {
  bucket = "observability-thanos-${var.environment}"
  force_destroy = true
  tags = local.common_tags
}

resource "aws_s3_bucket_versioning" "thanos_versioning" {
  bucket = aws_s3_bucket.thanos_bucket.id
  versioning_configuration { status = "Enabled" }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "thanos_encryption" {
  bucket = aws_s3_bucket.thanos_bucket.id
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_public_access_block" "thanos_block" {
  bucket = aws_s3_bucket.thanos_bucket.id
  block_public_acls   = true
  block_public_policy = true
  ignore_public_acls  = true
  restrict_public_buckets = true
}

resource "aws_s3_bucket_lifecycle_configuration" "thanos_lifecycle" {
  bucket = aws_s3_bucket.thanos_bucket.id
  rule {
    id     = "expire-metrics-730d"
    status = "Enabled"
    expiration { days = 730 }
  }
}

#######################################
# SNS Topic & Multiple Email Subscriptions
#######################################

resource "aws_sns_topic" "alerts" {
  name = "observability-alerts-${var.environment}"
  tags = local.common_tags
}

resource "aws_sns_topic_subscription" "alerts_emails" {
  for_each = toset(var.alert_emails)
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = each.value
}

#######################################
# IAM Policy for S3 & SNS
#######################################

data "aws_iam_policy_document" "observability_policy" {
  statement {
    actions = ["s3:GetObject", "s3:PutObject", "s3:DeleteObject", "s3:ListBucket"]
    resources = [
      aws_s3_bucket.loki_bucket.arn, "${aws_s3_bucket.loki_bucket.arn}/*",
      aws_s3_bucket.thanos_bucket.arn, "${aws_s3_bucket.thanos_bucket.arn}/*"
    ]
  }
  statement {
    actions = ["sns:Publish"]
    resources = [aws_sns_topic.alerts.arn]
  }
}

resource "aws_iam_policy" "observability_policy" {
  name        = "observability-${var.environment}"
  description = "Access to Loki/Thanos buckets and SNS alerts"
  policy      = data.aws_iam_policy_document.observability_policy.json
}

#######################################
# OIDC Provider & IRSA Roles
#######################################

data "aws_eks_cluster" "cluster" {
  name = var.eks_cluster_name
}

data "aws_iam_openid_connect_provider" "oidc" {
  url = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
}

locals {
  oidc_provider_arn = data.aws_iam_openid_connect_provider.oidc.arn
  oidc_provider_url = replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, "https://", "")
}

resource "aws_iam_role" "loki_irsa" {
  name = "loki-irsa-${var.environment}"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = { Federated = local.oidc_provider_arn },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${local.oidc_provider_url}:sub" = "system:serviceaccount:${var.namespace}:loki"
        }
      }
    }]
  })
  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "loki_attach" {
  role       = aws_iam_role.loki_irsa.name
  policy_arn = aws_iam_policy.observability_policy.arn
}

resource "aws_iam_role" "thanos_irsa" {
  name = "thanos-irsa-${var.environment}"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = { Federated = local.oidc_provider_arn },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${local.oidc_provider_url}:sub" = "system:serviceaccount:${var.namespace}:thanos"
        }
      }
    }]
  })
  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "thanos_attach" {
  role       = aws_iam_role.thanos_irsa.name
  policy_arn = aws_iam_policy.observability_policy.arn
}

resource "aws_iam_role" "prometheus_irsa" {
  name = "prometheus-irsa-${var.environment}"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = { Federated = local.oidc_provider_arn },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${local.oidc_provider_url}:sub" = "system:serviceaccount:${var.namespace}:prometheus"
        }
      }
    }]
  })
  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "prometheus_attach" {
  role       = aws_iam_role.prometheus_irsa.name
  policy_arn = aws_iam_policy.observability_policy.arn
}


------

variables.tf
--

variable "environment" {
  description = "Environment, e.g., dev, prod"
  type        = string
}

variable "alert_emails" {
  description = "List of email addresses for SNS alerts."
  type        = list(string)
}

variable "eks_cluster_name" {
  description = "EKS cluster name."
  type        = string
}

variable "namespace" {
  description = "K8s namespace for observability components."
  type        = string
}

variable "tags" {
  description = "Extra tags to apply."
  type        = map(string)
  default     = {}
}



local.tf
--

locals {
  common_tags = merge({
    Environment = var.environment,
    Project     = "Observability"
  }, var.tags)
}

---


outputs.tf
--

output "loki_bucket_name" {
  description = "Loki bucket name."
  value       = aws_s3_bucket.loki_bucket.bucket
}

output "thanos_bucket_name" {
  description = "Thanos bucket name."
  value       = aws_s3_bucket.thanos_bucket.bucket
}

output "sns_topic_arn" {
  description = "SNS topic ARN."
  value       = aws_sns_topic.alerts.arn
}

output "observability_policy_arn" {
  description = "Observability IAM policy ARN."
  value       = aws_iam_policy.observability_policy.arn
}

output "loki_irsa_role_arn" {
  description = "ARN for Loki IRSA role."
  value       = aws_iam_role.loki_irsa.arn
}

output "thanos_irsa_role_arn" {
  description = "ARN for Thanos IRSA role."
  value       = aws_iam_role.thanos_irsa.arn
}

output "prometheus_irsa_role_arn" {
  description = "ARN for Prometheus IRSA role."
  value       = aws_iam_role.prometheus_irsa.arn
}


----

provider.tf
--

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 4.0"
    }
  }
}

provider "aws" {
  region = var.region
}



-------

main-- main.tf
--

module "observability" {
  source           = "./observability"
  environment      = "prod"
  alert_emails     = ["alerts@example.com", "oncall@example.com"]
  eks_cluster_name = "my-eks-cluster"
  namespace        = "monitoring"
  tags = {
    Owner = "DevOps"
  }
}

