


{{- if .Values.monitoring.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: {{ include "keycloak.fullname" . }}-metrics
  namespace: {{ .Release.Namespace }}
  labels:
    release: prometheus-stack
    app.kubernetes.io/name: keycloakx
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  namespaceSelector:
    matchNames:
      - {{ .Release.Namespace }}
  selector:
    matchLabels:
      {{- toYaml .Values.service.selector | nindent 6 }}
  endpoints:
    - port: {{ .Values.monitoring.portName }}
      path: {{ .Values.monitoring.path }}
      interval: 30s
{{- end }}


---

serviceMonitor:
  enabled: true
  path: /auth/metrics
  port: http-internal                # Your 9000 port name
  interval: 30s
  scrapeTimeout: 10s
  scheme: http
  namespaceSelector:
    matchNames:
      - keycloak-eda                 # or leave blank to use current namespace
  labels:
    release: prometheus-stack

extraServiceMonitor: {}             # Required placeholder, leave empty

---

monitoring:
  enabled: true
  portName: http-internal     # Matches the port name 9000 in your Service
  path: /auth/metrics

service:
  selector:
    app.kubernetes.io/name: keycloakx  # Matches metadata.labels on the Service



-----
Immediate To-Do (Step 2.1): Mirror the Chart

Helm Source	Value
Chart repo URL	https://prometheus-community.github.io/helm-charts
Chart name	kube-prometheus-stack
Version	60.0.2 (stable, bundles Prometheus v2.51, Grafana 10.x)
Jenkins mirror form input:
Field	Value
SRC_REPO_URL	https://prometheus-community.github.io/helm-charts
SRC_REPO_NAME	prometheus-community
SRC_CHART_NAME	kube-prometheus-stack
SRC_CHART_VERSION	60.0.2
TARGET_REPO	hydra-helm-upstreams






----------------
Step-by-Step: Get Admin Token

üîê 1. Login with the admin-cli client
Use a POST request to the token endpoint:

export KEYCLOAK_URL="http://<keycloak-host-or-svc>:8080"  # or use HTTPS if exposed externally

export TOKEN=$(curl -s -X POST "$KEYCLOAK_URL/realms/master/protocol/openid-connect/token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=<admin-user>" \
  -d "password=<admin-password>" \
  -d "grant_type=password" \
  -d "client_id=admin-cli" \
  | jq -r .access_token)
üîê Replace:
<admin-user> ‚Üí your admin username (usually admin)
<admin-password> ‚Üí admin password
If not using jq, remove the | jq -r .access_token part and parse manually.
üß™ 2. Test the Token (Optional)
You can inspect the token to ensure it was retrieved:

echo $TOKEN | cut -c1-50
üöÄ 3. Use the Token in Admin API
Now you can make admin requests, like updating a realm:

curl -i -X PUT "$KEYCLOAK_URL/admin/realms/master" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{ "displayName": "Updated Realm" }'
Expected response: 204 No Content ‚Üí success

# Get token
TOKEN=$(curl -s -X POST https://<host>/realms/master/protocol/openid-connect/token \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin" \
  -d "password=yourpass" \
  -d "grant_type=password" \
  -d "client_id=admin-cli" | jq -r .access_token)

# Try update
curl -i -X PUT https://<host>/admin/realms/master \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"displayName": "Test Realm Name"}'


------
sh '''
  echo "Helm version check:"
  helm version

  echo "Docker version check:"
  docker version || echo "Docker not installed or not needed"

  echo "Enable OCI support"
  export HELM_EXPERIMENTAL_OCI=1

  echo "Attempting OCI helm pull"
  helm pull oci://registry-1.docker.io/bitnamicharts/metrics-server --version 7.4.5

  echo "List downloaded files:"
  ls -l metrics-server-*.tgz || echo "Expected .tgz chart not found"

  echo "Inspect file type:"
  file metrics-server-*.tgz || echo "File inspection failed"

  echo "Optional: print SHA256 for integrity"
  sha256sum metrics-server-*.tgz || echo "Checksum failed"
'''


-------
Subject: New Keycloak Deployment ‚Äì Access Details & Next Steps for Azure AD Integration

Good morning all,

We‚Äôre pleased to inform you that the new Keycloak instance has been successfully deployed inside the CIO-managed AWS Account. You can access it using the following URL:

üîó [<Insert New Keycloak URL>]

This is a fresh deployment of Keycloak version 26, and we‚Äôve migrated our existing non-productive data from the legacy Keycloak 18 environment. As you're aware, Keycloak has transitioned from the JBoss-based model to Quarkus, which brought several core configuration changes.

‚úÖ What‚Äôs Changed:
Keycloak Version Upgrade: From v18 (JBoss) to v26 (Quarkus)
Authentication Flow: We have replaced the older prod.local offline LDAP authentication with Azure Active Directory (prod.cloud AD) integration.
Login Method: You can now access the admin console using your Azure AD (fisglobal) credentials.
üîç Action Required:
Please take a moment to access the new environment and verify if:

You can log in successfully using your Azure AD credentials.
Your expected data is visible and accessible.
üìå Next Steps ‚Äì Azure AD Access Control:
We would appreciate your suggestions and support on the following:

Access Scope:
Should we allow login access to all users with a fisglobal.com email address?
Default Permissions:
What should be the default role or permission for users who are able to authenticate via Azure AD?
Role Assignments:
What is the preferred approach for assigning roles and privileges to specific users or groups?
Azure AD Controls:
Do we have any existing access restrictions or group-based policies defined at the Azure AD level that we can leverage?
If you have any information or policies regarding this, please share them. Otherwise, we can coordinate with the Azure AD team to define the right approach for access governance moving forward.

Looking forward to your feedback and guidance.

Best regards,
Kiran Gonela



-------
Summary: GitHub Migration - Current Status
Migration Request Initiated: Requested FIS Migration team to migrate DTS Org and EDP Org repositories from github.worldpay.com to FIS GitHub Enterprise.
Test Migration Completed: Repositories have been migrated to a temporary test location within FIS GitHub Enterprise.
Validation in Progress: Teams are currently testing connectivity and pipeline integration from the new test repo location.
Service Account Request Raised: A request for a service account in the new GitHub Enterprise environment has been raised and is currently in progress.
Next Steps:
Upon successful validation, the FIS GitHub Enterprise migration team will finalize the migration to the permanent destination.
Teams will be notified of the new repo paths and necessary updates to pipelines and other integrations will be made accordingly.


-------
# aws_cost_full_summary.py

import pandas as pd
import subprocess
import io
import json
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.styles import Font

excel_path = "aws_cost_full_summary.xlsx"

# ----------------------
# Utility: Run SQL Query
# ----------------------
def run_steampipe_query(sql):
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except subprocess.CalledProcessError as e:
        print(f"Steampipe error: {e.stderr.decode()}")
        return pd.DataFrame()
    except pd.errors.EmptyDataError:
        print("Empty CSV from Steampipe.")
        return pd.DataFrame()

# ----------------------
# 1. Total Monthly Cost Trend
# ----------------------
print("Generating Total Account Cost Trend...")
steampipe_trend_sql = """
SELECT
  TO_CHAR(period_start, 'YYYY-MM') AS month,
  SUM(unblended_cost_amount)::numeric::money AS total_cost
FROM
  aws_cost_by_service_monthly
WHERE
  period_start IN (
    DATE '2025-01-01',
    DATE '2025-02-01',
    DATE '2025-03-01'
  )
GROUP BY
  period_start
ORDER BY
  period_start;
"""
df_trend = run_steampipe_query(steampipe_trend_sql)
df_trend["total_cost"] = df_trend["total_cost"].replace('[\$,]', '', regex=True).astype(float)
df_trend["Trend vs Prev Month"] = df_trend["total_cost"].diff().fillna(0).apply(lambda x: f"${x:,.2f}")
df_trend["% Change"] = df_trend["total_cost"].pct_change().fillna(0).apply(lambda x: f"{x*100:.2f}%")
df_trend["total_cost"] = df_trend["total_cost"].apply(lambda x: f"${x:,.2f}")

# ----------------------
# 2. Top 10 Services with Top 5 Usage Types
# ----------------------
print("Generating Service Usage Breakdown...")
steampipe_main_sql = """
WITH active_services AS (
  SELECT service, SUM(unblended_cost_amount) AS total_cost
  FROM aws_cost_by_service_monthly
  WHERE period_start = DATE '2025-03-01'
  GROUP BY service
  ORDER BY total_cost DESC
  LIMIT 10
),
ranked_usage_types AS (
  SELECT s.service, s.usage_type, SUM(s.unblended_cost_amount) AS usage_cost,
         RANK() OVER (PARTITION BY s.service ORDER BY SUM(s.unblended_cost_amount) DESC) AS usage_rank
  FROM aws_cost_by_service_usage_type_monthly s
  WHERE s.period_start = DATE '2025-03-01'
  GROUP BY s.service, s.usage_type
)
SELECT a.service,
       a.total_cost::numeric::money AS total_service_cost,
       u.usage_type,
       u.usage_cost::numeric::money AS usage_type_cost
FROM active_services a
JOIN ranked_usage_types u ON a.service = u.service
WHERE u.usage_rank <= 5
ORDER BY a.total_cost DESC, u.usage_cost DESC;
"""
df = run_steampipe_query(steampipe_main_sql)
df["total_service_cost"] = df["total_service_cost"].replace('[\$,]', '', regex=True).astype(float)
df["usage_type_cost"] = df["usage_type_cost"].replace('[\$,]', '', regex=True).astype(float)
df["usage_%_of_service"] = ((df["usage_type_cost"] / df["total_service_cost"]) * 100).round(2)
df["total_service_cost_str"] = df["total_service_cost"].apply(lambda x: f"${x:,.2f}")
df["usage_type_cost_str"] = df["usage_type_cost"].apply(lambda x: f"${x:,.2f}")
df["usage_%_of_service_str"] = df["usage_%_of_service"].apply(lambda x: f"{x:.2f}%")
df["service_display"] = df["service"]
df["total_service_cost_display"] = df["total_service_cost_str"]
df.loc[df["service_display"].duplicated(), "service_display"] = ""
df.loc[df["total_service_cost_display"].duplicated(), "total_service_cost_display"] = ""
df_main = df[["service_display", "total_service_cost_display", "usage_type", "usage_type_cost_str", "usage_%_of_service_str"]]
df_main.columns = ["Service", "Total Service Cost", "Usage Type", "Usage Type Cost", "% of Service Cost"]

# ----------------------
# 3. Data Services Summary (EBS, RDS, DynamoDB, S3)
# ----------------------
print("Generating Data Service Summary...")
bucket_query = "SELECT name FROM aws_s3_bucket;"
df_buckets = run_steampipe_query(bucket_query)
bucket_names = df_buckets["name"].tolist()
s3_resource_count = len(bucket_names)
bucket_sizes = []

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT value AS size_bytes, timestamp
    FROM aws_cloudwatch_metric_data_point
    WHERE id = 'e1' AND metric_stat = '{json.dumps(metric_stat)}'
    ORDER BY timestamp DESC
    LIMIT 1;
    """
    df_metric = run_steampipe_query(sql)
    if not df_metric.empty and "size_bytes" in df_metric.columns:
        size_bytes = df_metric.iloc[0]["size_bytes"]
        size_gb = round(size_bytes / 1024 / 1024 / 1024, 2)
        bucket_sizes.append(size_gb)

s3_total_gb = round(sum(bucket_sizes), 2)

steampipe_sql = """
WITH ebs AS (
  SELECT 'EBS' AS service, COUNT(*) AS resource_count, SUM(size) AS total_gb
  FROM aws_ebs_volume
),
rds AS (
  SELECT 'RDS' AS service, COUNT(*) AS resource_count, SUM(allocated_storage) AS total_gb
  FROM aws_rds_db_cluster
),
dynamodb AS (
  SELECT 'DynamoDB' AS service,
         COUNT(*) AS resource_count,
         ROUND(SUM(table_size_bytes) FILTER (WHERE table_size_bytes > 0) / 1024 / 1024 / 1024, 2) AS total_gb
  FROM aws_dynamodb_table
  WHERE table_size_bytes IS NOT NULL
)
SELECT * FROM ebs
UNION ALL
SELECT * FROM rds
UNION ALL
SELECT * FROM dynamodb;
"""
df_others = run_steampipe_query(steampipe_sql)
df_others.columns = ["Service", "Resource Count", "Total Size (GB)"]
df_s3_row = pd.DataFrame([{
    "Service": "S3",
    "Resource Count": s3_resource_count,
    "Total Size (GB)": s3_total_gb
}])
df_data_services = pd.concat([df_others, df_s3_row], ignore_index=True)

# ----------------------
# 4. Write to Excel
# ----------------------
print("Writing final Excel report...")
with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
    df_trend.to_excel(writer, sheet_name="Cost Summary", startrow=1, index=False)
    workbook = writer.book
    sheet = writer.sheets["Cost Summary"]
    sheet.cell(row=1, column=1).value = "Total AWS Account Cost Trend"

    row_offset = len(df_trend) + 4
    sheet.cell(row=row_offset, column=1).value = "Top 10 Services (with Top 5 Usage Types)"
    for r_idx, row in enumerate(dataframe_to_rows(df_main, index=False, header=True), start=row_offset + 1):
        for c_idx, value in enumerate(row, start=1):
            sheet.cell(row=r_idx, column=c_idx).value = value

    row_offset += len(df_main) + 4
    sheet.cell(row=row_offset, column=1).value = "Data Services Summary (Resource Count + Size in GB)"
    for r_idx, row in enumerate(dataframe_to_rows(df_data_services, index=False, header=True), start=row_offset + 1):
        for c_idx, value in enumerate(row, start=1):
            sheet.cell(row=r_idx, column=c_idx).value = value

print("Report successfully written to Excel: aws_cost_full_summary.xlsx")






----------
# aws_cost_full_summary.py

import pandas as pd
import subprocess
import io
import json
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.styles import Font

excel_path = "aws_cost_full_summary.xlsx"

# ----------------------
# Utility: Run SQL Query
# ----------------------
def run_steampipe_query(sql):
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except subprocess.CalledProcessError as e:
        print(f"\u274c Steampipe error: {e.stderr.decode()}")
        return pd.DataFrame()
    except pd.errors.EmptyDataError:
        print("\u26a0\ufe0f Empty CSV from Steampipe.")
        return pd.DataFrame()

# ----------------------
# 1. Total Monthly Cost Trend
# ----------------------
print("\n\u2b06\ufe0f Total Account Cost Trend...")
steampipe_trend_sql = """
SELECT
  TO_CHAR(period_start, 'YYYY-MM') AS month,
  SUM(unblended_cost_amount)::numeric::money AS total_cost
FROM
  aws_cost_by_service_monthly
WHERE
  period_start IN (
    DATE '2025-01-01',
    DATE '2025-02-01',
    DATE '2025-03-01'
  )
GROUP BY
  period_start
ORDER BY
  period_start;
"""
df_trend = run_steampipe_query(steampipe_trend_sql)
df_trend["total_cost"] = df_trend["total_cost"].replace('[\$,]', '', regex=True).astype(float)
df_trend["Trend vs Prev Month"] = df_trend["total_cost"].diff().fillna(0).apply(lambda x: f"${x:,.2f}")
df_trend["% Change"] = df_trend["total_cost"].pct_change().fillna(0).apply(lambda x: f"{x*100:.2f}%")
df_trend["total_cost"] = df_trend["total_cost"].apply(lambda x: f"${x:,.2f}")

# ----------------------
# 2. Top 10 Services with Top 5 Usage Types
# ----------------------
print("\n\ud83d\udd04 Service Usage Breakdown...")
steampipe_main_sql = """
WITH active_services AS (
  SELECT service, SUM(unblended_cost_amount) AS total_cost
  FROM aws_cost_by_service_monthly
  WHERE period_start = DATE '2025-03-01'
  GROUP BY service
  ORDER BY total_cost DESC
  LIMIT 10
),
ranked_usage_types AS (
  SELECT s.service, s.usage_type, SUM(s.unblended_cost_amount) AS usage_cost,
         RANK() OVER (PARTITION BY s.service ORDER BY SUM(s.unblended_cost_amount) DESC) AS usage_rank
  FROM aws_cost_by_service_usage_type_monthly s
  WHERE s.period_start = DATE '2025-03-01'
  GROUP BY s.service, s.usage_type
)
SELECT a.service,
       a.total_cost::numeric::money AS total_service_cost,
       u.usage_type,
       u.usage_cost::numeric::money AS usage_type_cost
FROM active_services a
JOIN ranked_usage_types u ON a.service = u.service
WHERE u.usage_rank <= 5
ORDER BY a.total_cost DESC, u.usage_cost DESC;
"""
df = run_steampipe_query(steampipe_main_sql)
df["total_service_cost"] = df["total_service_cost"].replace('[\$,]', '', regex=True).astype(float)
df["usage_type_cost"] = df["usage_type_cost"].replace('[\$,]', '', regex=True).astype(float)
df["usage_%_of_service"] = ((df["usage_type_cost"] / df["total_service_cost"]) * 100).round(2)
df["total_service_cost_str"] = df["total_service_cost"].apply(lambda x: f"${x:,.2f}")
df["usage_type_cost_str"] = df["usage_type_cost"].apply(lambda x: f"${x:,.2f}")
df["usage_%_of_service_str"] = df["usage_%_of_service"].apply(lambda x: f"{x:.2f}%")
df["service_display"] = df["service"]
df["total_service_cost_display"] = df["total_service_cost_str"]
df.loc[df["service_display"].duplicated(), "service_display"] = ""
df.loc[df["total_service_cost_display"].duplicated(), "total_service_cost_display"] = ""
df_main = df[["service_display", "total_service_cost_display", "usage_type", "usage_type_cost_str", "usage_%_of_service_str"]]
df_main.columns = ["Service", "Total Service Cost", "Usage Type", "Usage Type Cost", "% of Service Cost"]

# ----------------------
# 3. Data Services Summary (EBS, RDS, DynamoDB, S3)
# ----------------------
print("\n\ud83d\udcca Data Service Metrics...")
# Fetch all S3 bucket names
bucket_query = "SELECT name FROM aws_s3_bucket;"
df_buckets = run_steampipe_query(bucket_query)
bucket_names = df_buckets["name"].tolist()
s3_resource_count = len(bucket_names)
bucket_sizes = []

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT value AS size_bytes, timestamp
    FROM aws_cloudwatch_metric_data_point
    WHERE id = 'e1' AND metric_stat = '{json.dumps(metric_stat)}'
    ORDER BY timestamp DESC
    LIMIT 1;
    """
    df_metric = run_steampipe_query(sql)
    if not df_metric.empty and "size_bytes" in df_metric.columns:
        size_bytes = df_metric.iloc[0]["size_bytes"]
        size_gb = round(size_bytes / 1024 / 1024 / 1024, 2)
        bucket_sizes.append(size_gb)

s3_total_gb = round(sum(bucket_sizes), 2)

steampipe_sql = """
WITH ebs AS (
  SELECT 'EBS' AS service, COUNT(*) AS resource_count, SUM(size) AS total_gb
  FROM aws_ebs_volume
),
rds AS (
  SELECT 'RDS' AS service, COUNT(*) AS resource_count, SUM(allocated_storage) AS total_gb
  FROM aws_rds_db_cluster
),
dynamodb AS (
  SELECT 'DynamoDB' AS service,
         COUNT(*) AS resource_count,
         ROUND(SUM(table_size_bytes) FILTER (WHERE table_size_bytes > 0) / 1024 / 1024 / 1024, 2) AS total_gb
  FROM aws_dynamodb_table
  WHERE table_size_bytes IS NOT NULL
)
SELECT * FROM ebs
UNION ALL
SELECT * FROM rds
UNION ALL
SELECT * FROM dynamodb;
"""
df_others = run_steampipe_query(steampipe_sql)
df_others.columns = ["Service", "Resource Count", "Total Size (GB)"]
df_s3_row = pd.DataFrame([{
    "Service": "S3",
    "Resource Count": s3_resource_count,
    "Total Size (GB)": s3_total_gb
}])
df_data_services = pd.concat([df_others, df_s3_row], ignore_index=True)

# ----------------------
# 4. Write all to Excel
# ----------------------
print("\n\ud83d\udcc4 Writing final Excel report...")
with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
    df_trend.to_excel(writer, sheet_name="Cost Summary", startrow=1, index=False)
    workbook = writer.book
    sheet = writer.sheets["Cost Summary"]
    sheet.cell(row=1, column=1).value = "\u2b06\ufe0f Total AWS Account Cost Trend"

    row_offset = len(df_trend) + 4
    sheet.cell(row=row_offset, column=1).value = "\ud83d\udd04 Top 10 Services (with Top 5 Usage Types)"
    for r_idx, row in enumerate(dataframe_to_rows(df_main, index=False, header=True), start=row_offset + 1):
        for c_idx, value in enumerate(row, start=1):
            cell = sheet.cell(row=r_idx, column=c_idx, value=value)
            if r_idx == row_offset + 1:
                cell.font = Font(bold=True)

    row_offset += len(df_main) + 4
    sheet.cell(row=row_offset, column=1).value = "\ud83d\udcca Data Services Summary (Resource Count + Size in GB)"
    for r_idx, row in enumerate(dataframe_to_rows(df_data_services, index=False, header=True), start=row_offset + 1):
        for c_idx, value in enumerate(row, start=1):
            cell = sheet.cell(row=r_idx, column=c_idx, value=value)
            if r_idx == row_offset + 1:
                cell.font = Font(bold=True)

print("\n‚úÖ All sections successfully written to Excel: aws_cost_full_summary.xlsx")














-------------------

import pandas as pd
import subprocess
import io
import json
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl import load_workbook

excel_path = "aws_cost_report_grouped_with_trend.xlsx"

# ----------------------
# Utility: Run SQL Query
# ----------------------
def run_steampipe_query(sql):
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Steampipe error: {e.stderr.decode()}")
        return pd.DataFrame()
    except pd.errors.EmptyDataError:
        print("‚ö†Ô∏è Empty CSV from Steampipe.")
        return pd.DataFrame()

# ----------------------
# Step 1: Get S3 Bucket Sizes
# ----------------------
print("üîç Scanning S3 bucket sizes...")

bucket_query = "SELECT name FROM aws_s3_bucket;"
df_buckets = run_steampipe_query(bucket_query)
bucket_names = df_buckets["name"].tolist()

bucket_sizes = []

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT
      value AS size_bytes,
      timestamp
    FROM
      aws_cloudwatch_metric_data_point
    WHERE
      id = 'e1'
      AND metric_stat = '{json.dumps(metric_stat)}'
    ORDER BY
      timestamp DESC
    LIMIT 1;
    """

    df_metric = run_steampipe_query(sql)
    if df_metric.empty or "size_bytes" not in df_metric.columns:
        print(f"‚ö†Ô∏è No metric for bucket: {bucket}")
        continue

    size_bytes = df_metric.iloc[0]["size_bytes"]
    size_gb = round(size_bytes / 1024 / 1024 / 1024, 2)
    bucket_sizes.append({"bucket_name": bucket, "size_gb": size_gb})

df_s3 = pd.DataFrame(bucket_sizes)
s3_resource_count = len(df_s3)
s3_total_gb = df_s3["size_gb"].sum()

print(f"‚úÖ S3 summary: {s3_resource_count} buckets, {s3_total_gb:.2f} GB total")

# ----------------------
# Step 2: Query for EBS, RDS, DynamoDB
# ----------------------
steampipe_sql = """
WITH ebs AS (
  SELECT 'EBS' AS service, COUNT(*) AS resource_count, SUM(size) AS total_gb
  FROM aws_ebs_volume
),
rds AS (
  SELECT 'RDS' AS service, COUNT(*) AS resource_count, SUM(allocated_storage) AS total_gb
  FROM aws_rds_db_cluster
),
dynamodb AS (
  SELECT 'DynamoDB' AS service,
         COUNT(*) AS resource_count,
         SUM(table_size_bytes) / 1024 / 1024 / 1024 AS total_gb
  FROM aws_dynamodb_table
)
SELECT * FROM ebs
UNION ALL
SELECT * FROM rds
UNION ALL
SELECT * FROM dynamodb;
"""

df_others = run_steampipe_query(steampipe_sql)
df_others.columns = ["Service", "Resource Count", "Total Size (GB)"]

# ----------------------
# Step 3: Append S3 Info to Table
# ----------------------
df_s3_row = pd.DataFrame([{
    "Service": "S3",
    "Resource Count": s3_resource_count,
    "Total Size (GB)": round(s3_total_gb, 2)
}])

df_data_services = pd.concat([df_others, df_s3_row], ignore_index=True)

# ----------------------
# Step 4: Export to Excel (same sheet)
# ----------------------
print("üìä Appending Data Services Summary to Excel...")

with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
    workbook = writer.book
    sheet = writer.sheets["Cost Summary"]

    # Find where to place this table
    existing_rows = sheet.max_row
    start_row = existing_rows + 3

    sheet.cell(row=start_row, column=1).value = "üì¶ Data Services Summary (Resource Count + Total GB)"
    for r_idx, row in enumerate(dataframe_to_rows(df_data_services, index=False, header=True), start=start_row + 1):
        for c_idx, value in enumerate(row, start=1):
            sheet.cell(row=r_idx, column=c_idx).value = value

print("‚úÖ Data Services Summary added to Excel.")





------------
WITH ebs AS (
  SELECT 'EBS' AS service, COUNT(*) AS resource_count, SUM(size) AS total_gb
  FROM aws_ebs_volume
),
rds AS (
  SELECT 'RDS' AS service, COUNT(*) AS resource_count, SUM(allocated_storage) AS total_gb
  FROM aws_rds_db_cluster
),
dynamodb AS (
  SELECT 'DynamoDB' AS service,
         COUNT(*) AS resource_count,
         ROUND(SUM(table_size_bytes) / 1024 / 1024 / 1024, 2) AS total_gb
  FROM aws_dynamodb_table
),
s3 AS (
  SELECT 'S3' AS service,
         COUNT(DISTINCT bucket_name) AS resource_count,
         ROUND(SUM(storage_bytes) / 1024 / 1024 / 1024, 2) AS total_gb
  FROM aws_s3_bucket_metric
)
SELECT * FROM ebs
UNION ALL
SELECT * FROM rds
UNION ALL
SELECT * FROM dynamodb
UNION ALL
SELECT * FROM s3;



-------
import pandas as pd
import subprocess
import io
import json

def run_steampipe_query(sql):
    """Run a Steampipe SQL query and return as DataFrame, with safe fallback."""
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        output = proc.stdout.decode()
        if not output.strip():
            print("‚ö†Ô∏è Steampipe query returned no output.")
            return pd.DataFrame()
        return pd.read_csv(io.StringIO(output))
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Steampipe error: {e.stderr.decode()}")
        return pd.DataFrame()
    except pd.errors.EmptyDataError:
        print("‚ö†Ô∏è Empty CSV result from Steampipe.")
        return pd.DataFrame()

# 1Ô∏è‚É£ Get all bucket names
bucket_query = "SELECT name FROM aws_s3_bucket;"
df_buckets = run_steampipe_query(bucket_query)
bucket_names = df_buckets["name"].tolist()

print(f"üîç Found {len(bucket_names)} buckets to scan...\n")

# 2Ô∏è‚É£ Loop through each bucket and collect size metrics
bucket_sizes = []

for bucket in bucket_names:
    print(f"üì¶ Checking bucket: {bucket}")

    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT
      value AS size_bytes,
      timestamp
    FROM
      aws_cloudwatch_metric_data_point
    WHERE
      metric_stat = '{json.dumps(metric_stat)}'
    ORDER BY
      timestamp DESC
    LIMIT 1;
    """

    df_metric = run_steampipe_query(sql)

    if df_metric.empty:
        print(f"‚ö†Ô∏è No metric returned for: {bucket}")
        continue

    if "size_bytes" not in df_metric.columns:
        print(f"‚ùó Missing 'size_bytes' column for: {bucket}")
        continue

    size_bytes = df_metric.iloc[0]["size_bytes"]
    timestamp = df_metric.iloc[0]["timestamp"]
    size_gb = round(size_bytes / 1024 / 1024 / 1024, 2)

    print(f"   ‚úÖ Size: {size_gb} GB @ {timestamp}")

    bucket_sizes.append({
        "bucket_name": bucket,
        "timestamp": timestamp,
        "size_gb": size_gb
    })

# 3Ô∏è‚É£ Build final DataFrame
df_sizes = pd.DataFrame(bucket_sizes)

if df_sizes.empty:
    print("‚ùå No bucket sizes could be retrieved. Exiting.")
    exit()

df_sizes.sort_values(by="size_gb", ascending=False, inplace=True)

# 4Ô∏è‚É£ Show top 5
print("\nüìä Top 5 S3 Buckets by Size:")
print(df_sizes.head(5).to_string(index=False))

# 5Ô∏è‚É£ Total
total_size = df_sizes["size_gb"].sum()
print(f"\nüßÆ Total Tracked S3 Storage: {total_size:,.2f} GB across {len(df_sizes)} buckets")

# 6Ô∏è‚É£ Save to CSV
df_sizes.to_csv("s3_bucket_sizes.csv", index=False)
print("\n‚úÖ Report saved to: s3_bucket_sizes.csv")





--------------
import pandas as pd
import subprocess
import io
import json

def run_steampipe_query(sql):
    """Execute a Steampipe SQL query and return as DataFrame."""
    try:
        proc = subprocess.run(
            ["steampipe", "query", "--output", "csv"],
            input=sql.encode(),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        return pd.read_csv(io.StringIO(proc.stdout.decode()))
    except subprocess.CalledProcessError as e:
        print(f"Error running query:\n{e.stderr.decode()}")
        return pd.DataFrame()

# 1Ô∏è‚É£ Get list of all S3 buckets
bucket_query = "SELECT name FROM aws_s3_bucket;"
df_buckets = run_steampipe_query(bucket_query)
bucket_names = df_buckets["name"].tolist()

# 2Ô∏è‚É£ Collect bucket size metrics
bucket_sizes = []

for bucket in bucket_names:
    metric_stat = {
        "Metric": {
            "Namespace": "AWS/S3",
            "MetricName": "BucketSizeBytes",
            "Dimensions": [
                {"Name": "BucketName", "Value": bucket},
                {"Name": "StorageType", "Value": "StandardStorage"}
            ]
        },
        "Stat": "Maximum"
    }

    sql = f"""
    SELECT
      value AS size_bytes,
      ROUND(value / 1024 / 1024 / 1024, 2) AS size_gb,
      timestamp
    FROM
      aws_cloudwatch_metric_data_point
    WHERE
      metric_stat = '{json.dumps(metric_stat)}'
    ORDER BY
      timestamp DESC
    LIMIT 1;
    """

    df_metric = run_steampipe_query(sql)
    if not df_metric.empty:
        size_gb = df_metric.iloc[0]["size_gb"]
        bucket_sizes.append({
            "bucket_name": bucket,
            "size_gb": size_gb,
            "timestamp": df_metric.iloc[0]["timestamp"]
        })

# 3Ô∏è‚É£ Create final DataFrame
df_sizes = pd.DataFrame(bucket_sizes)
df_sizes.sort_values(by="size_gb", ascending=False, inplace=True)

# 4Ô∏è‚É£ Total and Top 5 buckets
total_size = df_sizes["size_gb"].sum()
top5 = df_sizes.head(5)

print("\nüì¶ Top 5 Largest S3 Buckets:")
print(top5.to_string(index=False))

print(f"\nüßÆ Total S3 Usage: {total_size:,.2f} GB across {len(df_sizes)} buckets")

# 5Ô∏è‚É£ Optionally export to CSV or Excel
df_sizes.to_csv("s3_bucket_sizes.csv", index=False)
print("‚úÖ Full bucket size data saved to s3_bucket_sizes.csv")




----------
SELECT
  dimensions ->> 'BucketName' AS bucket_name,
  dimensions ->> 'StorageType' AS storage_type,
  MAX(timestamp) AS latest_data_ts,
  MAX(average) AS size_bytes,
  ROUND(MAX(average) / 1024 / 1024 / 1024, 2) AS size_gb
FROM
  aws_cloudwatch_metric_data_point
WHERE
  namespace = 'AWS/S3'
  AND metric_name = 'BucketSizeBytes'
  AND dimensions ->> 'StorageType' = 'StandardStorage'
GROUP BY
  dimensions ->> 'BucketName', dimensions ->> 'StorageType'
ORDER BY
  size_bytes DESC;


---
import pandas as pd
import subprocess
import io
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows

# 1Ô∏è‚É£ MAIN SERVICE USAGE QUERY
steampipe_main_sql = """
WITH active_services AS (
  SELECT
    service,
    SUM(unblended_cost_amount) AS total_cost
  FROM
    aws_cost_by_service_monthly
  WHERE
    period_start = DATE '2025-03-01'
  GROUP BY
    service
  ORDER BY
    total_cost DESC
  LIMIT 10
),
ranked_usage_types AS (
  SELECT
    s.service,
    s.usage_type,
    SUM(s.unblended_cost_amount) AS usage_cost,
    RANK() OVER (
      PARTITION BY s.service
      ORDER BY SUM(s.unblended_cost_amount) DESC
    ) AS usage_rank
  FROM
    aws_cost_by_service_usage_type_monthly s
  WHERE
    s.period_start = DATE '2025-03-01'
  GROUP BY
    s.service, s.usage_type
)
SELECT
  a.service,
  a.total_cost::numeric::money AS total_service_cost,
  u.usage_type,
  u.usage_cost::numeric::money AS usage_type_cost
FROM
  active_services a
JOIN
  ranked_usage_types u ON a.service = u.service
WHERE
  u.usage_rank <= 5
ORDER BY
  a.total_cost DESC, u.usage_cost DESC;
"""

# 2Ô∏è‚É£ MONTHLY TREND QUERY
steampipe_trend_sql = """
SELECT
  TO_CHAR(period_start, 'YYYY-MM') AS month,
  SUM(unblended_cost_amount)::numeric::money AS total_cost
FROM
  aws_cost_by_service_monthly
WHERE
  period_start IN (
    DATE '2025-01-01',
    DATE '2025-02-01',
    DATE '2025-03-01'
  )
GROUP BY
  period_start
ORDER BY
  period_start;
"""

# Function to run Steampipe SQL and return as DataFrame
def run_steampipe_query(sql):
    proc = subprocess.run(
        ["steampipe", "query", "--output", "csv"],
        input=sql.encode(),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        check=True
    )
    return pd.read_csv(io.StringIO(proc.stdout.decode()))

# üü¶ 1. Run Main Query
df = run_steampipe_query(steampipe_main_sql)

# Clean and format
df["total_service_cost"] = df["total_service_cost"].replace('[\$,]', '', regex=True).astype(float)
df["usage_type_cost"] = df["usage_type_cost"].replace('[\$,]', '', regex=True).astype(float)
df["usage_%_of_service"] = ((df["usage_type_cost"] / df["total_service_cost"]) * 100).round(2)
df["total_service_cost_str"] = df["total_service_cost"].apply(lambda x: f"${x:,.2f}")
df["usage_type_cost_str"] = df["usage_type_cost"].apply(lambda x: f"${x:,.2f}")
df["usage_%_of_service_str"] = df["usage_%_of_service"].apply(lambda x: f"{x:.2f}%")
df["service_display"] = df["service"]
df["total_service_cost_display"] = df["total_service_cost_str"]
df.loc[df["service_display"].duplicated(), "service_display"] = ""
df.loc[df["total_service_cost_display"].duplicated(), "total_service_cost_display"] = ""

df_main = df[[
    "service_display",
    "total_service_cost_display",
    "usage_type",
    "usage_type_cost_str",
    "usage_%_of_service_str"
]]
df_main.columns = ["Service", "Total Service Cost", "Usage Type", "Usage Type Cost", "% of Service Cost"]

# üü® 2. Run Trend Query
df_trend = run_steampipe_query(steampipe_trend_sql)
df_trend["total_cost"] = df_trend["total_cost"].replace('[\$,]', '', regex=True).astype(float)

# Add trend columns
df_trend["Trend vs Prev Month"] = df_trend["total_cost"].diff().fillna(0).apply(lambda x: f"${x:,.2f}")
df_trend["% Change"] = df_trend["total_cost"].pct_change().fillna(0).apply(lambda x: f"{x*100:.2f}%")
df_trend["total_cost"] = df_trend["total_cost"].apply(lambda x: f"${x:,.2f}")

# üü© 3. Export both tables into one Excel sheet
excel_path = "aws_cost_report_grouped_with_trend.xlsx"
with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
    df_main.to_excel(writer, sheet_name="Cost Summary", index=False, startrow=0)
    
    # Load workbook and sheet
    workbook = writer.book
    sheet = writer.sheets["Cost Summary"]

    # Add some spacing then insert trend data
    start_row = len(df_main) + 3
    sheet.cell(row=start_row, column=1).value = "Total Account Cost Trend (Past 3 Months)"
    for r_idx, row in enumerate(dataframe_to_rows(df_trend, index=False, header=True), start=start_row + 1):
        for c_idx, value in enumerate(row, start=1):
            sheet.cell(row=r_idx, column=c_idx).value = value

print(f"‚úÖ Excel file with grouped cost + trend saved: {excel_path}")



--------
import pandas as pd
import subprocess
import io

# SQL: Top 10 services with top 5 usage types per service
steampipe_sql = """
WITH active_services AS (
  SELECT
    service,
    SUM(unblended_cost_amount) AS total_cost
  FROM
    aws_cost_by_service_monthly
  WHERE
    period_start = DATE '2025-03-01'
  GROUP BY
    service
  ORDER BY
    total_cost DESC
  LIMIT 10
),
ranked_usage_types AS (
  SELECT
    s.service,
    s.usage_type,
    SUM(s.unblended_cost_amount) AS usage_cost,
    RANK() OVER (
      PARTITION BY s.service
      ORDER BY SUM(s.unblended_cost_amount) DESC
    ) AS usage_rank
  FROM
    aws_cost_by_service_usage_type_monthly s
  WHERE
    s.period_start = DATE '2025-03-01'
  GROUP BY
    s.service, s.usage_type
)
SELECT
  a.service,
  a.total_cost::numeric::money AS total_service_cost,
  u.usage_type,
  u.usage_cost::numeric::money AS usage_type_cost
FROM
  active_services a
JOIN
  ranked_usage_types u ON a.service = u.service
WHERE
  u.usage_rank <= 5
ORDER BY
  a.total_cost DESC, u.usage_cost DESC;
"""

# Run steampipe query
print("Running Steampipe top 10 services usage breakdown query...")
proc = subprocess.run(
    ["steampipe", "query", "--output", "csv"],
    input=steampipe_sql.encode(),
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    check=True
)

# Load CSV into DataFrame
df = pd.read_csv(io.StringIO(proc.stdout.decode()))
print("Query result loaded into DataFrame.")

# Clean up cost columns
df["total_service_cost"] = df["total_service_cost"].replace('[\$,]', '', regex=True).astype(float)
df["usage_type_cost"] = df["usage_type_cost"].replace('[\$,]', '', regex=True).astype(float)

# Add percentage of usage type within its service
df["usage_%_of_service"] = ((df["usage_type_cost"] / df["total_service_cost"]) * 100).round(2)

# Format costs and percentages as strings
df["total_service_cost_str"] = df["total_service_cost"].apply(lambda x: f"${x:,.2f}")
df["usage_type_cost_str"] = df["usage_type_cost"].apply(lambda x: f"${x:,.2f}")
df["usage_%_of_service_str"] = df["usage_%_of_service"].apply(lambda x: f"{x:.2f}%")

# Blank out repeated service rows for aesthetic output
df["service_display"] = df["service"]
df["total_service_cost_display"] = df["total_service_cost_str"]
df.loc[df["service_display"].duplicated(), "service_display"] = ""
df.loc[df["total_service_cost_display"].duplicated(), "total_service_cost_display"] = ""

# Final clean dataframe for Excel
df_final = df[[
    "service_display",
    "total_service_cost_display",
    "usage_type",
    "usage_type_cost_str",
    "usage_%_of_service_str"
]]
df_final.columns = ["Service", "Total Service Cost", "Usage Type", "Usage Type Cost", "% of Service Cost"]

# Export to Excel
excel_path = "top10_service_cost_summary_grouped.xlsx"
df_final.to_excel(excel_path, index=False, engine='openpyxl')
print(f"‚úÖ Excel file saved successfully: {excel_path}")


----
WITH active_services AS (
  SELECT
    service,
    SUM(unblended_cost_amount) AS total_cost
  FROM
    aws_cost_by_service_monthly
  WHERE
    period_start = DATE '2025-03-01'
  GROUP BY
    service
),
service_usage_ranked AS (
  SELECT
    s.service,
    s.usage_type,
    SUM(s.unblended_cost_amount) AS usage_cost,
    RANK() OVER (
      PARTITION BY s.service
      ORDER BY SUM(s.unblended_cost_amount) DESC
    ) AS usage_rank
  FROM
    aws_cost_by_service_usage_type_monthly s
  WHERE
    s.period_start = DATE '2025-03-01'
  GROUP BY
    s.service, s.usage_type
)
SELECT
  a.service,
  a.total_cost::numeric::money AS total_service_cost,
  u.usage_type,
  u.usage_cost::numeric::money AS usage_type_cost
FROM
  active_services a
JOIN
  service_usage_ranked u
ON
  a.service = u.service
WHERE
  u.usage_rank <= 5
ORDER BY
  a.total_cost DESC, u.usage_cost DESC;


---
SELECT
  service,
  usage_type,
  SUM(unblended_cost_amount)::numeric::money AS cost
FROM
  aws_cost_by_service_usage_type_monthly
WHERE
  period_start = DATE '2025-03-01'
  AND service IN (
    'Amazon CloudWatch',
    'AWSCloudTrail',
    'Amazon GuardDuty',
    'AWS Directory Service',
    'AWS Key Management Service',
    'Amazon Elastic Load Balancing',
    'Amazon DynamoDB',
    'Amazon Simple Storage Service',
    'Amazon Elastic Container Service',
    'Amazon Route 53'
  )
GROUP BY
  service, usage_type
ORDER BY
  cost DESC;


---
import pandas as pd
import subprocess
import io

steampipe_sql = """
-- paste the SQL query above here
"""

print("Running Steampipe VPC cost + count summary query...")
proc = subprocess.run(
    ["steampipe", "query", "--output", "csv"],
    input=steampipe_sql.encode(),
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    check=True
)

df = pd.read_csv(io.StringIO(proc.stdout.decode()))
print("Query result loaded into DataFrame.")

# Convert cost column from money to float
df["cost"] = df["cost"].replace('[\$,]', '', regex=True).astype(float)

# Append total summary row
total_row = {
    "service": "TOTAL",
    "usage_type": "",
    "cost": df["cost"].sum(),
    "total_vpcs": df["total_vpcs"].iloc[0]  # all values are the same
}
df = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)

# Re-format cost for display
df["cost"] = df["cost"].apply(lambda x: f"${x:,.2f}")

# Export to Excel
excel_path = "vpc_summary_report.xlsx"
df.to_excel(excel_path, index=False, engine='openpyxl')
print(f"Excel file saved: {excel_path}")


---
WITH vpc_count AS (
  SELECT COUNT(*) AS total_vpcs
  FROM aws_vpc
),
vpc_related_costs AS (
  SELECT
    service,
    usage_type,
    SUM(unblended_cost_amount)::numeric::money AS cost
  FROM
    aws_cost_by_service_usage_type_monthly
  WHERE
    period_start = DATE '2025-03-01'
    AND (
      service = 'Amazon Virtual Private Cloud' OR
      usage_type ILIKE '%NatGateway%' OR
      usage_type ILIKE '%VPN%' OR
      usage_type ILIKE '%DataTransfer%' OR
      usage_type ILIKE '%ElasticIP%' OR
      usage_type ILIKE '%VPC%'
    )
  GROUP BY
    service, usage_type
)
SELECT
  c.service,
  c.usage_type,
  c.cost,
  v.total_vpcs
FROM
  vpc_related_costs c
  CROSS JOIN vpc_count v
ORDER BY
  c.cost DESC;



-----
import pandas as pd
import subprocess
import io

# Define the Steampipe SQL query
steampipe_sql = """
WITH ec2_other_cost AS (
  SELECT
    usage_type,
    SUM(unblended_cost_amount)::numeric::money AS unblended_cost
  FROM
    aws_cost_by_service_usage_type_monthly
  WHERE
    service = 'Amazon Elastic Compute Cloud'
    AND date_trunc('month', period_start) = DATE '2025-03-01'
    AND usage_type NOT ILIKE '%BoxUsage%'
  GROUP BY
    usage_type
),
ebs_sizes AS (
  SELECT
    volume_type,
    ROUND(SUM(size)) AS total_volume_gb
  FROM
    aws_ebs_volume
  WHERE
    state = 'in-use'
  GROUP BY
    volume_type
),
top5 AS (
  SELECT
    c.usage_type,
    c.unblended_cost,
    COALESCE(s.total_volume_gb::text, 'NA') AS total_volume_gb
  FROM
    ec2_other_cost c
    LEFT JOIN ebs_sizes s ON (
      (c.usage_type ILIKE '%gp3%' AND s.volume_type = 'gp3') OR
      (c.usage_type ILIKE '%gp2%' AND s.volume_type = 'gp2') OR
      (c.usage_type ILIKE '%io1%' AND s.volume_type = 'io1') OR
      (c.usage_type ILIKE '%io2%' AND s.volume_type = 'io2') OR
      (c.usage_type ILIKE '%st1%' AND s.volume_type = 'st1') OR
      (c.usage_type ILIKE '%sc1%' AND s.volume_type = 'sc1')
    )
  ORDER BY
    c.unblended_cost DESC
  LIMIT 5
)
SELECT
  usage_type,
  unblended_cost,
  total_volume_gb
FROM
  top5;
"""

# Run steampipe query
print("Running Steampipe query...")
proc = subprocess.run(
    ["steampipe", "query", "--output", "csv"],
    input=steampipe_sql.encode(),
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    check=True
)

# Read CSV into DataFrame
df = pd.read_csv(io.StringIO(proc.stdout.decode()))
print("Query result loaded into DataFrame.")

# Convert 'unblended_cost' from money to numeric
df["unblended_cost"] = df["unblended_cost"].replace('[\$,]', '', regex=True).astype(float)

# Convert 'total_volume_gb' to numeric (optional, NA stays)
df["total_volume_gb"] = pd.to_numeric(df["total_volume_gb"], errors='coerce')

# Append TOTAL row
total_row = {
    "usage_type": "TOTAL (Top 5)",
    "unblended_cost": df["unblended_cost"].sum(),
    "total_volume_gb": df["total_volume_gb"].sum(skipna=True)
}
df = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)

# Format NA again
df["total_volume_gb"] = df["total_volume_gb"].fillna("NA")

# Export to Excel
excel_path = "ec2_other_cost_report.xlsx"
df.to_excel(excel_path, index=False, engine='openpyxl')
print(f"Excel file saved: {excel_path}")


---
WITH ec2_other_cost AS (
  SELECT
    usage_type,
    SUM(unblended_cost_amount)::numeric::money AS unblended_cost
  FROM
    aws_cost_by_service_usage_type_monthly
  WHERE
    service = 'Amazon Elastic Compute Cloud'
    AND date_trunc('month', period_start) = DATE '2025-03-01'
    AND usage_type NOT ILIKE '%BoxUsage%'  -- exclude EC2 compute
  GROUP BY
    usage_type
),
ebs_sizes AS (
  SELECT
    volume_type,
    ROUND(SUM(size)) AS total_volume_gb
  FROM
    aws_ebs_volume
  WHERE
    state = 'in-use'
  GROUP BY
    volume_type
)
SELECT
  c.usage_type,
  c.unblended_cost,
  COALESCE(s.total_volume_gb::text, 'NA') AS total_volume_gb
FROM
  ec2_other_cost c
  LEFT JOIN ebs_sizes s
    ON (
      (c.usage_type ILIKE '%gp3%' AND s.volume_type = 'gp3') OR
      (c.usage_type ILIKE '%gp2%' AND s.volume_type = 'gp2') OR
      (c.usage_type ILIKE '%io1%' AND s.volume_type = 'io1') OR
      (c.usage_type ILIKE '%io2%' AND s.volume_type = 'io2') OR
      (c.usage_type ILIKE '%st1%' AND s.volume_type = 'st1') OR
      (c.usage_type ILIKE '%sc1%' AND s.volume_type = 'sc1')
    )
ORDER BY
  c.unblended_cost DESC
LIMIT 5;




-----
WITH ec2_other_cost AS (
  SELECT
    usage_type,
    SUM(unblended_cost_amount)::numeric::money AS unblended_cost
  FROM
    aws_cost_by_service_usage_type_monthly
  WHERE
    service = 'Amazon Elastic Compute Cloud'
    AND date_trunc('month', period_start) = DATE '2025-03-01'
    AND usage_type NOT ILIKE '%BoxUsage%'  -- exclude EC2 compute
  GROUP BY
    usage_type
),
ebs_sizes AS (
  SELECT
    volume_type,
    ROUND(SUM(size)) AS total_volume_gb
  FROM
    aws_ebs_volume
  WHERE
    state = 'in-use'
  GROUP BY
    volume_type
)
SELECT
  c.usage_type,
  c.unblended_cost,
  s.total_volume_gb
FROM
  ec2_other_cost c
  LEFT JOIN ebs_sizes s
    ON (
      (c.usage_type ILIKE '%gp3%' AND s.volume_type = 'gp3') OR
      (c.usage_type ILIKE '%gp2%' AND s.volume_type = 'gp2') OR
      (c.usage_type ILIKE '%io1%' AND s.volume_type = 'io1') OR
      (c.usage_type ILIKE '%io2%' AND s.volume_type = 'io2') OR
      (c.usage_type ILIKE '%st1%' AND s.volume_type = 'st1') OR
      (c.usage_type ILIKE '%sc1%' AND s.volume_type = 'sc1')
    )
ORDER BY
  c.unblended_cost DESC;



------------
WITH ec2_other_cost AS (
  SELECT
    usage_type,
    SUM(unblended_cost_amount)::numeric::money AS unblended_cost
  FROM
    aws_cost_by_service_usage_type_monthly
  WHERE
    service = 'Amazon Elastic Compute Cloud'
    AND date_trunc('month', period_start) = DATE '2025-03-01'
    AND usage_type NOT ILIKE '%BoxUsage%'  -- exclude EC2 compute
  GROUP BY
    usage_type
),
ebs_sizes AS (
  SELECT
    'EBS:' || volume_type AS usage_type_key,
    ROUND(SUM(size)) AS total_volume_gb
  FROM
    aws_ebs_volume
  GROUP BY
    volume_type
)
SELECT
  e.usage_type,
  e.unblended_cost,
  s.total_volume_gb
FROM
  ec2_other_cost e
  LEFT JOIN ebs_sizes s
    ON e.usage_type ILIKE s.usage_type_key || '%'
ORDER BY
  e.unblended_cost DESC;


---
SELECT
  service,
  usage_type,
  period_start,
  unblended_cost_amount::numeric::money AS unblended_cost,
  amortized_cost_amount::numeric::money AS amortized_cost
FROM
  aws_cost_by_service_usage_type_monthly
WHERE
  service = 'Amazon Elastic Compute Cloud'
  AND date_trunc('month', period_start) = DATE '2025-03-01'
  AND usage_type NOT ILIKE '%BoxUsage%'  -- removes EC2-Instances compute cost
ORDER BY
  unblended_cost_amount DESC;


---
SELECT
  usage_type,
  product_code,
  SUM(unblended_cost_amount)::numeric::money AS cost_usd
FROM
  aws_cost_usage_monthly
WHERE
  line_item_product_code = 'AmazonEC2'
  AND date_trunc('month', usage_start_date) = DATE '2025-03-01'
  AND usage_type NOT ILIKE '%BoxUsage%'  -- exclude EC2 compute
GROUP BY
  usage_type, product_code
ORDER BY
  cost_usd DESC;


---
SELECT DISTINCT service
FROM aws_cost_by_service_monthly
WHERE date_trunc('month', period_start) = DATE '2025-03-01'
ORDER BY service;



======
WITH ec2_instance_count AS (
  SELECT
    COUNT(*) AS resource_count
  FROM
    aws_ec2_instance
),
march_ec2_cost AS (
  SELECT
    SUM(unblended_cost_amount)::numeric::money AS march_2025_cost
  FROM
    aws_cost_by_service_monthly
  WHERE
    service IN (
      'Amazon Elastic Compute Cloud',
      'Amazon Elastic Compute Cloud - Compute',
      'Amazon Elastic Compute Cloud - Other',
      'EC2-Instances',
      'EC2-Other'
    )
    AND date_trunc('month', period_start) = DATE '2025-03-01'
)
SELECT
  'Elastic Compute Cloud provides resizable compute capacity in the cloud.' AS description,
  i.resource_count,
  c.march_2025_cost AS cost_march_2025,
  'N/A (Compute instances don‚Äôt directly track data usage in this context)' AS data_used
FROM
  ec2_instance_count i
  CROSS JOIN march_ec2_cost c;



----
Steampipe SQL Queries (Save these as .sql files):

Create separate .sql files for each AWS service to retrieve the necessary information.

ec2.sql:

SQL
select
  count(*) as instance_count
from aws_ec2_instance;
 s3.sql:

SQL
select
  count(*) as bucket_count,
  sum(size) as total_size_bytes
from aws_s3_bucket;
 secretsmanager.sql:

SQL
select
  count(*) as secret_count
from aws_secretsmanager_secret;
 dynamodb.sql:

SQL
select
  count(*) as table_count,
  sum(table_size_bytes) as total_size_bytes
from aws_dynamodb_table;
 rds.sql:

SQL
select
  count(*) as instance_count,
  sum(allocated_storage * 1024 * 1024 * 1024) as total_allocated_storage_bytes
from aws_rds_db_instance;
 vpc.sql:

SQL
select
  count(*) as vpc_count
from aws_vpc;
 subnet.sql:

SQL
select
  count(*) as subnet_count
from aws_subnet;
 elb.sql (Classic ELB):

SQL
select
  count(*) as elb_count
from aws_elb;
 elbv2.sql (Application/Network ELB):

SQL
select
  count(*) as alb_nlb_count
from aws_elbv2_load_balancer;
 ebs.sql:

SQL
select
  count(*) as volume_count,
  sum(size * 1024 * 1024 * 1024) as total_size_bytes
from aws_ebs_volume;
 iam_user.sql:

SQL
select
  count(*) as user_count
from aws_iam_user;
 iam_role.sql:

SQL
select
  count(*) as role_count
from aws_iam_role;
 kms.sql:

SQL
select
  count(*) as key_count
from aws_kms_key;
 cloudwatch_alarms.sql:

SQL
select
  count(*) as alarm_count
from aws_cloudwatch_alarm;
 cloudformation.sql:

SQL
select
  count(*) as stack_count
from aws_cloudformation_stack
where stack_status not in ('DELETE_COMPLETE');
 lambda.sql:

SQL
select
  count(*) as function_count
from aws_lambda_function;
 sns.sql:

SQL
select
  count(*) as topic_count
from aws_sns_topic;
 sqs.sql:

SQL
select
  count(*) as queue_count
from aws_sqs_queue;
 2. Python Script to Execute Steampipe and Process Data:

Python
import subprocess
import json

def execute_steampipe(query_file):
    """Executes a Steampipe SQL query and returns the result as a dictionary."""
    try:
        result = subprocess.run(
            ["steampipe", "query", "-f", query_file, "--output", "json"],
            capture_output=True,
            text=True,
            check=True  # Raise an exception for non-zero exit codes
        )
        data = json.loads(result.stdout)
        if data:
            return data[0]
        else:
            return {}
    except subprocess.CalledProcessError as e:
        print(f"Error executing Steampipe query for {query_file}: {e}")
        print(f"Stderr: {e.stderr}")
        return None
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON output for {query_file}: {e}")
        print(f"Stdout: {result.stdout}")
        return None

def get_aws_summary():
    """Retrieves and summarizes information about various AWS services."""
    summary = {}

    # EC2
    ec2_data = execute_steampipe("ec2.sql")
    if ec2_data:
        summary["EC2"] = {
            "description": "Elastic Compute Cloud provides resizable compute capacity in the cloud.",
            "resource_count": ec2_data.get("instance_count", 0),
            "cost": "Cost information is dynamic and requires AWS Cost Explorer or similar tools.",
            "data_used": "N/A (Compute instances don't directly track data usage in this context)"
        }

    # S3
    s3_data = execute_steampipe("s3.sql")
    if s3_data:
        total_size_gb = round(s3_data.get("total_size_bytes", 0) / (1024 ** 3), 2)
        summary["S3"] = {
            "description": "Simple Storage Service offers scalable object storage in the cloud.",
            "resource_count": s3_data.get("bucket_count", 0),
            "cost": "Cost depends on storage class, data transfer, and requests. Use AWS Cost Explorer.",
            "data_used": f"{total_size_gb} GB (approximate total size of all buckets)"
        }

    # Secrets Manager
    secrets_data = execute_steampipe("secretsmanager.sql")
    if secrets_data:
        summary["Secrets Manager"] = {
            "description": "Helps you manage, retrieve, and rotate database credentials, API keys, and other secrets.",
            "resource_count": secrets_data.get("secret_count", 0),
            "cost": "Priced per secret stored and per API request. See AWS pricing.",
            "data_used": "N/A"
        }

    # DynamoDB
    dynamodb_data = execute_steampipe("dynamodb.sql")
    if dynamodb_data:
        total_size_gb = round(dynamodb_data.get("total_size_bytes", 0) / (1024 ** 3), 2)
        summary["DynamoDB"] = {
            "description": "A fully managed NoSQL database service.",
            "resource_count": dynamodb_data.get("table_count", 0),
            "cost": "Based on provisioned/on-demand capacity, storage, and data transfer. Use AWS Cost Explorer.",
            "data_used": f"{total_size_gb} GB (approximate total size of all tables)"
        }

    # RDS
    rds_data = execute_steampipe("rds.sql")
    if rds_data:
        total_allocated_gb = round(rds_data.get("total_allocated_storage_bytes", 0) / (1024 ** 3), 2)
        summary["RDS"] = {
            "description": "Relational Database Service makes it easy to set up, operate, and scale a relational database in the cloud.",
            "resource_count": rds_data.get("instance_count", 0),
            "cost": "Depends on instance type, storage, data transfer, and backups. Use AWS Cost Explorer.",
            "data_used": f"{total_allocated_gb} GB (total allocated storage across all instances)"
        }

    # VPC
    vpc_data = execute_steampipe("vpc.sql")
    if vpc_data:
        summary["VPC"] = {
            "description": "Virtual Private Cloud lets you provision a logically isolated section of the AWS Cloud.",
            "resource_count": vpc_data.get("vpc_count", 0),
            "cost": "Generally no direct cost, but costs can be associated with NAT Gateways, VPNs, etc.",
            "data_used": "N/A"
        }

    # Subnets
    subnet_data = execute_steampipe("subnet.sql")
    if subnet_data:
        summary["Subnets"] = {
            "description": "Subdivisions of a VPC that you can use to group resources based on security and operational needs.",
            "resource_count": subnet_data.get("subnet_count", 0),
            "cost": "No direct cost.",
            "data_used": "N/A"
        }

    # ELB (Handling both Classic and Application/Network ELBs)
    elb_classic_data = execute_steampipe("elb.sql")
    elb_v2_data = execute_steampipe("elbv2.sql")
    elb_count = (elb_classic_data.get("elb_count", 0) if elb_classic_data else 0) + \
                (elb_v2_data.get("alb_nlb_count", 0) if elb_v2_data else 0)
    summary["Elastic Load Balancing (ELB)"] = {
        "description": "Distributes incoming application traffic across multiple targets.",
        "resource_count": elb_count,
        "cost": "Depends on the type of ELB, duration, and data processed. See AWS pricing.",
        "data_used": "N/A"
    }

    # EBS
    ebs_data = execute_steampipe("ebs.sql")
    if ebs_data:
        total_size_gb = round(ebs_data.get("total_size_bytes", 0) / (1024 ** 3), 2)
        summary["EBS"] = {
            "description": "Elastic Block Store provides block-level storage volumes for use with EC2 instances.",
            "resource_count": ebs_data.get("volume_count", 0),
            "cost": "Based on volume type and provisioned IOPS/throughput. See AWS pricing.",
            "data_used": f"{total_size_gb} GB (total provisioned storage across all volumes)"
        }

    # IAM
    iam_user_data = execute_steampipe("iam_user.sql")
    iam_role_data = execute_steampipe("iam_role.sql")
    user_count = iam_user_data.get("user_count", 0) if iam_user_data else 0
    role_count = iam_role_data.get("role_count", 0) if iam_role_data else 0
    summary["IAM"] = {
        "description": "Identity and Access Management enables you to manage access to AWS services and resources securely.",
        "resource_count": f"{user_count} Users, {role_count} Roles",
        "cost": "No additional charge for IAM.",
        "data_used": "N/A"
    }

    # KMS
    kms_data = execute_steampipe("kms.sql")
    if kms_data:
        summary["KMS"] = {
            "description": "Key Management Service makes it easy for you to create and manage keys and control the use of encryption across AWS services.",
            "resource_count": kms_data.get("key_count", 0),
            "cost": "Priced per KMS key and API request. See AWS pricing.",
            "data_used": "N/A"
        }

    # CloudWatch
    cloudwatch_data = execute_steampipe("cloudwatch_alarms.sql")
    if cloudwatch_data:
        summary["CloudWatch"] = {
            "description": "A monitoring and observability service built for DevOps engineers, developers, SREs, and IT managers.",
            "resource_count": f"{cloudwatch_data.get('alarm_count', 0)} Alarms",
            "cost": "Depends on metrics ingested, alarms, logs, and events. See AWS pricing.",
            "data_used": "N/A"
        }

    # CloudFormation
    cloudformation_data = execute_steampipe("cloudformation.sql")
    if cloudformation_data:
        summary["CloudFormation"] = {
            "description": "Provides a way to model, provision, and manage AWS and third-party resources as code.",
            "resource_count": f"{cloudformation_data.get('stack_count', 0)} Stacks",
            "cost": "No additional charge for CloudFormation itself, but you pay for the AWS resources provisioned.",
            "data_used": "N/A"
        }

    # Lambda
    lambda_data = execute_steampipe("lambda.sql")
    if lambda_data:
        summary["Lambda"] = {
            "description": "A serverless, event-driven compute service that lets you run code without provisioning or managing servers.",
            "resource_count": f"{lambda_data.get('function_count', 0)} Functions",
            "cost": "Based on the number of requests and compute duration. See AWS pricing.",
            "data_used": "N/A"
        }

    # SNS
    sns_data = execute_steampipe("sns.sql")
    if sns_data:
        summary["SNS"] = {
            "description": "Simple Notification Service is a highly available, durable, secure, and fully managed messaging service.",
            "resource_count": f"{sns_data.get('topic_count', 0)} Topics",
            "cost": "Based on the number of requests and data transfer. See AWS pricing.",
            "data_used": "N/A"
        }

    # SQS
    sqs_data = execute_steampipe("sqs.sql")
    if sqs_data:
        summary["SQS"] = {
            "description": "Simple Queue Service is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.",
            "resource_count": f"{sqs_data.get('queue_count', 0)} Queues",
            "cost": "Based on the number of requests. See AWS pricing.",
            "data_used": "N/A"
        }

    return summary

def format_summary(summary_data):
    """Formats the summary data into a readable one-page output."""
    output = ""
    for service, details in summary_data.items():
        output += f"## {service}\n\n"
        output += f"{details['description']}\n"
        output += f"- **Resources:** {details['resource_count']}\n"
        output += f"- **Cost:** {details['cost']}\n"
        output += f"- **Data Used:** {details['data_used']}\n\n"
    return output

if __name__ == "__main__":
    aws_summary = get_aws_summary()
    if aws_summary:
        formatted_output = format_summary(aws_summary)
        print(formatted_output)
    else:
        print("Failed to retrieve AWS summary.")
