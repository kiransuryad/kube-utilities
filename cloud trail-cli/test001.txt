

import pandas as pd
from pathlib import Path

cost_df = pd.read_csv("active_services_cost.csv")

csv_files = {
    csv.stem: csv
    for csv in Path(".").glob("*.csv")
    if csv.name != "active_services_cost.csv"
}

# Explicit map to force correct file usage
forced_table_usage = {
    "Amazon Elastic Block Store": "aws_ebs_volume",
    "Amazon DynamoDB": "aws_dynamodb_table"
}

size_fields = {
    "aws_s3_bucket": ("bucket_size_bytes", 1e9),
    "aws_rds_db_instance": ("allocated_storage", 1),
    "aws_ebs_volume": ("size", 1),
    "aws_dynamodb_table": ("table_size_bytes", 1e9)
}

summary_rows = []

print("\n===== Matching Services to CSVs =====")

for _, row in cost_df.iterrows():
    service = row["service"]
    cost = row["cost_usd"]
    matched_file = None

    # Check forced override first
    if service in forced_table_usage:
        matched_file = forced_table_usage[service]
    else:
        # Fallback: try normalized match
        for table in csv_files:
            if table.replace("aws_", "").replace("_", "") in service.lower().replace(" ", ""):
                matched_file = table
                break

    resource_count = 0
    total_storage = 0
    region_count = 0
    last_updated = None

    if matched_file and matched_file in csv_files:
        print(f"✔ Matched '{service}' to {matched_file}.csv")
        try:
            df = pd.read_csv(csv_files[matched_file], on_bad_lines="skip")
            if not df.empty:
                resource_count = len(df)
                region_count = len(df["region"].unique()) if "region" in df.columns else 0

                if "launch_time" in df.columns:
                    try:
                        df["launch_time"] = pd.to_datetime(df["launch_time"], errors="coerce")
                        df["launch_time"] = df["launch_time"].dt.tz_localize(None)
                        last_updated = df["launch_time"].max()
                    except Exception as e:
                        print(f"⚠ Skipped launch_time for {matched_file}: {e}")

                if matched_file in size_fields:
                    size_col, div = size_fields[matched_file]
                    if size_col in df.columns:
                        try:
                            storage_sum = df[size_col].fillna(0).astype(float).sum()
                            total_storage = round(storage_sum / div, 2)
                            print(f"  ↪ {service} total storage: {total_storage} GB")
                        except Exception as e:
                            print(f"⚠ Could not compute storage for {matched_file}: {e}")
                    else:
                        print(f"⚠ No column '{size_col}' in {matched_file}.csv")
        except Exception as e:
            print(f"⚠ Failed to process {matched_file}.csv: {e}")
    else:
        print(f"❌ No CSV match found for: {service}")

    summary_rows.append({
        "Service": service,
        "Cost (USD)": round(cost, 2),
        "Resource Count": resource_count,
        "Total Storage (GB)": total_storage,
        "Regions Used": region_count,
        "Last Updated": last_updated
    })

summary_df = pd.DataFrame(summary_rows).sort_values(by="Cost (USD)", ascending=False)

with pd.ExcelWriter("aws_inventory_report_enhanced.xlsx", engine="xlsxwriter") as writer:
    summary_df.to_excel(writer, sheet_name="Summary", index=False)
    for name, path in csv_files.items():
        try:
            df = pd.read_csv(path, on_bad_lines="skip")
            if not df.empty:
                for col in df.select_dtypes(include=["datetime", "datetimetz"]).columns:
                    df[col] = pd.to_datetime(df[col], errors="coerce").dt.tz_localize(None)
                df.to_excel(writer, sheet_name=name[:31], index=False)
        except Exception as e:
            print(f"⚠ Skipped {path.name}: {e}")

print("\n✅ Final Excel report with EBS + DynamoDB storage included: aws_inventory_report_enhanced.xlsx")

