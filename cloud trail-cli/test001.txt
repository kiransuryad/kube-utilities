
import pandas as pd
from pathlib import Path

# Load cost summary
cost_df = pd.read_csv("active_services_cost.csv")

# Collect all aws_*.csv files (excluding the cost summary itself)
csv_files = {
    csv.stem: csv
    for csv in Path(".").glob("aws_*.csv")
    if csv.name != "active_services_cost.csv"
}

# Storage fields per table
size_fields = {
    "aws_s3_bucket": ("bucket_size_bytes", 1e9),
    "aws_rds_db_instance": ("allocated_storage", 1),
    "aws_ebs_volume": ("size", 1),
    "aws_dynamodb_table": ("table_size_bytes", 1e9),
    "aws_ecr_repository": ("image_size_bytes", 1e9)
}

summary_rows = []

# Map service name to table name for accuracy
service_table_map = {
    "Amazon EC2": "aws_ec2_instance",
    "Amazon Simple Storage Service": "aws_s3_bucket",
    "Amazon DynamoDB": "aws_dynamodb_table",
    "Amazon RDS": "aws_rds_db_instance",
    "Amazon Elastic Block Store": "aws_ebs_volume",
    "AWS CloudTrail": "aws_cloudtrail_trail",
    "AWS CloudWatch": "aws_cloudwatch_log_group",
    "AWS IAM": "aws_iam_user",
    "AWS Secrets Manager": "aws_secretsmanager_secret",
    "AWS Key Management Service": "aws_kms_key",
    "Amazon SQS": "aws_sqs_queue",
    "Amazon SNS": "aws_sns_topic",
    "Amazon ECR": "aws_ecr_repository",
    "AWS Lambda": "aws_lambda_function",
    "AWS Config": "aws_config_configuration_recorder",
    "AWS CloudFormation": "aws_cloudformation_stack"
}

# Build enhanced summary
for _, row in cost_df.iterrows():
    service = row["service"]
    cost = row["cost_usd"]
    table_name = service_table_map.get(service, None)

    resource_count = 0
    total_storage = 0
    region_count = 0
    last_updated = None

    if table_name and table_name in csv_files:
        try:
            df = pd.read_csv(csv_files[table_name], on_bad_lines="skip")
            if not df.empty:
                resource_count = len(df)
                region_count = len(df["region"].unique()) if "region" in df.columns else 0
                if "launch_time" in df.columns:
                    df["launch_time"] = pd.to_datetime(df["launch_time"], errors="coerce")
                    last_updated = df["launch_time"].max()
                if table_name in size_fields:
                    size_col, div = size_fields[table_name]
                    if size_col in df.columns:
                        total_storage = round(df[size_col].fillna(0).astype(float).sum() / div, 2)
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to parse {table_name}: {e}")

    summary_rows.append({
        "Service": service,
        "Cost (USD)": round(cost, 2),
        "Resource Count": resource_count,
        "Total Storage (GB)": total_storage,
        "Regions Used": region_count,
        "Last Updated": last_updated
    })

summary_df = pd.DataFrame(summary_rows).sort_values(by="Cost (USD)", ascending=False)

# Save final Excel report
with pd.ExcelWriter("aws_inventory_report_enhanced.xlsx", engine="xlsxwriter") as writer:
    summary_df.to_excel(writer, sheet_name="Summary", index=False)

    for name, path in csv_files.items():
        try:
            df = pd.read_csv(path, on_bad_lines="skip")
            if not df.empty:
                df.to_excel(writer, sheet_name=name[:31], index=False)
        except Exception as e:
            print(f"‚ùå Skipped {path.name}: {e}")

print("‚úÖ Final enhanced Excel report generated: aws_inventory_report_enhanced.xlsx")




-----------------
import subprocess
import pandas as pd
import json
from pathlib import Path

# === Step 1: Get active services from Steampipe cost table (final query) ===
print("üìä Fetching active services with cost > 0...")

cost_query = """
select
  service,
  sum(unblended_cost_amount) as cost_usd
from
  aws_cost_by_service_monthly
where
  period_start >= current_date - interval '30 days'
group by
  service
having
  sum(unblended_cost_amount) > 0
order by
  cost_usd desc;
"""

result = subprocess.run(
    ["steampipe", "query", "--output=json", cost_query],
    text=True,
    capture_output=True
)

if result.returncode != 0:
    print("‚ùå Failed to fetch cost data:", result.stderr)
    exit(1)

cost_data = json.loads(result.stdout)
rows = cost_data.get("rows", [])
columns = [col["name"] for col in cost_data.get("columns", [])]
cost_df = pd.DataFrame(rows, columns=columns)
cost_df.to_csv("active_services_cost.csv", index=False)
print("‚úÖ Saved active services with cost > 0 to active_services_cost.csv")

# === Step 2: Define relevant AWS service table mappings ===
# You can customize this list as per your needs
service_table_map = {
    "Amazon EC2": "aws_ec2_instance",
    "Amazon Simple Storage Service": "aws_s3_bucket",
    "Amazon DynamoDB": "aws_dynamodb_table",
    "Amazon RDS": "aws_rds_db_instance",
    "Amazon Elastic Block Store": "aws_ebs_volume",
    "AWS CloudTrail": "aws_cloudtrail_trail",
    "AWS CloudWatch": "aws_cloudwatch_log_group",
    "AWS IAM": "aws_iam_user",
    "AWS Secrets Manager": "aws_secretsmanager_secret",
    "AWS Key Management Service": "aws_kms_key",
    "Amazon SQS": "aws_sqs_queue",
    "Amazon SNS": "aws_sns_topic",
    "Amazon ECR": "aws_ecr_repository",
    "AWS Lambda": "aws_lambda_function",
    "AWS Config": "aws_config_configuration_recorder",
    "AWS CloudFormation": "aws_cloudformation_stack"
}

# === Step 3: Run Steampipe queries for matched tables ===
print("\nüöÄ Querying active services by Steampipe table...")

for service, table in service_table_map.items():
    print(f"‚Üí Querying {service} ({table}) ...")
    query = f"select * from {table} limit 10000;"
    try:
        result = subprocess.run(
            ["steampipe", "query", "--output=csv", "--", query],
            text=True,
            capture_output=True
        )
        if result.returncode == 0 and len(result.stdout.strip().splitlines()) > 1:
            csv_file = f"{table}.csv"
            Path(csv_file).write_text(result.stdout)
            print(f"   ‚úÖ Saved to {csv_file}")
        else:
            print(f"   ‚ö† No data returned for {table}")
    except Exception as e:
        print(f"   ‚ùå Failed to query {table}: {e}")

print("\nüéâ Done! All active service data collected.")





-----------------------
import pandas as pd
from pathlib import Path

# Load cost summary
cost_df = pd.read_csv("active_services_cost.csv")

# Collect all service CSVs
csv_files = list(Path(".").glob("*.csv"))
csv_map = {csv.stem.lower(): csv for csv in csv_files if csv.name != "active_services_cost.csv"}

# Known size fields per service (for storage GB calc)
size_fields = {
    "s3": ("bucket_size_bytes", 1e9),
    "rds db instance": ("allocated_storage", 1),
    "ebs volume": ("size", 1),
    "dynamodb table": ("table_size_bytes", 1e9),
    "ecr repository": ("image_size_bytes", 1e9)
}

summary_rows = []

print("\n===== Debug Matching Log =====")

for index, row in cost_df.iterrows():
    service_name = row["service"]
    cost = row["cost_usd"]

    normalized = service_name.lower().replace("amazon", "").replace("aws", "").replace("-", "").strip()
    matched_file = None

    for stem, path in csv_map.items():
        if stem in normalized:
            matched_file = path
            print(f"‚úî Matched: '{service_name}' -> {path.name}")
            break

    if not matched_file:
        print(f"‚ùå No match found for: '{service_name}'")
        summary_rows.append({
            "Service": service_name,
            "Cost (USD)": round(cost, 2),
            "Resource Count": 0,
            "Total Storage (GB)": 0,
            "Regions Used": 0,
            "Last Updated": None
        })
        continue

    try:
        df = pd.read_csv(matched_file, on_bad_lines="skip")
        if df.empty:
            print(f"‚ö† File is empty: {matched_file}")
            raise Exception("Empty")

        resource_count = len(df)
        region_count = len(df["region"].unique()) if "region" in df.columns else 0
        last_updated = None

        if "launch_time" in df.columns:
            df["launch_time"] = pd.to_datetime(df["launch_time"], errors="coerce")
            last_updated = df["launch_time"].max()

        total_storage = 0
        for key, (col, div) in size_fields.items():
            if key in matched_file.stem and col in df.columns:
                total_storage = round(df[col].fillna(0).astype(float).sum() / div, 2)

        summary_rows.append({
            "Service": service_name,
            "Cost (USD)": round(cost, 2),
            "Resource Count": resource_count,
            "Total Storage (GB)": total_storage,
            "Regions Used": region_count,
            "Last Updated": last_updated
        })

    except Exception as e:
        print(f"‚ö† Failed to parse {matched_file}: {e}")
        summary_rows.append({
            "Service": service_name,
            "Cost (USD)": round(cost, 2),
            "Resource Count": 0,
            "Total Storage (GB)": 0,
            "Regions Used": 0,
            "Last Updated": None
        })

# Generate summary
summary_df = pd.DataFrame(summary_rows)
summary_df = summary_df.sort_values(by="Cost (USD)", ascending=False)

# Save to Excel
with pd.ExcelWriter("aws_inventory_report_enhanced.xlsx", engine="xlsxwriter") as writer:
    summary_df.to_excel(writer, sheet_name="Summary", index=False)

    for csv_file in csv_files:
        if csv_file.name == "active_services_cost.csv":
            continue
        try:
            df = pd.read_csv(csv_file, on_bad_lines="skip")
            if not df.empty:
                df.to_excel(writer, sheet_name=csv_file.stem[:31], index=False)
        except Exception as e:
            print(f"Failed to include {csv_file.name}: {e}")

print("\n‚úÖ Debug-enhanced report generated: aws_inventory_report_enhanced.xlsx")
