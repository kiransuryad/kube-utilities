
import pandas as pd
from pathlib import Path
from datetime import datetime
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Mapping service names to CSV files
service_to_csv = {
    "Amazon Elastic Compute Cloud - Compute": "aws_ec2_instance.csv",
    "EC2 - Other": "aws_ec2_instance.csv",
    "Amazon Virtual Private Cloud": "aws_vpc.csv",
    "AWS CloudTrail": "aws_cloudtrail_trail.csv",
    "AmazonCloudWatch": "aws_cloudwatch_log_group.csv",
    "Amazon Elastic Load Balancing": "aws_elb_load_balancer.csv",
    "AWS Directory Service": "aws_directory_service_directory.csv",
    "AWS Key Management Service": "aws_kms_key.csv",
    "Red Hat Enterprise Linux": "aws_ec2_instance.csv",
    "Amazon GuardDuty": "aws_guardduty_detector.csv",
    "Amazon DynamoDB": "aws_dynamodb_table.csv",
    "AWS Config": "aws_config_configuration_recorder.csv",
    "Amazon Simple Storage Service": "aws_s3_bucket.csv",
    "Amazon Route 53": "aws_route53_zone.csv",
    "Amazon Elastic Container Service": "aws_ecs_cluster.csv",
    "AWS Cost Explorer": "",
    "Amazon Elastic Container Registry Public": "aws_ecr_repository.csv",
    "Amazon EC2 Container Registry (ECR)": "aws_ecr_repository.csv",
    "AWS Secrets Manager": "aws_secretsmanager_secret.csv",
    "Amazon Simple Queue Service": "aws_sqs_queue.csv",
    "AWS Lambda": "aws_lambda_function.csv",
    "Amazon Simple Notification Service": "aws_sns_topic.csv",
    "AWS Step Functions": "aws_sfn_state_machine.csv",
    "Amazon Glacier": "aws_glacier_vault.csv"
}

# Load the active service summary
summary_df = pd.read_csv("active_services_cost.csv")
summary_df["Resource Count"] = 0
summary_df["Total Storage (GB)"] = 0
summary_df["Regions Used"] = 0
summary_df["Last Updated"] = 0

def extract_storage(df, service):
    if "size" in df.columns:
        return df["size"].fillna(0).sum()
    elif "table_size_bytes" in df.columns:
        return df["table_size_bytes"].fillna(0).sum() / 1024**3
    elif "bucket_size_bytes" in df.columns:
        return df["bucket_size_bytes"].fillna(0).sum() / 1024**3
    return 0

# Loop through services and enrich summary
for i, row in summary_df.iterrows():
    service = row["service"]
    csv_name = service_to_csv.get(service)
    if not csv_name:
        print(f"⚠ No CSV mapped for: {service}")
        continue

    csv_path = Path(csv_name)
    if not csv_path.exists():
        print(f"❌ File not found: {csv_name}")
        continue

    try:
        df = pd.read_csv(csv_path, on_bad_lines='skip')
        summary_df.at[i, "Resource Count"] = len(df)
        if "region" in df.columns:
            summary_df.at[i, "Regions Used"] = df["region"].nunique()
        if "launch_time" in df.columns:
            try:
                df["launch_time"] = pd.to_datetime(df["launch_time"], errors='coerce')
                summary_df.at[i, "Last Updated"] = df["launch_time"].dt.date.max()
            except:
                pass
        summary_df.at[i, "Total Storage (GB)"] = round(extract_storage(df, service), 2)
        print(f"✓ Processed: {service} ({csv_name})")
    except Exception as e:
        print(f"⚠ Failed to process {csv_name}: {e}")

# Output Excel
output_file = "aws_inventory_report_enhanced.xlsx"
summary_df.to_excel(output_file, index=False, sheet_name="Summary")
print(f"✅ Final Excel report generated: {output_file}")




-------
# ------------------- Total Storage Calculation -------------------

# Initialize total storage per service
storage_summary = {}

# EBS Volumes (from aws_ebs_volume.csv)
try:
    ebs_df = pd.read_csv("aws_ebs_volume.csv")
    ebs_total_gb = ebs_df['size'].fillna(0).sum()
    storage_summary["EC2 - Other"] = round(ebs_total_gb, 2)
    print(f"✓ EBS Total: {ebs_total_gb:.2f} GB")
except Exception as e:
    print(f"✗ EBS Volume CSV error: {e}")

# DynamoDB (from aws_dynamodb_table.csv)
try:
    dynamo_df = pd.read_csv("aws_dynamodb_table.csv")
    dynamo_total_bytes = dynamo_df['table_size_bytes'].fillna(0).sum()
    dynamo_total_gb = dynamo_total_bytes / (1024 ** 3)
    storage_summary["Amazon DynamoDB"] = round(dynamo_total_gb, 2)
    print(f"✓ DynamoDB Total: {dynamo_total_gb:.2f} GB")
except Exception as e:
    print(f"✗ DynamoDB CSV error: {e}")

# ------------------- Patch Summary Sheet -------------------
# Apply to the final summary DataFrame
for idx, row in summary_df.iterrows():
    service = row["Service"]
    if service in storage_summary:
        summary_df.at[idx, "Total Storage (GB)"] = storage_summary[service]


--------
import pandas as pd
from pathlib import Path
from datetime import datetime
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Mapping from Cost Explorer service name to corresponding CSV file
service_to_csv = {
    "Amazon Elastic Compute Cloud - Compute": "aws_ec2_instance.csv",
    "EC2 - Other": "aws_ec2_instance.csv",
    "Amazon Virtual Private Cloud": "aws_vpc.csv",
    "AWS CloudTrail": "aws_cloudtrail_trail.csv",
    "AmazonCloudWatch": "aws_cloudwatch_log_group.csv",
    "Amazon Elastic Load Balancing": "aws_elb_load_balancer.csv",
    "AWS Directory Service": "aws_directory_service_directory.csv",
    "AWS Key Management Service": "aws_kms_key.csv",
    "Red Hat Enterprise Linux": "aws_ec2_instance.csv",
    "Amazon GuardDuty": "aws_guardduty_detector.csv",
    "Amazon DynamoDB": "aws_dynamodb_table.csv",
    "AWS Config": "aws_config_configuration_recorder.csv",
    "Amazon Simple Storage Service": "aws_s3_bucket.csv",  # Fallback if bucket metrics missing
    "Amazon Route 53": "aws_route53_zone.csv",
    "Amazon Elastic Container Service": "aws_ecs_cluster.csv",
    "AWS Cost Explorer": "",  # No CSV expected
    "Amazon Elastic Container Registry Public": "aws_ecr_repository.csv",
    "Amazon EC2 Container Registry (ECR)": "aws_ecr_repository.csv",
    "AWS Secrets Manager": "aws_secretsmanager_secret.csv",
    "Amazon Simple Queue Service": "aws_sqs_queue.csv",
    "AWS Lambda": "aws_lambda_function.csv",
    "Amazon Simple Notification Service": "aws_sns_topic.csv",
    "AWS Step Functions": "aws_sfn_state_machine.csv",
    "Amazon Glacier": "aws_glacier_vault.csv"
}

# Load the summary sheet from previous query
summary_df = pd.read_csv("active_services_cost.csv")
summary_df["Resource Count"] = 0
summary_df["Total Storage (GB)"] = 0
summary_df["Regions Used"] = 0
summary_df["Last Updated"] = ""

def extract_storage(df):
    if "size" in df.columns:
        return df["size"].fillna(0).sum()
    elif "table_size_bytes" in df.columns:
        return df["table_size_bytes"].fillna(0).sum() / (1024 ** 3)
    elif "bucket_size_bytes" in df.columns:
        return df["bucket_size_bytes"].fillna(0).sum() / (1024 ** 3)
    return 0

# Walk each service row and enrich
print("===== Matching Services to CSVs =====")
for idx, row in summary_df.iterrows():
    service = row["service"]
    csv_file = service_to_csv.get(service)
    if not csv_file:
        print(f"❌ No CSV match found for: {service}")
        continue

    path = Path(csv_file)
    if not path.exists():
        print(f"❌ File not found: {csv_file}")
        continue

    try:
        df = pd.read_csv(path, on_bad_lines='skip')

        # Count resources
        summary_df.at[idx, "Resource Count"] = len(df)

        # Count regions
        if "region" in df.columns:
            summary_df.at[idx, "Regions Used"] = df["region"].nunique()

        # Last updated date
        if "launch_time" in df.columns:
            try:
                df["launch_time"] = pd.to_datetime(df["launch_time"], errors='coerce')
                last_used = df["launch_time"].dropna().max()
                if pd.notnull(last_used):
                    summary_df.at[idx, "Last Updated"] = last_used.strftime("%Y-%m-%d")
            except Exception as e:
                print(f"⚠ Warning: launch_time parse issue in {csv_file}: {e}")

        # Add up storage size
        total_storage = extract_storage(df)
        summary_df.at[idx, "Total Storage (GB)"] = round(total_storage, 2)

        print(f"✅ Matched {service} to {csv_file}")
    except Exception as e:
        print(f"❌ Failed to process {csv_file}: {e}")

# Save to Excel
excel_file = "aws_inventory_report_enhanced.xlsx"
summary_df.to_excel(excel_file, index=False, sheet_name="Summary")
print(f"\n✅ Final Excel report with storage summary: {excel_file}")
