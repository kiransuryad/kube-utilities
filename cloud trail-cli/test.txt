
import pandas as pd
from pathlib import Path

# Load cost summary
cost_df = pd.read_csv("active_services_cost.csv")

# Map service name to corresponding CSV file
service_to_file = {csv.stem: csv for csv in Path(".").glob("*.csv") if csv.name != "active_services_cost.csv"}

# Service table name mappings (simplified key from table file name)
service_file_map = {
    "ec2": "EC2 - Other",
    "s3": "Amazon Simple Storage Service",
    "rds_db_instance": "Amazon Relational Database Service",
    "ebs_volume": "Amazon Elastic Block Store",
    "dynamodb_table": "Amazon DynamoDB",
    "ecr_repository": "Amazon EC2 Container Registry (ECR)"
}

# Field mappings for size metrics
size_fields = {
    "s3": ("bucket_size_bytes", 1e9),
    "rds_db_instance": ("allocated_storage", 1),
    "ebs_volume": ("size", 1),
    "dynamodb_table": ("table_size_bytes", 1e9),
    "ecr_repository": ("image_size_bytes", 1e9)
}

summary_rows = []

for index, row in cost_df.iterrows():
    service_name = row["service"]
    cost = row["cost_usd"]
    matched = None
    for key, label in service_file_map.items():
        if label == service_name:
            matched = key
            break

    csv_filename = f"{matched}.csv" if matched else None
    resource_count = 0
    total_storage = 0
    region_count = 0
    last_updated = None

    if csv_filename and Path(csv_filename).exists():
        df = pd.read_csv(csv_filename)
        resource_count = len(df)
        region_count = len(df["region"].unique()) if "region" in df.columns else 0
        if "launch_time" in df.columns:
            try:
                df["launch_time"] = pd.to_datetime(df["launch_time"], errors="coerce")
                last_updated = df["launch_time"].max()
            except:
                pass
        if matched in size_fields:
            size_col, divisor = size_fields[matched]
            if size_col in df.columns:
                total_storage = round(df[size_col].fillna(0).astype(float).sum() / divisor, 2)

    summary_rows.append({
        "Service": service_name,
        "Cost (USD)": round(cost, 2),
        "Resource Count": resource_count,
        "Total Storage (GB)": total_storage,
        "Regions Used": region_count,
        "Last Updated": last_updated
    })

# Convert and save
summary_df = pd.DataFrame(summary_rows)
summary_df = summary_df.sort_values(by="Cost (USD)", ascending=False)

# Write to Excel
with pd.ExcelWriter("aws_inventory_report_enhanced.xlsx", engine="xlsxwriter") as writer:
    summary_df.to_excel(writer, sheet_name="Summary", index=False)

    for file in service_to_file.values():
        try:
            df = pd.read_csv(file)
            df.to_excel(writer, sheet_name=file.stem[:31], index=False)
        except Exception as e:
            print(f"Failed to include {file.name}: {e}")

print("✅ Enhanced Excel report generated: aws_inventory_report_enhanced.xlsx")




------
-- Comprehensive EC2 Instance Inventory with EBS Volume Details and Cost Information
WITH ec2_service_costs AS (
  SELECT
    usage_type,
    resource_id,
    SUM(unblended_cost_amount) as cost_usd
  FROM
    aws_cost_by_service_usage_type_daily
  WHERE
    service = 'Amazon Elastic Compute Cloud - Compute'
    AND period_start >= current_date - interval '30 days'
    AND resource_id IS NOT NULL
    AND resource_id != ''
  GROUP BY
    usage_type, resource_id
),
ec2_tag_costs AS (
  SELECT
    resource_id,
    SUM(unblended_cost_amount) as cost_usd
  FROM
    aws_cost_by_tag
  WHERE
    service = 'Amazon Elastic Compute Cloud - Compute'
    AND period_start >= current_date - interval '30 days'
    AND resource_id IS NOT NULL
    AND resource_id != ''
  GROUP BY
    resource_id
)
SELECT
  -- EC2 Instance Details
  i.instance_id,
  i.instance_type,
  i.instance_state,
  i.availability_zone,
  i.region,
  i.tags,
  i.launch_time,
  i.private_ip_address,
  i.public_ip_address,
  i.vpc_id,
  i.subnet_id,
  
  -- Instance Usage and Last Activity
  i.state_transition_time as last_state_change,
  i.cpu_options_core_count as cpu_cores,
  i.cpu_options_threads_per_core as threads_per_core,
  i.metadata_options_http_endpoint as metadata_endpoint,
  
  -- EBS Volume Details
  v.volume_id,
  v.volume_type,
  v.size as volume_size_gb,
  v.iops,
  v.throughput,
  v.encrypted as volume_encrypted,
  v.state as volume_state,
  v.create_time as volume_create_time,
  v.tags as volume_tags,
  
  -- Cost information (last 30 days)
  COALESCE(usage_costs.cost_usd, 0) as instance_cost_by_usage_30_days,
  COALESCE(tag_costs.cost_usd, 0) as instance_cost_by_tag_30_days,
  (COALESCE(usage_costs.cost_usd, 0) + COALESCE(tag_costs.cost_usd, 0)) as total_estimated_cost_30_days
FROM
  aws_ec2_instance as i
LEFT JOIN
  aws_ebs_volume as v
  ON v.attachments @> ANY(ARRAY[jsonb_build_object('InstanceId', i.instance_id)])
LEFT JOIN
  ec2_service_costs usage_costs
  ON usage_costs.resource_id = i.instance_id
LEFT JOIN
  ec2_tag_costs tag_costs
  ON tag_costs.resource_id = i.instance_id
WHERE
  i.region = 'us-east-1'  -- Change this to your desired region
ORDER BY
  total_estimated_cost_30_days DESC,
  i.instance_id, v.volume_id;


-------
Option 1: Using CloudTrail Events

Tracks last interaction with EC2 instances.

select
  recipient_account_id,
  event_name,
  event_time,
  user_identity ->> 'arn' as user,
  resources -> 0 ->> 'ARN' as resource_arn
from
  aws_cloudtrail_event
where
  event_source = 'ec2.amazonaws.com'
  and event_name in ('StartInstances', 'StopInstances', 'RebootInstances', 'TerminateInstances')
order by
  event_time desc
limit 50;
🔍 What you get:
Most recent EC2 actions per account
Good for admin-triggered events
✅ Option 2: Using CloudWatch Metrics

Tracks actual usage like CPU or network activity.

select
  namespace,
  metric_name,
  resource_id,
  max(datapoints -> 0 ->> 'Timestamp') as last_active_time
from
  aws_cloudwatch_metric_statistics_set
where
  namespace = 'AWS/EC2'
  and metric_name = 'CPUUtilization'
group by
  namespace, metric_name, resource_id
order by
  last_active_time desc
limit 50;




-----------
select
  instance_id,
  instance_type,
  region,
  availability_zone,
  state,
  launch_time,
  private_ip_address,
  public_ip_address,
  tags
from
  aws_ec2_instance
where
  region = 'us-east-1'
limit 10;

------------------

import pandas as pd
from pathlib import Path

# Define the CSV files
cost_summary_file = Path("active_services_cost.csv")
service_files = list(Path(".").glob("*.csv"))
service_files.remove(cost_summary_file)  # remove summary from service list

# Load cost summary
cost_df = pd.read_csv(cost_summary_file)

# Start writing Excel report
with pd.ExcelWriter("aws_inventory_report.xlsx", engine="xlsxwriter") as writer:
    # Write cost summary first
    cost_df.to_excel(writer, sheet_name="Cost Summary", index=False)

    # Write each service file into its own sheet
    for csv_file in service_files:
        try:
            df = pd.read_csv(csv_file)
            sheet_name = csv_file.stem[:31]  # Excel limit
            df.to_excel(writer, sheet_name=sheet_name, index=False)
        except Exception as e:
            print(f"Failed to write {csv_file}: {e}")

print("✅ Excel report generated: aws_inventory_report.xlsx")

# Generate basic HTML version of the cost summary
html_content = cost_df.to_html(index=False, border=0, classes="table table-striped", justify="center")

# Wrap it in some minimal styling
html_final = f"""
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>AWS Inventory Cost Summary</title>
  <style>
    body {{ font-family: sans-serif; margin: 40px; }}
    .table {{ width: 80%; margin: auto; border-collapse: collapse; }}
    .table td, .table th {{ padding: 8px 12px; border: 1px solid #ddd; }}
    .table th {{ background-color: #f4f4f4; text-align: center; }}
    .table-striped tr:nth-child(even) {{ background-color: #f9f9f9; }}
  </style>
</head>
<body>
  <h2 style="text-align:center;">AWS Inventory Cost Summary (Last 30 Days)</h2>
  {html_content}
</body>
</html>
"""

Path("aws_inventory_summary.html").write_text(html_final)
print("✅ HTML summary generated: aws_inventory_summary.html")




--------------
import subprocess
import pandas as pd
import json
from pathlib import Path

# === Step 1: Get active services from Steampipe cost table (final query) ===
print("Fetching active services with cost > 0...")
cost_query = """
select
  service,
  sum(unblended_cost_amount) as cost_usd
from
  aws_cost_by_service_monthly
where
  period_start >= current_date - interval '30 days'
group by
  service
having sum(unblended_cost_amount) > 0
order by
  cost_usd desc;
"""

result = subprocess.run(
    ["steampipe", "query", "--output=json"],
    input=cost_query,
    text=True,
    capture_output=True
)

if result.returncode != 0:
    print("Failed to fetch cost data:", result.stderr)
    exit(1)

cost_data = json.loads(result.stdout)
if "rows" not in cost_data:
    print("Error: No 'rows' in Steampipe output.")
    exit(1)

rows = cost_data["rows"]
if len(rows) == 0:
    print("No active services with cost > 0 found.")
    exit(0)

cost_df = pd.DataFrame(rows)
cost_df.to_csv("active_services_cost.csv", index=False)
print("✅ Saved active services with cost > 0 to active_services_cost.csv")

# === Step 2: Query inventory per service across known regions ===
regions = ["us-east-1", "eu-west-1", "eu-west-2"]

# Expanded service mappings
service_table_map = {
    "EC2 - Other": "aws_ec2_instance",
    "Amazon Elastic Compute Cloud - Compute": "aws_ec2_instance",
    "Amazon Virtual Private Cloud": "aws_vpc",
    "Amazon CloudWatch": "aws_cloudwatch_log_group",
    "Amazon Simple Storage Service": "aws_s3_bucket",
    "Amazon Relational Database Service": "aws_rds_db_instance",
    "Amazon DynamoDB": "aws_dynamodb_table",
    "AWS Lambda": "aws_lambda_function",
    "AWS Key Management Service": "aws_kms_key",
    "AWS Secrets Manager": "aws_secretsmanager_secret",
    "Elastic Load Balancing": "aws_elbv2_load_balancer",
    "Amazon SNS": "aws_sns_topic",
    "Amazon SQS": "aws_sqs_queue",
    "Amazon Route 53": "aws_route53_zone",
    "AWS Directory Service": "aws_directory_service_directory",
    "AWS Config": "aws_config_configuration_recorder",
    "AWS CloudFormation": "aws_cloudformation_stack",
    "AWS Identity and Access Management": "aws_iam_role",
    "Amazon EC2 Container Registry (ECR)": "aws_ecr_repository",
    "Amazon Elastic Container Service": "aws_ecs_cluster",
    "Amazon GuardDuty": "aws_guardduty_detector"
}

queried_services = []

for _, row in cost_df.iterrows():
    service = row["service"]
    if service not in service_table_map:
        continue

    table = service_table_map[service]

    for region in regions:
        sql = f"select * from {table} where region = '{region}' limit 1000;"

        out_file = f"{table.replace('aws_', '')}_{region}.csv"
        print(f"Querying {service} ({table}) in {region}...")

        result = subprocess.run(
            ["steampipe", "query", "--output=csv"],
            input=sql,
            text=True,
            capture_output=True
        )

        if result.returncode == 0:
            Path(out_file).write_text(result.stdout)
            queried_services.append((service, table, region))
        else:
            print(f"Failed to query {table} in {region}: {result.stderr}")

# === Step 3: Done ===
print("✅ All done. Queried services:")
for s, t, r in queried_services:
    print(f"  - {s} ({t}) in {r}")




-------
cost_data = json.loads(result.stdout)

# Validate the structure
if not cost_data or "columns" not in cost_data[0] or "rows" not in cost_data[0]:
    print("Error: Unexpected or empty response from Steampipe.")
    print("Raw output:", result.stdout)
    exit(1)

columns = cost_data[0]["columns"]
rows = cost_data[0]["rows"]

# Build DataFrame safely
if len(rows) == 0:
    print("No active services with cost > 0 found.")
    exit(0)

cost_df = pd.DataFrame(rows, columns=columns)
cost_df.to_csv("active_services_cost.csv", index=False)
print("✅ Saved active services with cost > 0 to active_services_cost.csv")




-------------
import subprocess
import pandas as pd
import json
from pathlib import Path

# === Step 1: Get active services from Steampipe cost table (final query) ===
print("Fetching active services with cost > 0...")
cost_query = """
select
  service,
  sum(unblended_cost_amount) as cost_usd
from
  aws_cost_by_service_monthly
where
  period_start >= current_date - interval '30 days'
group by
  service
having sum(unblended_cost_amount) > 0
order by
  cost_usd desc;
"""

result = subprocess.run(
    ["steampipe", "query", "--output=json"],
    input=cost_query,
    text=True,
    capture_output=True
)

if result.returncode != 0:
    print("Failed to fetch cost data:", result.stderr)
    exit(1)

cost_data = json.loads(result.stdout)
cost_df = pd.DataFrame(cost_data)
cost_df.to_csv("active_services_cost.csv", index=False)
print("Saved active services with cost > 0 to active_services_cost.csv")

# === Step 2: Query inventory per service across known regions ===
regions = ["us-east-1", "eu-west-1", "eu-west-2"]

service_table_map = {
    "Amazon EC2": "aws_ec2_instance",
    "Amazon S3": "aws_s3_bucket",
    "Amazon RDS": "aws_rds_db_instance",
    "Amazon DynamoDB": "aws_dynamodb_table",
    "AWS Lambda": "aws_lambda_function",
    "AWS Key Management Service": "aws_kms_key",
    "AWS Secrets Manager": "aws_secretsmanager_secret",
    "Elastic Load Balancing": "aws_elbv2_load_balancer",
    "Amazon CloudWatch": "aws_cloudwatch_log_group",
    "Amazon SNS": "aws_sns_topic",
    "Amazon SQS": "aws_sqs_queue",
    "AWS CloudFormation": "aws_cloudformation_stack",
    "AWS Identity and Access Management": "aws_iam_role"
}

queried_services = []

for _, row in cost_df.iterrows():
    service = row["service"]
    if service not in service_table_map:
        continue

    table = service_table_map[service]

    for region in regions:
        sql = f"select * from {table} where region = '{region}' limit 1000;"

        out_file = f"{table.replace('aws_', '')}_{region}.csv"
        print(f"Querying {service} ({table}) in {region}...")

        result = subprocess.run(
            ["steampipe", "query", "--output=csv"],
            input=sql,
            text=True,
            capture_output=True
        )

        if result.returncode == 0:
            Path(out_file).write_text(result.stdout)
            queried_services.append((service, table, region))
        else:
            print(f"Failed to query {table} in {region}: {result.stderr}")

# === Step 3: Done ===
print("✅ All done. Queried services:")
for s, t, r in queried_services:
    print(f"  - {s} ({t}) in {r}")





----------
select
  service,
  round(sum(unblended_cost), 2) as cost_usd
from
  aws_cost_by_service_monthly
where
  start_time >= current_date - interval '30 days'
group by
  service
having sum(unblended_cost) > 0
order by
  cost_usd desc;



----
Run the queries via Steampipe:
steampipe query --output csv --file queries.sql
This will create multiple .csv files in your folder.
Generate the final report:
python generate_report.py
You'll get:
aws_inventory_report.xlsx — for Excel users
aws_inventory_report.html — viewable in any browser




------
Access Verification
 Verify access to new GitHub repositories (UI + CLI).
 Request missing access if needed.
CI/CD Service Account Setup
 Verify existing service accounts have access to new repos.
 Create/update service accounts or tokens as needed.
Update References
 Update pipeline configurations (Jenkins, GitHub Actions, Azure DevOps, etc.) with new repo URLs.
 Update Git URLs in tools like Jenkins, ArgoCD, FluxCD, etc.
 Update any local .git/config or workspace references (for developers or tools).
 Update any scripts or IaC templates that use hardcoded Git URLs.
Validation
 Trigger CI/CD pipelines end-to-end.
 Confirm that pipeline stages can fetch the repo successfully.
 Verify ArgoCD/git-sync-based tools are deploying as expected.
Cleanup (Optional)
 Remove references to old GitHub URLs.
 Archive or decommission old repositories if applicable.

------
✅ Step 1: Export the Vault namespace
export VAULT_NAMESPACE="Caas-Parent/A10003579-Harness"
export VAULT_ADDR="https://your-vault-url"
export VAULT_TOKEN="your-admin-or-root-token"
✅ Step 2: Enable the AppRole auth method (if not already)
vault auth enable approle
If it's already enabled, you’ll get a warning which can be ignored.

✅ Step 3: Enable KV secret engine (if not already)
vault secrets enable -path=kv kv-v2
Verify with:

vault secrets list
✅ Step 4: Create a policy
Create a file harness-policy.hcl:

path "kv/data/harness/*" {
  capabilities = ["read", "create", "update", "delete", "list"]
}

path "kv/metadata/harness/*" {
  capabilities = ["list", "delete"]
}
Apply it:

vault policy write harness-policy harness-policy.hcl
✅ Step 5: Create the AppRole and bind the policy
vault write auth/approle/role/harness-role \
    token_ttl=8760h token_max_ttl=8760h \
    secret_id_ttl=8760h \
    token_policies="harness-policy"
8760h = 1 year
Check role ID:

vault read auth/approle/role/harness-role/role-id
Get secret ID:

vault write -f auth/approle/role/harness-role/secret-id
✅ Step 6: Test by logging in via AppRole
vault write auth/approle/login \
    role_id="<copied-role-id>" \
    secret_id="<copied-secret-id>"
This gives you a Vault token valid for 1 year.

Save this securely, as this token is what Harness can use.

✅ Step 7: Use in Harness
In Harness, go to:

Project → Connectors → Secrets Manager → Add New Vault

Vault URL: https://your-vault-url
Auth Method: AppRole
Role ID: <from Step 5>
Secret ID: <from Step 5>
Vault Namespace: Caas-Parent/A10003579-Harness (if using Enterprise)
KV Version: v2
Secret Path Prefix: kv/harness
Save and test the connection.

🧪 (Optional) Step 8: Store a sample secret in Vault
vault kv put kv/harness/sample-secret \
    username="admin" password="p@ssw0rd"


--------
LDAP Configuration for Keycloak in CIO Environment

Hi Casey & Rajesh,

Hope you're both doing well.

We’re approaching the testing phase of Keycloak in our new CIO environment, and I had a quick query regarding the LDAP configuration.

Currently, in our EDA Management account, we are using ldap://worldpay.local. However, since we don’t have access to this LDAP from our new CIO AWS account, we’ll need to connect to an alternative.

Could you please advise on the preferred Active Directory LDAP that we should connect to from the CIO environment? Below is the list I received from the LDAP team—please let me know if any of these would be suitable, or if there's a recommended option we should use for our Ethos LDAP setup.

Thank you!

Best regards,
Kiran

-------
GitHub Migration Plan: github.worldpay.com → FIS GitHub Enterprise

Current Status & Activities
License Procurement
License request for FIS GitHub Enterprise has been raised with the respective teams.
Procurement is currently in progress.
Access Provisioning
Access request for FIS GitHub has been submitted.
This is currently being processed.
Migration Coordination
Engaged with Kailash and team for detailed planning.
A ServiceNow ticket has been raised with the GitHub migration team for both DTS and EDP organization migrations.
DTS Organization Migration
Current State: Hosted on github.worldpay.com.
Migration Type: Straightforward, as it’s still within the same platform.
Repository Count: ~405 repositories total.
POC Plan:

Initial Scope: 20 representative repositories selected for a Proof of Concept (POC).
Timeline: Migration team committed to delivering these 20 repos in FIS GitHub by early next week.
Next Step:
Validate the POC repositories once delivered.
Provide confirmation/feedback.
Upon approval, the migration team will proceed with the remaining ~385 repositories.
Final migration timeline: To be confirmed based on POC outcome.
EDP Organization Migration
Current State: Already migrated by Worldpay to their GitHub Cloud instance a week ago.
Decision Pending: Need to confirm the source of truth for EDP migration:
Should the migration be from:
github.worldpay.com (legacy), or
Worldpay GitHub Cloud (new)?
Next Step:
Await confirmation from the application team.
Once confirmed, the migration team will finalize the migration plan and timeline accordingly.

----
echo "<+secrets.getValue(\"KEYCLOAK_ADMIN\")>" | sed -E 's/.*value: (.*)}/\1/' | base64 > /tmp/kc_admin.txt

echo "{value: keycloakadmin}" | awk -F'value: ' '{print $2}' | tr -d '}'


echo "<+secrets.getValue(\"KEYCLOAK_ADMIN\")>" | grep -oP '(?<=value: ).*' > /tmp/kc_admin.txt
cat /tmp/kc_admin.txt



---
replicaCount: 1

image:
  repository: my.jfrog.internal/keycloak/keycloakx
  tag: "21.1.2"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8080

ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    cert-manager.io/cluster-issuer: cio-ca-issuer
    nginx.ingress.kubernetes.io/rewrite-target: /
  hosts:
    - host: keycloak.test.internal
      paths:
        - path: /keycloak
          pathType: Prefix
    - host: <your-nlb-dns>.elb.amazonaws.com
      paths:
        - path: /keycloak
          pathType: Prefix
    - host: <your-alb-dns>.elb.amazonaws.com
      paths:
        - path: /keycloak
          pathType: Prefix
  tls:
    - secretName: keycloakx-tls
      hosts:
        - keycloak.test.internal
        - <your-nlb-dns>.elb.amazonaws.com
        - <your-alb-dns>.elb.amazonaws.com

extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: <+secrets.getValue("keycloak-admin-username")>
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: <+secrets.getValue("keycloak-admin-password")>
  - name: KC_DB
    value: postgres
  - name: KC_DB_URL_HOST
    value: <+secrets.getValue("keycloak-db-host")>
  - name: KC_DB_URL_DATABASE
    value: keycloak
  - name: KC_DB_USERNAME
    value: <+secrets.getValue("keycloak-db-username")>
  - name: KC_DB_PASSWORD
    value: <+secrets.getValue("keycloak-db-password")>
  - name: KC_PROXY
    value: edge
  - name: KC_HTTP_ENABLED
    value: "true"
  - name: KC_HOSTNAME_STRICT
    value: "false"
  - name: KC_HOSTNAME_STRICT_HTTPS
    value: "false"

resources:
  requests:
    cpu: 250m
    memory: 512Mi
  limits:
    cpu: 500m
    memory: 1Gi



-----------
replicaCount: 1

image:
  repository: httpd
  tag: 2.4-alpine
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: cio-ca-issuer
  hosts:
    - host: sample.test.internal
      paths:
        - path: /sample
          pathType: Prefix
  tls:
    - secretName: sample-app-tls
      hosts:
        - sample.test.internal




-------
curl -k https://<NLB-DNS> -H "Host: sample.test.internal"


----
controller:
  replicaCount: 2

  service:
    enabled: true
    externalTrafficPolicy: Local

    annotations:
      service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
      service.beta.kubernetes.io/aws-load-balancer-internal: "true"
      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
      service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"

  ingressClassResource:
    name: nginx
    enabled: true
    default: true

  metrics:
    enabled: true

  admissionWebhooks:
    enabled: true

defaultBackend:
  enabled: true



-----
git add ${CHART_NAME}/chart || true

echo "Checking if there are any staged changes..."
if ! git diff --cached --quiet; then
  git commit -m "Overlay Sync: ${CHART_NAME} ${CHART_VERSION} from upstream main"
  git push https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git HEAD:${OVERLAY_FEATURE_BRANCH}
else
  echo "No changes to commit. Skipping push."
fi


----
stage('Sync Chart (Preserve Overlay Customizations - Option B)') {
    sh '''
    mkdir -p overlays/${CHART_NAME}/chart

    echo "Backing up overlay-managed files (values-*.yaml, custom-values.yaml, patches/, overlays/)..."
    mkdir -p tmp-preserve
    cp -a overlays/${CHART_NAME}/chart/values-*.yaml tmp-preserve/ || true
    cp -a overlays/${CHART_NAME}/chart/custom-values.yaml tmp-preserve/ || true
    cp -a overlays/${CHART_NAME}/chart/patches tmp-preserve/ || true
    cp -a overlays/${CHART_NAME}/chart/overlays tmp-preserve/ || true

    echo "Removing old chart/ contents..."
    rm -rf overlays/${CHART_NAME}/chart/*

    echo "Copying upstream chart into overlays..."
    cp -a upstream/mirror/${CHART_NAME}/${CHART_VERSION}/${CHART_NAME}/* overlays/${CHART_NAME}/chart/

    echo "Restoring preserved overlay custom files..."
    cp -a tmp-preserve/* overlays/${CHART_NAME}/chart/ || true
    cp -a tmp-preserve/patches overlays/${CHART_NAME}/chart/ || true
    cp -a tmp-preserve/overlays overlays/${CHART_NAME}/chart/ || true
    rm -rf tmp-preserve
    '''
}



---
@Library(['common-lib@c3-stable']) _

parameters {
    string(name: 'CHART_NAME', description: 'Helm chart name (e.g. nginx, keycloak)')
    string(name: 'CHART_VERSION', description: 'Helm chart version (e.g. 1.21.0)')
    string(name: 'OVERLAY_FEATURE_BRANCH', description: 'Feature branch name to create/push in hydra-helm-overlays')
}

def label = "hydra-overlay-sync-${UUID.randomUUID().toString()}"
String podTemplateString = '''...''' // keep your pod spec as-is (aws container)

podTemplate(
    label: label,
    yaml: podTemplateString,
    serviceAccount: 'jenkins',
    runAsUser: '1000'
) {
    node(label) {
        environment {
            UPSTREAM_GIT_URL = "https://bitbucket.fis.dev/scm/~lc5736691/hydra-helm-upstreams.git"
            OVERLAYS_GIT_URL = "https://bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git"
            OVERLAYS_MAIN_BRANCH = 'develop'
        }

        stage('Clone Repositories') {
            withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                sh '''
                rm -rf upstream overlays

                echo "Cloning upstream repo (read-only)..."
                git clone --branch main https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-upstreams.git upstream

                echo "Cloning overlays repo (develop)..."
                git clone https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git overlays
                '''
            }
        }

        stage('Create or Checkout Feature Branch') {
            dir('overlays') {
                withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                    sh '''
                    git checkout ${OVERLAYS_MAIN_BRANCH}
                    git checkout -b ${OVERLAY_FEATURE_BRANCH} || git checkout ${OVERLAY_FEATURE_BRANCH}
                    '''
                }
            }
        }

        stage('Sync Chart (Preserve Overlay Customizations)') {
            sh '''
            mkdir -p overlays/${CHART_NAME}/chart

            echo "Syncing upstream chart into overlays/${CHART_NAME}/chart/..."
            rsync -av --delete \
              --exclude 'values-prod.yaml' \
              --exclude 'values-dev.yaml' \
              --exclude 'custom-values.yaml' \
              --exclude 'patches/' \
              --exclude 'overlays/' \
              upstream/mirror/${CHART_NAME}/${CHART_VERSION}/${CHART_NAME}/ \
              overlays/${CHART_NAME}/chart/
            '''
        }

        stage('Commit and Push Feature Branch') {
            dir('overlays') {
                withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                    sh '''
                    git config user.name "${BB_USER}"
                    git config user.email "${BB_USER}@yourorg.com"

                    git add ${CHART_NAME}/chart
                    git commit -m "Overlay Sync: ${CHART_NAME} ${CHART_VERSION} from upstream"
                    git push https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git HEAD:${OVERLAY_FEATURE_BRANCH}
                    '''
                }
            }
        }
    }
}


-------
@Library(['common-lib@c3-stable']) _

parameters {
    string(name: 'CHART_NAME', description: 'Helm chart name (e.g. nginx, keycloak)')
    string(name: 'CHART_VERSION', description: 'Helm chart version (e.g. 1.21.0)')
    string(name: 'OVERLAY_FEATURE_BRANCH', description: 'Feature branch name to create/push in hydra-helm-overlays')
}

def label = "hydra-helm-pod-${UUID.randomUUID().toString()}"
String podTemplateString = '''...''' // leave your existing podTemplate string here

podTemplate(
    label: label,
    yaml: podTemplateString,
    serviceAccount: 'jenkins',
    runAsUser: '1000'
) {
    node(label) {
        environment {
            UPSTREAM_GIT_URL = "https://bitbucket.fis.dev/scm/~lc5736691/hydra-helm-upstreams.git"
            OVERLAYS_GIT_URL = "https://bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git"
            OVERLAYS_MAIN_BRANCH = 'develop'
        }

        stage('Clone Repositories using HTTPS') {
            environment {
                GIT_SSL_NO_VERIFY = "true"
            }

            withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                sh '''
                rm -rf upstream overlays

                echo "Cloning upstream (read-only) repo..."
                git clone --branch main https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-upstreams.git upstream

                echo "Cloning overlays repo..."
                git clone https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git overlays
                '''
            }
        }

        stage('Create/Checkout Overlay Feature Branch') {
            dir('overlays') {
                withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                    sh '''
                    git checkout ${OVERLAYS_MAIN_BRANCH}
                    git checkout -b ${OVERLAY_FEATURE_BRANCH} || git checkout ${OVERLAY_FEATURE_BRANCH}
                    '''
                }
            }
        }

        stage('Sync Chart Folder from Upstream Repo') {
            sh '''
            rm -rf overlays/${CHART_NAME}/chart
            mkdir -p overlays/${CHART_NAME}/chart

            cp -R upstream/mirror/${CHART_NAME}/${CHART_VERSION}/${CHART_NAME}/* overlays/${CHART_NAME}/chart/
            '''
        }

        stage('Commit & Push to Overlay Feature Branch') {
            dir('overlays') {
                withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                    sh '''
                    git config user.name "${BB_USER}"
                    git config user.email "${BB_USER}@yourorg.com"

                    git add ${CHART_NAME}/chart
                    git commit -m "Overlay Sync: ${CHART_NAME} ${CHART_VERSION} from upstream main"
                    git push https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git HEAD:${OVERLAY_FEATURE_BRANCH}
                    '''
                }
            }
        }
    }
}




-----
stage('Push Helm Chart to Bitbucket (versioned + latest)') {
  withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
    container('jdk11') {
      sh '''
        set -eo pipefail

        git config --global user.email "kiran.gonela@fisglobal.com"
        git config --global user.name "Kiran G"

        echo "Cloning target Bitbucket repo: ${TARGET_BITBUCKET_REPO}"
        git clone https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/${TARGET_BITBUCKET_REPO}.git target-repo
        cd target-repo

        CHART_DIR="mirror/${SRC_CHART_NAME}/${SRC_CHART_VERSION}"
        LATEST_DIR="mirror/${SRC_CHART_NAME}/latest"

        echo "Preparing chart directories..."
        mkdir -p ${CHART_DIR}
        rm -rf ${LATEST_DIR}
        mkdir -p ${LATEST_DIR}

        echo "Copying chart contents into versioned and latest directories..."
        cp -R ../${SRC_CHART_NAME}/* ${CHART_DIR}/
        cp -R ../${SRC_CHART_NAME}/* ${LATEST_DIR}/

        echo "Committing and pushing changes..."
        git add mirror/${SRC_CHART_NAME}
        git commit -m "Mirror: ${SRC_CHART_NAME} ${SRC_CHART_VERSION} from ${SRC_REPO_URL}"
        git push https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/${TARGET_BITBUCKET_REPO}.git HEAD
      '''
    }
  }
}



-------
Subject: Request to Configure Vault Kubernetes Auth Backend or Grant Temporary Admin Policy
Hi [Vault Admin / Platform Team],

I'm currently working on integrating our EKS workloads with HashiCorp Vault, specifically to enable the Vault Agent Injector to pull secrets (like Keycloak admin credentials and internal TLS certificates) using the Kubernetes authentication method.

However, I've hit a blocker while trying to configure the Kubernetes auth backend from the CLI on our bastion host using my Vault token.

🔍 Problem
Attempts to run the following fail with 403 Forbidden errors:

vault auth enable -path=kubernetes -max-lease-ttl=24h kubernetes

vault write auth/kubernetes/config \
  token_reviewer_jwt="<EKS_JWT_TOKEN>" \
  kubernetes_host="<EKS_API_URL>" \
  kubernetes_ca_cert="<CLUSTER_CA_CERT>" \
  issuer="https://kubernetes.default.svc"
The token I’m using is authenticated under namespace:
CaaS-Parent/A10003579-Keycloak

Even after switching to:
CaaS-Parent (where the auth/kubernetes path is enabled), the same error persists — confirming it’s a permissions issue.

✅ What We Need
Please help us with one of the following:

Option 1: You configure Kubernetes auth on our behalf

Run these (we will supply JWT, CA, and API server values as needed):

vault write auth/kubernetes/config \
  token_reviewer_jwt="<...>" \
  kubernetes_host="https://..." \
  kubernetes_ca_cert="-----BEGIN CERTIFICATE-----..." \
  issuer="https://kubernetes.default.svc"
And ensure the following role exists:

vault write auth/kubernetes/role/k8s-general \
  bound_service_account_names="*" \
  bound_service_account_namespaces="*" \
  policies="k8s-read-eks-apps" \
  ttl="24h"
Option 2: Grant my token temporary access to configure Kubernetes auth

Please assign my token (or identity) the following policy within CaaS-Parent namespace:

# Enable/Configure Kubernetes auth backend
path "sys/auth/kubernetes" {
  capabilities = ["create", "update"]
}
path "auth/kubernetes/config" {
  capabilities = ["create", "update"]
}

# Manage Vault roles under Kubernetes auth
path "auth/kubernetes/role/*" {
  capabilities = ["create", "update", "read", "delete", "list"]
}
Once this is done, I can finish setting up the Vault integration for the EKS workloads securely.

Please let me know if you'd prefer me to provide the JWT and CA cert values for you to configure directly. I’m happy to assist with validation after the setup.

Thanks in advance!

Best regards,

---
vault token create \
  -namespace="CaaS-Parent/A10003579-Keycloak" \
  -orphan \
  -policy="default" \
  -policy="k8s-read-eks-apps" \
  -period=24h


---
Steps to Set Up a Dedicated Namespace

kubectl create namespace vault-auth
kubectl create serviceaccount vault-auth-setup -n vault-auth
Then retrieve the values for Vault:

JWT_TOKEN=$(kubectl create token vault-auth-setup -n vault-auth)
K8S_CA_CERT=$(kubectl get configmap kube-root-ca.crt -n vault-auth -o jsonpath="{.data.ca\.crt}")
K8S_HOST=$(kubectl config view --minify -o jsonpath="{.clusters[0].cluster.server}")


---
Updated Steps (Child Namespace Scope)

Here’s what you need to do next:

1. ✅ Enable Kubernetes Auth within your namespace
vault auth enable -path=kubernetes kubernetes
This enables the Kubernetes auth backend scoped to your namespace.

2. ✅ Configure the Kubernetes Auth Backend
Now the previous vault write should work:

vault write auth/kubernetes/config \
  token_reviewer_jwt="${JWT_TOKEN}" \
  kubernetes_host="${K8S_HOST}" \
  kubernetes_ca_cert="${K8S_CA_CERT}" \
  issuer="https://kubernetes.default.svc"
✅ This will succeed now because you’re operating inside your own namespace.

3. ✅ Continue with the Role + Secret Setup
vault write auth/kubernetes/role/k8s-general \
  bound_service_account_names="*" \
  bound_service_account_namespaces="*" \
  policies="k8s-read-eks-apps" \
  ttl="24h"

vault kv put eks-apps/prod/keycloak/admin username="admin" password="supersecret123"

---
Update your Vault policy to include this:

# file: vault-admin-k8s-setup.hcl

# For setting up Kubernetes auth backend
path "auth/kubernetes/config" {
  capabilities = ["create", "update"]
}

# For creating roles that map K8s SAs to policies
path "auth/kubernetes/role/*" {
  capabilities = ["create", "update", "read", "delete", "list"]
}
Then apply:

vault policy write vault-admin-k8s-setup vault-admin-k8s-setup.hcl

----
#!/bin/bash

set -e

VAULT_ADDR="${VAULT_ADDR:-http://127.0.0.1:8200}"
VAULT_NAMESPACE="${VAULT_NAMESPACE:-root}"

ENGINE_PATH="eks-apps"
POLICY_NAME="k8s-read-eks-apps"
ROLE_NAME="k8s-general"

echo "🔐 [1/6] Enabling new KV v2 secrets engine at path: ${ENGINE_PATH}/"

vault secrets enable -path="${ENGINE_PATH}" -version=2 kv || echo "🟡 Already enabled"

echo "📜 [2/6] Writing Vault policy: ${POLICY_NAME}"

cat <<EOF | vault policy write ${POLICY_NAME} -
path "${ENGINE_PATH}/data/*" {
  capabilities = ["read"]
}
EOF

echo "🔑 [3/6] Extracting Kubernetes auth details from cluster..."

JWT_TOKEN=$(kubectl get secret $(kubectl get sa default -n default -o jsonpath="{.secrets[0].name}") -n default -o jsonpath="{.data.token}" | base64 -d)
K8S_HOST=$(kubectl config view --minify -o jsonpath="{.clusters[0].cluster.server}")
K8S_CA_CERT=$(kubectl get secret $(kubectl get sa default -n default -o jsonpath="{.secrets[0].name}") -n default -o jsonpath="{.data['ca\.crt']}" | base64 -d)

echo "⚙️ [4/6] Configuring Vault Kubernetes auth backend"

vault write auth/kubernetes/config \
  token_reviewer_jwt="${JWT_TOKEN}" \
  kubernetes_host="${K8S_HOST}" \
  kubernetes_ca_cert="${K8S_CA_CERT}" \
  issuer="https://kubernetes.default.svc"

echo "🔗 [5/6] Creating Vault role: ${ROLE_NAME} (bound to all SAs & namespaces)"

vault write auth/kubernetes/role/${ROLE_NAME} \
  bound_service_account_names="*" \
  bound_service_account_namespaces="*" \
  policies="${POLICY_NAME}" \
  ttl="24h"

echo "🔐 [6/6] Storing a sample secret at: ${ENGINE_PATH}/prod/keycloak/admin"

vault kv put ${ENGINE_PATH}/prod/keycloak/admin username="admin" password="supersecret123"

echo "✅ Vault Kubernetes integration complete!"




--------
Quick Test Plan — Certificate Issuance

📄 Step 1: Create a Test Certificate Resource
Here’s a simple manifest that will request a TLS cert using your cio-ca-issuer.

# test-certificate.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: test-tls
  namespace: default
spec:
  secretName: test-tls-secret
  duration: 2160h # 90 days
  renewBefore: 360h # 15 days
  commonName: test.keycloak.internal
  dnsNames:
    - test.keycloak.internal
  issuerRef:
    name: cio-ca-issuer
    kind: ClusterIssuer
Apply it:

kubectl apply -f test-certificate.yaml
🔍 Step 2: Validate Certificate Status
Check if cert-manager successfully issued it:

kubectl get certificate test-tls -n default
Expected output:

NAME        READY   SECRET             AGE
test-tls    True    test-tls-secret    1m
If READY is not True, inspect events:

kubectl describe certificate test-tls -n default
kubectl get events -n default --sort-by=.metadata.creationTimestamp
🔐 Step 3: Inspect the Generated Secret
kubectl get secret test-tls-secret -n default -o yaml
Should contain:

tls.crt → Certificate
tls.key → Private key
Optional: Decode and inspect the certificate:

kubectl get secret test-tls-secret -n default -o jsonpath='{.data.tls\.crt}' | base64 -d | openssl x509 -noout -text
Check:

CN = test.keycloak.internal
Validity period = 90 days
Issuer = cio.internal

-----
chart.yaml
---

apiVersion: v2
name: cert-bootstrap
description: Bootstrap internal self-signed CA and ClusterIssuer for cert-manager
type: application
version: 0.1.0
appVersion: "1.0"


vaules.yaml
-----

bootstrapCerts:
  enabled: true
  caCommonName: "cio.internal"
  duration: "8760h"         # 1 year
  renewBefore: "720h"       # 30 days
  caSecretName: "cio-internal-ca-secret"
  clusterIssuerName: "cio-ca-issuer"
  certificateName: "cio-internal-ca"
  namespace: "cert-manager"




-----

---
selfsigned-cluster-issuer.yaml
---
{{- if .Values.bootstrapCerts.enabled }}
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-cluster-issuer
spec:
  selfSigned: {}
{{- end }}


------

----
cio-ca-certificate.yaml
----

{{- if .Values.bootstrapCerts.enabled }}
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: cio-internal-ca
  namespace: cert-manager
spec:
  isCA: true
  commonName: "{{ .Values.bootstrapCerts.caCommonName }}"
  secretName: "{{ .Values.bootstrapCerts.caSecretName }}"
  duration: {{ .Values.bootstrapCerts.duration | quote }}
  renewBefore: {{ .Values.bootstrapCerts.renewBefore | quote }}
  issuerRef:
    name: selfsigned-cluster-issuer
    kind: ClusterIssuer
{{- end }}


------

----
cio-ca-issuer.yaml
----

{{- if .Values.bootstrapCerts.enabled }}
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: {{ .Values.bootstrapCerts.clusterIssuerName }}
spec:
  ca:
    secretName: "{{ .Values.bootstrapCerts.caSecretName }}"
{{- end }}

