kubectl run test-pull \
  --image=edas-docker-snapshot-local.docker.fis.dev/test/velero:v1.16.0 \
  --image-pull-policy=Always \
  --restart=Never \
  --namespace=velero \
  --overrides='{"spec":{"imagePullSecrets":[{"name":"jfrog-docker-secret"}]}}'


----------
namespace: velero

image:
  repository: myregistry.example.com/velero/velero
  tag: v1.13.2
  pullPolicy: IfNotPresent

initContainers:
  - name: velero-plugin-for-aws
    image: myregistry.example.com/velero/velero-plugin-for-aws:v1.8.0
    imagePullPolicy: IfNotPresent
    volumeMounts:
      - mountPath: /target
        name: plugins

serviceAccount:
  server:
    create: false  # Using pre-created SA
    name: velero
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_ID>:role/<VELERO_IRSA_ROLE>

credentials:
  useSecret: false

deployRestic: true
backupsEnabled: true
snapshotsEnabled: true
upgradeCRDs: true

configuration:
  backupStorageLocation:
    - name: default
      provider: aws
      bucket: velero-backups-prod  # your dedicated S3 bucket
      default: true
      accessMode: ReadWrite
      config:
        region: us-east-1
        s3ForcePathStyle: "true"

  volumeSnapshotLocation:
    - name: default
      provider: aws
      config:
        region: us-east-1

  defaultBackupTTL: 168h  # 7 days
  uploaderType: restic

schedules:
  daily-all-namespaces:
    disabled: false
    schedule: "0 2 * * *"  # every day at 2am UTC
    useOwnerReferencesInBackup: false
    template:
      ttl: "168h"
      snapshotVolumes: true
      includedNamespaces:
        - '*'

metrics:
  enabled: true
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8085"
    prometheus.io/path: "/metrics"

nodeAgent:
  podSecurityContext:
    runAsUser: 0






-----------------------
replicaCount: 2

image:
  repository: your-registry.local/oauth2-proxy
  tag: latest
  pullPolicy: Always

extraArgs:
  provider: "oidc"
  oidc-issuer-url: "https://login.microsoftonline.com/e3ff91d8-34c8-4b15-a0b4-18910a6ac575/v2.0"
  client-id: "0b2b4378-29ec-4b19-bb1f-5a3a83cbe344"
  client-secret: "1fc93259-28ad-48ef-b7e8-fb6f2746268c"
  redirect-url: "https://dev-ethos-idp.fiscloudservices.com/oauth2/callback"
  cookie-secret: "<BASE64_32_BYTE_SECRET>"  # Replace this with secure output from openssl or base64
  email-domain: "*"
  whitelist-domain: ".fiscloudservices.com"
  set-authorization-header: "true"
  pass-access-token: "true"
  pass-user-headers: "true"
  pass-authorization-header: "true"
  cookie-secure: "true"
  cookie-domain: ".fiscloudservices.com"
  scope: "openid email profile"
  skip-provider-button: "true"

config:
  existingSecret: ""
  clientID: ""
  clientSecret: ""
  cookieSecret: ""

ingress:
  enabled: true
  className: nginx
  annotations:
    alb.ingress.kubernetes.io/scheme: internal
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  hosts:
    - dev-ethos-idp.fiscloudservices.com
  path: /oauth2(/|$)(.*)
  pathType: ImplementationSpecific
  tls:
    - hosts:
        - dev-ethos-idp.fiscloudservices.com
      secretName: cio-cert-tls

resources:
  requests:
    cpu: 50m
    memory: 64Mi
  limits:
    cpu: 100m
    memory: 128Mi

serviceAccount:
  create: true

--
Run this to replace the <BASE64_32_BYTE_SECRET> placeholder:

head -c 32 /dev/urandom | base64


----------------------
alertmanager:
  enabled: true
  alertmanagerSpec:
    replicas: 2
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 2Gi
          storageClassName: gp3-encrypted
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      alb.ingress.kubernetes.io/scheme: internal
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
      alb.ingress.kubernetes.io/backend-protocol: HTTP
      alb.ingress.kubernetes.io/healthcheck-path: /-/ready
      alb.ingress.kubernetes.io/healthcheck-port: traffic-port
      nginx.ingress.kubernetes.io/rewrite-target: /$2
    hosts:
      - host: dev-ethos-idp.fiscloudservices.com
        paths:
          - path: /alertmanager(/|$)(.*)
            pathType: ImplementationSpecific
    tls:
      - hosts:
          - dev-ethos-idp.fiscloudservices.com
        secretName: cio-cert-tls





---------------------
grafana:
  useStatefulSet: true
  deploymentStrategy: Recreate
  replicas: 1

  persistence:
    enabled: true
    type: statefulset
    storageClassName: gp2
    accessModes:
      - ReadWriteOnce
    size: 10Gi




----------
kubectl -n ingress-nginx get pods -l app.kubernetes.io/name=ingress-nginx -o name | \
xargs -I{} kubectl -n ingress-nginx logs -f {} | grep --line-buffered grafana



----------
we implemented a fresh Keycloak deployment for the Ethos team within the CIO-managed AWS account, moving from an extremely outdated version to the latest Keycloak 26.2. We successfully migrated all critical realm data from the legacy system into the new, production-grade setup running on private AWS EKS with TLS, custom themes, and realm-based isolation. The solution was redesigned from the ground up to align with the FIS CIO team's standards and security requirements. Alongside this, we onboarded and integrated modern DevSecOps tooling, including Jenkins (CaaS), HashiCorp Vault, Artifactory, and Harness for streamlined release pipelinesâ€”adopting the latest FIS best practices to ensure scalability, compliance, and operational efficiency.

------
#######################
# ACM Certificate Lookups
#######################

data "aws_acm_certificate" "keycloak_cert" {
  domain      = "keycloak.dev.fiscloudservices.com"
  statuses    = ["ISSUED"]
  most_recent = true
}

data "aws_acm_certificate" "grafana_cert" {
  domain      = "grafana.dev.fiscloudservices.com"
  statuses    = ["ISSUED"]
  most_recent = true
}

#######################
# HTTPS Listener
#######################

resource "aws_lb_listener" "dev_keycloak_ext_alb_listener" {
  load_balancer_arn = aws_lb.dev_keycloak_ext_alb.id
  port              = 443
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-2016-08"

  certificate {
    certificate_arn = data.aws_acm_certificate.keycloak_cert.arn
  }

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.dev_keycloak_ext_alb_target_http.id
  }
}

#######################
# Add SNI Certificate for Grafana
#######################

resource "aws_lb_listener_certificate" "grafana_cert" {
  listener_arn    = aws_lb_listener.dev_keycloak_ext_alb_listener.arn
  certificate_arn = data.aws_acm_certificate.grafana_cert.arn
}

#######################
# Host-Based Routing Rules
#######################

# Route Grafana domain to same backend
resource "aws_lb_listener_rule" "grafana_host_rule" {
  listener_arn = aws_lb_listener.dev_keycloak_ext_alb_listener.arn
  priority     = 5

  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.dev_keycloak_ext_alb_target_http.arn
  }

  condition {
    host_header {
      values = ["grafana.dev.fiscloudservices.com"]
    }
  }
}

# Route Keycloak domain to same backend
resource "aws_lb_listener_rule" "keycloak_host_rule" {
  listener_arn = aws_lb_listener.dev_keycloak_ext_alb_listener.arn
  priority     = 10

  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.dev_keycloak_ext_alb_target_http.arn
  }

  condition {
    host_header {
      values = ["keycloak.dev.fiscloudservices.com"]
    }
  }
}









---------------------------------------------------------------------
---------------------------------------------------------------------
resource "aws_lb_listener_rule" "keycloak_default_rule" {
  listener_arn = aws_lb_listener.dev_keycloak_ext_alb_listener.arn
  priority     = 10

  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.dev_keycloak_ext_alb_target_http.arn
  }

  condition {
    host_header {
      values = ["keycloak.dev.fiscloudservices.com"]
    }
  }
}



resource "aws_lb_listener_rule" "grafana_host_rule" {
  listener_arn = aws_lb_listener.dev_keycloak_ext_alb_listener.arn
  priority     = 5

  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.dev_keycloak_ext_alb_target_http.arn
  }

  condition {
    host_header {
      values = ["grafana.dev.fiscloudservices.com"]
    }
  }
}


resource "aws_lb_listener_certificate" "grafana_cert" {
  listener_arn    = aws_lb_listener.dev_keycloak_ext_alb_listener.arn
  certificate_arn = var.grafana_cert_arn
}





----------------------
FROM quay.io/keycloak/keycloak:26.2

USER root

# Remove old JDBC driver
RUN rm -f /opt/keycloak/providers/postgresql-*.jar

# Add newer driver
ADD https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.6/postgresql-42.7.6.jar /opt/keycloak/providers/

USER 1000




--------------------------
FROM alpine:3.20

# Install only required tools
RUN apk add --no-cache unzip wget

# Set working directory
WORKDIR /tmp

# Copy your base Keycloak themes (assumes structure includes login2.css and styles.css)
COPY containers/keycloak-themes/keycloak-theme/theme /worldpay-themes

# Download Zocial CSS
RUN wget https://github.com/smcllns/css-social-buttons/archive/refs/tags/v1.4.0.zip -O /tmp/zocial.zip && \
    unzip /tmp/zocial.zip -d /tmp && \
    rm /tmp/zocial.zip

# Download PatternFly minified CSS and additions
RUN mkdir -p /tmp/patternfly/css && \
    wget https://unpkg.com/@patternfly/patternfly/patternfly.min.css -O /tmp/patternfly/css/patternfly.min.css && \
    wget https://unpkg.com/@patternfly/patternfly/patternfly-additions.min.css -O /tmp/patternfly/css/patternfly-additions.min.css

# Create non-root user
RUN addgroup -g 1001 keycloakthemes && \
    adduser -S -u 1001 -G keycloakthemes keycloakthemes && \
    chown -R keycloakthemes:keycloakthemes /worldpay-themes

USER keycloakthemes

# Copy all required static files into each themeâ€™s login/resources folder
RUN for theme in /worldpay-themes/theme/login/resources/*; do \
      mkdir -p "$theme/patternfly/css" "$theme/zocial" "$theme/css"; \
      cp /tmp/patternfly/css/patternfly.min.css "$theme/patternfly/css/"; \
      cp /tmp/patternfly/css/patternfly-additions.min.css "$theme/patternfly/css/"; \
      cp /tmp/css-social-buttons-1.4.0/css/zocial.css "$theme/zocial/"; \
      cp "$theme/css/login2.css" "$theme/css/"; \
      cp "$theme/css/styles.css" "$theme/css/"; \
    done

WORKDIR /worldpay-themes

CMD ["sleep", "infinity"]




------------------------

FROM node:18-alpine3.20

ENV NODE_ENV=production

# Install unzip, wget
RUN apk add --no-cache unzip wget

# Set working directory
WORKDIR /tmp

# Optional: if using any NodeJS libraries
# Create clean package with only safe versions
COPY <<EOF ./package.json
{
  "name": "keycloak-theme-deps",
  "version": "1.0.0",
  "dependencies": {
    "patternfly": "^4.0.0", // adjust as needed
    "cross-spawn": "^7.0.5",
    "debug": "^4.3.4"
  },
  "resolutions": {
    "cross-spawn": "^7.0.5"
  }
}
EOF

# (Optional) if you still need to install packages
RUN npm install --omit=dev && rm -rf ~/.npm

# Download shared CSS files
RUN wget https://github.com/smcllns/css-social-buttons/archive/refs/tags/v1.4.0.zip -O /tmp/zocial.zip && \
    unzip /tmp/zocial.zip -d /tmp && rm /tmp/zocial.zip

# Copy your theme files
COPY containers/keycloak-themes/keycloak-theme/theme /worldpay-themes

# Create non-root user and fix permissions
RUN addgroup -g 1001 keycloakthemes && \
    adduser -S -u 1001 -G keycloakthemes keycloakthemes && \
    chown -R keycloakthemes:keycloakthemes /worldpay-themes

# Switch user
USER keycloakthemes
WORKDIR /worldpay-themes

# Add static zocial CSS into each theme
COPY --chown=keycloakthemes:keycloakthemes ./tmp/css-social-buttons-1.4.0/css/ /worldpay-themes/theme/login/resources/zocial/

CMD [ "node" ]




-------------
FROM alpine:3.20

ENV VAULT_VERSION=1.15.2 \
    AWS_CLI_VERSION=2.15.59 \
    KUBECTL_VERSION=1.30.1

# Install core debugging tools
RUN apk update && apk add --no-cache \
    bash \
    curl \
    wget \
    jq \
    iproute2 \
    iputils \
    bind-tools \
    busybox-extras \
    net-tools \
    tcpdump \
    postgresql15-client \
    unzip \
    py3-pip \
    gettext \
    groff \
    libc6-compat

# Install Vault CLI
RUN wget https://releases.hashicorp.com/vault/${VAULT_VERSION}/vault_${VAULT_VERSION}_linux_amd64.zip && \
    unzip vault_${VAULT_VERSION}_linux_amd64.zip -d /usr/local/bin && \
    rm vault_${VAULT_VERSION}_linux_amd64.zip

# Install kubectl
RUN curl -L https://dl.k8s.io/release/v${KUBECTL_VERSION}/bin/linux/amd64/kubectl -o /usr/local/bin/kubectl && \
    chmod +x /usr/local/bin/kubectl

# Install AWS CLI v2
RUN curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64-${AWS_CLI_VERSION}.zip" -o "awscliv2.zip" && \
    unzip awscliv2.zip && \
    ./aws/install && \
    rm -rf awscliv2.zip aws

# Drop to non-root user
RUN addgroup -S debug && adduser -S debug -G debug
USER debug

WORKDIR /home/debug

CMD [ "bash" ]




---------
Subject: Non-Prod Environment for CIO Keycloak Now Live â€“ Please Validate

Hello All,

Iâ€™m pleased to inform you that the non-production environment for the new CIO-hosted Keycloak instance is now up and running successfully.

âœ… Current Status
The new environment is live and accessible at:
ðŸ”— https://dev-ethos-idp.fiscloudservices.com
I have migrated data from the old EDA non-prod Keycloak up to yesterdayâ€™s snapshot.
For initial access and testing, I have created two local admin users:
Rajesh
Casey
Iâ€™ll be sharing their login credentials via separate, individual emails shortly for security reasons.
ðŸ” Your Action
Please begin validating the environment based on your respective use cases and workflows. If you encounter any issues or need assistance, do reach out, and weâ€™ll help resolve them promptly.

ðŸ”„ What Happens Next
Once we receive confirmation that testing is successful:

We will coordinate with the Akamai team to switch over the non-prod fisglobal domain to point to this new CIO Keycloak instance.
After this non-prod switch is validated and signed off, we will replicate the same steps for production deployment and cutover.
Thank you for your support and collaboration during this transition. Looking forward to your feedback.

Best regards,
Kiran Gonela




----------------
https://login.microsoftonline.com/e3ff91d8-34c8-4b15-a0b4-18910a6ac575/v2.0/.well-known/openid-configuration



-----------
Step 1: Identify All LDAP Components
SELECT id, name FROM component WHERE provider_id = 'ldap';
Assume you get one or more component IDs â€” e.g., 'ldap-1234-uuid'.

Step 2: Delete Component Configs (Required)
This removes key-value settings tied to the LDAP components.

DELETE FROM component_config
WHERE component_id IN (
  SELECT id FROM component WHERE provider_id = 'ldap'
);
This must be done before deleting from component.

Step 3: Delete Subcomponents (Mappers, etc.)
These are usually mappers tied to the LDAP provider.

DELETE FROM component_config
WHERE component_id IN (
  SELECT id FROM component
  WHERE parent_id IN (
    SELECT id FROM component WHERE provider_id = 'ldap'
  )
);

DELETE FROM component
WHERE parent_id IN (
  SELECT id FROM component WHERE provider_id = 'ldap'
);
Step 4: Delete the Main LDAP Component
Now that dependencies are gone, delete the main entries.

DELETE FROM component
WHERE provider_id = 'ldap';
âœ… Optional Final Clean-Up (Any orphaned config)

DELETE FROM component_config
WHERE component_id NOT IN (SELECT id FROM component);



------
ðŸ” 1.1: Review LDAP Configuration (Optional)
SELECT id, name, provider_id, parent_id
FROM component
WHERE provider_id = 'ldap';
ðŸ”¥ 1.2: Delete All LDAP Mappers (Subcomponents)
DELETE FROM component
WHERE parent_id IN (
  SELECT id FROM component WHERE provider_id = 'ldap'
);
ðŸ§¼ 1.3: Delete Main LDAP Component
DELETE FROM component
WHERE provider_id = 'ldap';
ðŸ§¹ 1.4: (Optional) Clean Up Orphaned Components (Safety Net)
DELETE FROM component
WHERE parent_id NOT IN (
  SELECT id FROM component
);




Is there a way to verify through the Azure portal whether authentication for a given resource is indeed being handled via Entra ID (e.g., using User Assigned or System Assigned Managed Identity)?
Since key-based authentication methods have been explicitly disabled, does this also imply that Multi-Factor Authentication (MFA) is bypassed or not applicable in this context?

---
Subject: Keycloak Environment URL Planning and Akamai Redirection

Hi Casey & Rajesh,

I hope you're both doing well.

As part of our Keycloak migration and environment setup, I wanted to share the planned URLs we intend to use across the Dev, Non-Prod, and Prod environments. These will complement the existing fisglobal URLs, with some changes in access control as noted below.

âœ… Planned Keycloak URLs (Environment-wise)
Dev: https://dev.fiscloudservices.com/auth
Non-Prod: https://nonprod.fiscloudservices.com/auth
Prod: https://prod.fiscloudservices.com/auth
These fiscloudservices URLs will provide access to the master realm.

In contrast, the current fisglobal URLs will not have access to the master realm, which is a deliberate decision for better access control and boundary definition across environments.

ðŸ”„ Redirection Plan via Akamai
Once we receive your sign-off for the Non-Prod environment, we will proceed to raise a request with the Akamai team to redirect the existing non-prod URL to the new fiscloudservices non-prod URL.

We will follow the same approach for Prod after the respective sign-off, ensuring smooth transition and minimal disruption.

ðŸ‘¥ Azure AD Group Alignment (already in progress)
As a quick update, we have initiated the creation of the following Azure AD groups to support role-based access to Keycloak:

A10003579-ethos-keycloak-admins
A10003579-ethos-keycloak-members
Membership into these groups will be controlled via SNOW request approvals, with both of you set as the group admins.

â“Action Items / Queries
Could you please review and confirm your comfort with the above URL structure and access design?
Once Non-Prod is verified, can you please share a quick sign-off so we can raise the Akamai request?
Let us know if there are any objections or preferences regarding the redirection timelines or access control plans.



---------
Hi, we already have an Azure AD app in place. Could you please create two Azure AD groups named A10003579-ethos-keycloak-admins and A10003579-ethos-keycloak-members? Membership should be managed via SNOW request, with Casey and Rajesh set as group admins for both groups. Thanks!

---
- "-listen=:5678"
            - "-text=403 Forbidden - Admin Access Denied"
            - "-code=403"


-----
{{- if and .Values.enableBlockService .Values.adminIngressRules }}
apiVersion: v1
kind: Service
metadata:
  name: block-service
  namespace: {{ .Release.Namespace }}
spec:
  type: ClusterIP
  selector:
    app: block
  ports:
    - port: 80
      targetPort: 5678
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: block
  namespace: {{ .Release.Namespace }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: block
  template:
    metadata:
      labels:
        app: block
    spec:
      containers:
        - name: block
          image: hashicorp/http-echo:latest
          args:
            - "-text=403 Forbidden - Admin Access Denied"
            - "-status=403"
          ports:
            - containerPort: 5678
{{- end }}


----
{{- range .Values.adminIngressRules }}
{{- if .blockAdminConsole }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "keycloak.fullname" $ }}-adminblock-{{ .host | replace "." "-" }}
  namespace: {{ $.Release.Namespace }}
spec:
  ingressClassName: {{ $.Values.ingress.ingressClassName }}
  rules:
    - host: {{ .host }}
      http:
        paths:
          - path: /auth/admin
            pathType: Prefix
            backend:
              service:
                name: block-service
                port:
                  number: 80
---
{{- end }}
{{- end }}



----------
{{- if and .Values.enableBlockService .Values.adminIngressRules }}
{{- if .Values.adminIngressRules }}
apiVersion: v1
kind: Service
metadata:
  name: block-service
  namespace: {{ .Release.Namespace }}
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: block
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: block
  namespace: {{ .Release.Namespace }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: block
  template:
    metadata:
      labels:
        app: block
    spec:
      containers:
        - name: block
          image: nginx:alpine
          ports:
            - containerPort: 80
          command: ["sh", "-c"]
          args: ["echo '<h1>403 Forbidden</h1>' > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'"]
{{- end }}
{{- end }}
---


enableBlockService: true



-----
# existing ingress section
ingress:
  enabled: true
  ingressClassName: nginx
  servicePort: http
  ...

# NEW: separate admin control block
adminIngressRules:
  - host: admin-blocked.example.com
    blockAdminConsole: true

  - host: admin-allowed.example.com
    blockAdminConsole: false



----
templates/ingress-admin-block.yaml
-------
{{- range .Values.adminIngressRules }}
{{- if .blockAdminConsole }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "keycloak.fullname" $ }}-adminblock-{{ .host | replace "." "-" }}
  namespace: {{ $.Release.Namespace }}
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($request_uri ~* "^/auth/admin") {
        return 403;
      }
spec:
  ingressClassName: {{ $.Values.ingress.ingressClassName }}
  rules:
    - host: {{ .host }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: {{ include "keycloak.fullname" $ }}-http
                port:
                  name: {{ $.Values.ingress.servicePort }}
---
{{- end }}
{{- end }}





--------
Dear Akamai Support Team,

I hope you're well.

We are currently integrating the Keycloak Admin Console into our internal infrastructure, and weâ€™ve identified that HTTP PUT requests are being blocked at the Akamai layer for our Keycloak endpoint:

https://test-keycloak.fiscloudservices.com
These PUT requests are essential for several administrative operations, including:

Updating realm configurations (e.g., theme selection, login settings)
Managing user credentials (e.g., setting/resetting passwords)
Applying client and policy updates through the admin UI
At the moment, these operations are failing with 501 Not Implemented responses,



--------------
extraEnv:
  - name: KC_LOG_LEVEL
    value: DEBUG
  - name: KC_LOG_LEVEL_ORG_KEYCLOAK_SERVICES_RESOURCES_ADMIN
    value: DEBUG
  - name: KC_LOG_LEVEL_ORG_KEYCLOAK_STORAGE_USERSTORAGEPROVIDER
    value: DEBUG
  - name: KC_LOG_LEVEL_ORG_KEYCLOAK_AUTHENTICATION
    value: DEBUG
  - name: KC_LOG_LEVEL_ORG_KEYCLOAK_EVENTS
    value: DEBUG
  - name: KC_LOG_LEVEL_ORG_KEYCLOAK_MODELS
    value: DEBUG



---------
/* Debug banner to confirm CSS loaded */
body:before {
  content: "âœ… custom.css loaded";
  display: block;
  color: green;
  font-size: 14px;
  background: #fff7d4;
  padding: 8px;
}

/* Ensure parent container is positioned relative */
#kc-form-password,
.form-group.password {
  position: relative;
}

/* Style the Show/Hide password button */
input[name="password"] + button[data-password-toggle] {
  position: absolute !important;
  top: 50% !important;
  right: 10px !important;
  transform: translateY(-50%) !important;
  background: #f0f0f0 !important;
  border: 1px solid #ccc !important;
  border-radius: 4px !important;
  color: #333 !important;
  font-size: 0.9rem !important;
  padding: 4px 8px !important;
  cursor: pointer !important;
  z-index: 10 !important;
}

/* Add space inside input box for the button */
input[name="password"] {
  padding-right: 60px !important;
}


--------
body:before {
  content: "âœ… custom.css loaded";
  display: block;
  color: green;
  font-size: 14px;
  background: #fff7d4;
  padding: 8px;
}

/* Real selector for your password toggle */
input[name="password"] + button[data-password-toggle] {
  position: absolute !important;
  top: 50% !important;
  right: 12px !important;
  transform: translateY(-50%) !important;
  background: transparent !important;
  color: #000 !important;
  font-size: 1rem !important;
  z-index: 5 !important;
  border: 1px solid #ccc !important;
}

/* Ensure space inside the input for the button */
input[name="password"] {
  padding-right: 40px !important;
}




------
/* =============================================
   Style: Show/Hide Password Inline Toggle Button
   Location: resources/css/login.css
   ============================================= */

/* Ensure container can position children absolutely */
#kc-form-password,
.form-group.password {
  position: relative;
}

/* Style the toggle button */
button[data-password-toggle] {
  position: absolute;
  top: 50%;
  right: 12px;
  transform: translateY(-50%);
  background: transparent;
  border: none;
  font-size: 1rem;
  font-weight: bold;
  color: #555;
  cursor: pointer;
  padding: 4px;
  line-height: 1;
  z-index: 2;
}

/* Optional hover/focus styles for better UX */
button[data-password-toggle]:hover,
button[data-password-toggle]:focus {
  color: #000;
  outline: none;
}

/* Replace emoji with elegant Unicode icon */
button[data-password-toggle]::before {
  content: 'ðŸ”“'; /* Use open lock as the default symbol */
  margin-right: 4px;
}

/* Optional: if data-password="hidden" toggle is used dynamically in JS, this switches icons */
/*
button[data-password-toggle][data-password="hidden"]::before {
  content: 'ðŸ”’';  // Closed lock when password is hidden
}
*/

/* Adjust input padding if needed to make room inside the field */
input[name="password"] {
  padding-right: 40px;
}




---------
locals {
  allowed_ingress_ips = jsondecode(file("${path.module}/allowed_ips.json"))
}

resource "aws_security_group_rule" "alb_ingress_whitelist" {
  for_each = toset(local.allowed_ingress_ips)

  type              = "ingress"
  from_port         = 443
  to_port           = 443
  protocol          = "tcp"
  cidr_blocks       = [each.key]
  security_group_id = aws_security_group.test_alb_sec_grp.id
  description       = "Allow HTTPS from ${each.key}"
}

output "allowed_ingress_ips_checksum" {
  value = filesha256("${path.module}/allowed_ips.json")
}



[
  "1.2.3.4/32",
  "5.6.7.8/32",
  "203.0.113.45/32",
  "198.51.100.12/32",
  "192.168.10.0/24"
]



------------
Meeting Summary
Subject: GitHub Migration â€“ DTS & EDP Organizations
Date: [Insert Date of Call]
Attendees: [Optional: List names if needed]

Summary:
Thank you for the call today. Below is a summary of the key discussion points and next steps related to the migration of DTS and EDP organizations from github.worldpay.com to FIS.github.com.

Key Points Discussed:

Migration Ownership:
Kailash and team will continue to lead the migration efforts from github.worldpay.com to FIS.github.com.
Access to Existing Repositories:
We will retain access to github.worldpay.com repositories until the end of June 2025.
Post-Migration Validation (Target: End of May 2025):
Once migration is complete, we will:
Validate and test the new repositories.
Update our CI/CD pipelines to reference the new repository locations.
Ensure there are no issues impacting production support.
Decommissioning Legacy Access:
Upon successful validation, we will formally remove our dependency on the old repositories.
Access to github.worldpay.com will be revoked in a phased manner to monitor for any remaining dependencies.
The goal is to ensure a clean and complete transition by the end of June 2025.
Next Steps:

Migration to continue under Kailashâ€™s ownership.
Our team to complete testing and pipeline updates by end of May 2025.
Plan phased deactivation of github.worldpay.com access post-validation.




-------------------------
FROM node:18-alpine

RUN apk add --no-cache unzip wget

# Create non-root user
RUN addgroup -S keycloakthemes && \
    adduser -S -u 1001 -G keycloakthemes keycloakthemes

USER keycloakthemes
WORKDIR /home/keycloakthemes

# Copy your Keycloak themes
COPY containers/keycloak-themes/keycloak-theme/theme /opt/keycloak/themes/

# Install PatternFly and download full Zocial assets (css + fonts)
RUN npm init -y && \
    npm install patternfly && \
    wget -q https://github.com/smcllns/css-social-buttons/archive/refs/tags/v1.4.0.zip -O /tmp/zocial.zip && \
    unzip -q /tmp/zocial.zip -d /tmp && \
    for theme in worldpay aric ethos prod-local; do \
      mkdir -p /opt/keycloak/themes/$theme/login/resources/patternfly && \
      mkdir -p /opt/keycloak/themes/$theme/login/resources/zocial && \
      # Copy PatternFly
      cp -r node_modules/patternfly/dist/* /opt/keycloak/themes/$theme/login/resources/patternfly/ && \
      # Copy all Zocial CSS + fonts
      cp /tmp/css-social-buttons-1.4.0/css/* /opt/keycloak/themes/$theme/login/resources/zocial/; \
    done


styles=patternfly/css/patternfly.min.css patternfly/css/patternfly-additions.min.css zocial/zocial.css css/login.css css/styles.css


-------
FROM node:18-alpine

# Set working directory (optional, can be anywhere since we use absolute paths)
WORKDIR /tmp

# Copy theme files into the correct path
COPY keycloak-themes/keycloak-theme/ /worldpay-themes/

# Install dependencies and download shared CSS files as root
RUN npm install patternfly && \
    mkdir -p /tmp/zocial && \
    wget -q https://raw.githubusercontent.com/samcollins/css-social-buttons/master/zocial.css -O /tmp/zocial/zocial.css

# Create a non-root user and ensure proper permissions on /worldpay-themes
RUN adduser -S -u 1001 keycloakthemes && \
    chown -R keycloakthemes:keycloakthemes /worldpay-themes

# Switch to non-root user
USER keycloakthemes

# Copy shared CSS into each theme
RUN for theme in worldpay aric ethos prod-local; do \
      mkdir -p /worldpay-themes/$theme/login/resources/css && \
      cp /tmp/zocial/zocial.css \
         node_modules/patternfly/dist/css/patternfly.min.css \
         node_modules/patternfly/dist/css/patternfly-additions.min.css \
         /worldpay-themes/$theme/login/resources/css/; \
    done




------
FROM node:18-alpine

# Create a safe non-root user
RUN addgroup -S keycloakthemes && \
    adduser -S -u 1001 -G keycloakthemes keycloakthemes

USER keycloakthemes
WORKDIR /home/keycloakthemes

# Copy your existing themes into the image
COPY containers/keycloak-themes/keycloak-theme/theme /opt/keycloak/themes/

# Install frontend dependencies and inject CSS into each theme
RUN npm init -y && \
    npm install patternfly && \
    mkdir -p /tmp/zocial && \
    wget -q https://raw.githubusercontent.com/samcollins/css-social-buttons/master/zocial.css -O /tmp/zocial/zocial.css && \
    for theme in worldpay aric ethos prod-local; do \
      mkdir -p /opt/keycloak/themes/$theme/login/resources/css && \
      cp /tmp/zocial/zocial.css \
         node_modules/patternfly/dist/css/patternfly.min.css \
         node_modules/patternfly/dist/css/patternfly-additions.min.css \
         /opt/keycloak/themes/$theme/login/resources/css/; \
    done


-----
FROM node:18-alpine

# 1. Add user & group only once
RUN addgroup -S keycloakthemes && \
    adduser -S -u 1000 -G keycloakthemes keycloakthemes

# 2. Use that user from here onward
USER keycloakthemes
WORKDIR /home/keycloakthemes

# 3. Copy all themes into /opt
COPY containers/keycloak-themes/keycloak-theme/theme /opt/keycloak/themes/

# 4. Install deps and embed CSS into each theme
RUN npm init -y && \
    npm install patternfly zocial-css && \
    for theme in worldpay aric ethos prod-local; do \
      mkdir -p /opt/keycloak/themes/$theme/login/resources/css && \
      cp node_modules/patternfly/dist/css/patternfly.min.css \
         node_modules/patternfly/dist/css/patternfly-additions.min.css \
         node_modules/zocial-css/css/zocial.css \
         /opt/keycloak/themes/$theme/login/resources/css/; \
    done



-----
FROM node:18-alpine

# Create the user once
RUN addgroup -S keycloakthemes && \
    adduser -S -u 1000 -G keycloakthemes keycloakthemes

USER keycloakthemes
WORKDIR /home/keycloakthemes

# Copy themes
COPY containers/keycloak-themes/keycloak-theme/theme /worldpay-themes/

# Install frontend dependencies and copy shared CSS to each theme
RUN npm init -y && \
    npm install patternfly zocial-css && \
    for theme in worldpay aric ethos prod-local; do \
      mkdir -p /worldpay-themes/$theme/login/resources/css && \
      cp node_modules/patternfly/dist/css/patternfly.min.css \
         node_modules/patternfly/dist/css/patternfly-additions.min.css \
         node_modules/zocial-css/css/zocial.css \
         /worldpay-themes/$theme/login/resources/css/; \
    done


---
FROM node:18-alpine

# Safe user creation
RUN addgroup -S keycloakthemes || true && \
    adduser -S -u 1000 -G keycloakthemes keycloakthemes || true

USER keycloakthemes
WORKDIR /home/keycloakthemes

# Copy all themes into a temp location
COPY containers/keycloak-themes/keycloak-theme/theme /opt/keycloak/themes/

# Install frontend dependencies
RUN npm init -y && \
    npm install patternfly zocial-css && \
    for theme in worldpay aric ethos prod-local; do \
      mkdir -p /opt/keycloak/themes/$theme/login/resources/css && \
      cp node_modules/patternfly/dist/css/patternfly.min.css \
         node_modules/patternfly/dist/css/patternfly-additions.min.css \
         node_modules/zocial-css/css/zocial.css \
         /opt/keycloak/themes/$theme/login/resources/css/; \
    done



-----
styles=css/patternfly.min.css css/patternfly-additions.min.css css/zocial.css css/login.css css/styles.css



----
FROM node:18-alpine

# Setup user and working directory
RUN addgroup -S keycloakthemes && adduser -S -u 1000 -G keycloakthemes keycloakthemes
USER keycloakthemes
WORKDIR /home/keycloakthemes

# Copy all theme folders
COPY containers/keycloak-themes/keycloak-theme/theme /opt/keycloak/themes/

# Install required frontend CSS packages
RUN npm init -y && \
    npm install patternfly zocial-css && \
    mkdir -p /opt/keycloak/themes/common-resources/css && \
    cp node_modules/patternfly/dist/css/patternfly.min.css \
       node_modules/patternfly/dist/css/patternfly-additions.min.css \
       node_modules/zocial-css/css/zocial.css \
       /opt/keycloak/themes/common-resources/css/


---

styles=../common-resources/css/patternfly.min.css \
       ../common-resources/css/patternfly-additions.min.css \
       ../common-resources/css/zocial.css \
       css/login.css \
       css/styles.css
---
---

------






-------
FROM node:18-alpine

# Create user for consistency with your base image
RUN addgroup -S keycloakthemes && adduser -S -u 1000 -G keycloakthemes keycloakthemes
USER keycloakthemes
WORKDIR /home/keycloakthemes

# Copy your custom Keycloak theme first
COPY containers/keycloak-themes/keycloak-theme/theme/worldpay /opt/keycloak/themes/worldpay

# Init npm and install required frontend packages
RUN npm init -y && \
    npm install patternfly zocial-css && \
    mkdir -p /opt/keycloak/themes/worldpay/login/resources/css && \
    cp node_modules/patternfly/dist/css/patternfly.min.css \
       node_modules/patternfly/dist/css/patternfly-additions.min.css \
       node_modules/zocial-css/css/zocial.css \
       /opt/keycloak/themes/worldpay/login/resources/css/



----
styles=css/patternfly.min.css css/patternfly-additions.min.css css/zocial.css css/login.css css/styles.css






---------
curl -X POST https://login.microsoftonline.com/e3f19d08-34c8-4b15-a0b4-18916aa6c575/oauth2/v2.0/token \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "grant_type=client_credentials" \
  -d "client_id=ca8b52f7-0708-44bb-bd07-b692ae7449ff" \
  -d "client_secret=51e64f0c-8c56-4440-86b2-748d23409dea" \
  -d "scope=https://graph.microsoft.com/.default"



----
INSERT INTO user_role_mapping (user_id, role_id)
SELECT u.id, r.id
FROM user_entity u, keycloak.role r
WHERE u.username = 'keycloakadmnew'
  AND u.realm_id = '633a1156-997c-46d9-babe-6c62241c466f'
  AND r.name = 'realm-admin'
  AND r.client_role = true
  AND r.client_id = '6d1fc753-6d23-43b0-aeb1-2debb1728b86';



----------



-- Grant full realm admin privileges from the realm-management client
INSERT INTO user_role_mapping (user_id, role_id)
SELECT u.id, r.id
FROM user_entity u, keycloak.role r
WHERE u.username = 'keycloakadmnew'
  AND r.client_role = true
  AND r.name = 'realm-admin'
  AND r.client_id = (
    SELECT id FROM client
    WHERE client_id = 'realm-management' AND realm_id = 'your_realm_id'
  );



----
ThisIsNewBeginning@EthFIS
-- 1. Create new user
INSERT INTO user_entity (
  id, email, email_constraint, enabled, email_verified,
  federation_link, realm_id, username
) VALUES (
  gen_random_uuid(), 'keycloakadmnew@temp.local', 'keycloakadmnew@temp.local', true, true,
  NULL, 'your_realm_id', 'keycloakadmnew'
);

-- 2. Add password (PBKDF2-based)
INSERT INTO credential (
  id, type, created_date, user_id, secret_data, credential_data, priority
) VALUES (
  gen_random_uuid(), 'password', extract(epoch from now()) * 1000,
  (SELECT id FROM user_entity WHERE username = 'keycloakadmnew'),
  '{"value":"C0kHwNc/7jhmD8u3OQE1IfZmMhcV7Ta9J62/TQ/YCFZvlT2+t213yj5/7dWFe73iHmmXwcdZb+abF8P8Q0DzIA==", "salt":"GTtv5VMXRDXO3KkUyq2Hkg=="}',
  '{"hashIterations":27500,"algorithm":"pbkdf2-sha256","additionalParameters":{}}',
  0
);

-- 3. Assign admin role
INSERT INTO user_role_mapping (user_id, role_id)
SELECT
  (SELECT id FROM user_entity WHERE username = 'keycloakadmnew'),
  id
FROM keycloak.role
WHERE name = 'admin' AND realm_id = 'your_realm_id';


---
/opt/keycloak/bin/kcadm.sh config credentials --server http://localhost:8080/auth --realm master --user keycloakadmin --password <pwd>



----
DELETE FROM credential WHERE user_id = '<user_id>';

INSERT INTO credential (
  id, type, created_date, user_id, secret_data, credential_data, priority
) VALUES (
  gen_random_uuid(), 'password', extract(epoch from now()) * 1000,
  '<user_id>',
  '{"value":"UyOaezXWyCUX3lDYaQlGoN5HOq5F3SRX0Swkl4hIH4nr+IdJxtqLs9YV8rUu7u4tQo+dW9IvN6a1E4G/Ci8Bfg==", "salt":"kyFfTdoKlpUgaPikzA0iwA=="}',
  '{"hashIterations":27500,"algorithm":"pbkdf2-sha256","additionalParameters":{}}',
  0
);


---
INSERT INTO credential (
  id, type, created_date, user_id, secret_data, credential_data
) VALUES (
  gen_random_uuid(), 'password', extract(epoch from now()) * 1000,
  '<user_id>',
  '{"value":"<base64_encoded_hash>"}',
  '{"hashIterations":27500,"algorithm":"pbkdf2-sha256","salt":"<base64_encoded_salt>","additionalParameters":{}}'
);


----
import hashlib, os, base64
from hashlib import pbkdf2_hmac

password = b"NewSecurePassword123"
salt = os.urandom(16)
iterations = 27500
key = pbkdf2_hmac('sha256', password, salt, iterations, dklen=64)

print("value:", base64.b64encode(key).decode())
print("salt:", base64.b64encode(salt).decode())


----
INSERT INTO credential (
  id, type, created_date, user_id, secret_data, credential_data
) VALUES (
  gen_random_uuid(), 'password', extract(epoch from now()) * 1000,
  '1234abcd-5678-efgh-ijkl-9876mnopqrst',
  '{"value":"$2y$10$D9YfkwrhPlYJkRZHO1u1Ne7RMzD8OxPGU4kVrZC2U6jMeD5uQiVXa"}',
  '{"hashIterations":27500,"algorithm":"bcrypt"}'
);


------
Subject: Confirmation of Access and Responsibilities Post Cut-over

Current State:
We currently have access to github.worldpay.com, with full ability to view, edit, and manage the repositories hosted there.

Final State (Post Cut-over):
You will proceed with your cut-over as planned. However, our access to github.worldpay.com will remain unchanged. The repositories will not be archived and will continue to be accessible in their current state.

Actions and Responsibilities:

We will retain the ability to make updates to the existing repositories, particularly in case of emergency or critical requests.
We take full responsibility for keeping these repositories in sync with the new repositories following your cut-over.



------------
Option 1: Extract the dockerconfigjson and manually test

Your Kubernetes docker-registry type secret (jfrog-docker-secret) contains a base64-encoded .dockerconfigjson.
You can decode it locally and try a docker login manually.

Here's how:

Step 1: Get the current secret
kubectl get secret jfrog-docker-secret -n keycloak-eda -o jsonpath="{.data.\.dockerconfigjson}" | base64 --decode > dockerconfig.json
âœ… This will create a local dockerconfig.json file.

Step 2: Inspect the dockerconfig.json
Open it:

cat dockerconfig.json
You should see something like:

{
  "auths": {
    "edas-docker-snapshot-local.docker.fis.dev": {
      "username": "your-username",
      "password": "your-password-or-token",
      "auth": "base64(username:password)"
    }
  }
}
âœ… Confirm the registry hostname matches. âœ… Confirm username/password look correct.

Step 3: Try Docker Login Using Extracted Details
Run:

docker login edas-docker-snapshot-local.docker.fis.dev
It will ask for username and password:

Use the username from the JSON
Use the password from the JSON
If login succeeds âœ…, your secret is correct.
If login fails âŒ (401 Unauthorized), your secret is bad and needs recreation.

Option 2: Simulate Kubernetes Pull with Test Pod

If you want, you can also launch a dummy pod to test the pull separately.

Sample YAML:

apiVersion: v1
kind: Pod
metadata:
  name: test-image-pull
spec:
  containers:
  - name: test
    image: edas-docker-snapshot-local.docker.fis.dev/test/hydra-custom-keycloak-themes:0.0.1
    command: ["sleep", "3600"]
  imagePullSecrets:
  - name: jfrog-docker-secret
  restartPolicy: Never
Apply:

kubectl apply -f test-image-pull.yaml -n keycloak-eda
Then check:

kubectl describe pod test-image-pull -n keycloak-eda
âœ… If it pulls successfully, your secret is good. âŒ If it fails with 401 Unauthorized, secret is wrong.






--------
I hope you are doing well.

To provide you with some context, we are currently hosting our Ethos Keycloak application inside our EDA cluster. As part of our ongoing initiatives, we are now migrating Ethos Keycloak into a CIO-managed AWS account.

At present, our Keycloak application is integrated with the worldpay.local LDAP server. When we reached out to the team managing the LDAP service, we were provided with the updated server details listed below.

As part of the migration, we would like to continue leveraging this LDAP service from within our CIO-managed AWS accounts. Could you please advise if this approach is recommended? If there are any concerns or limitations with using the LDAP server in this way, we would appreciate your suggestions on a better or more appropriate alternative.

Looking forward to your guidance.


import pandas as pd
from pathlib import Path

# Load cost summary
cost_df = pd.read_csv("active_services_cost.csv")

# Collect all service CSVs
service_csvs = {
    csv.stem.replace("aws_", "").replace("_", " ").lower(): csv
    for csv in Path(".").glob("*.csv")
    if csv.name != "active_services_cost.csv"
}

# Size-related fields (with unit divisor)
size_fields = {
    "s3": ("bucket_size_bytes", 1e9),
    "rds db instance": ("allocated_storage", 1),
    "ebs volume": ("size", 1),
    "dynamodb table": ("table_size_bytes", 1e9),
    "ecr repository": ("image_size_bytes", 1e9)
}

summary_rows = []

for index, row in cost_df.iterrows():
    service_name = row["service"]
    cost = row["cost_usd"]

    # Match based on simplified names
    normalized_service = service_name.lower().replace("-", " ").replace("  ", " ").strip()
    matched_file = None

    for key, csv_file in service_csvs.items():
        if key in normalized_service:
            matched_file = csv_file
            break

    resource_count = 0
    total_storage = 0
    region_count = 0
    last_updated = None

    if matched_file and matched_file.exists():
        try:
            df = pd.read_csv(matched_file, on_bad_lines="skip")
            resource_count = len(df)
            region_count = len(df["region"].unique()) if "region" in df.columns else 0
            if "launch_time" in df.columns:
                df["launch_time"] = pd.to_datetime(df["launch_time"], errors="coerce")
                last_updated = df["launch_time"].max()
            # Size if available
            for key, (size_col, div) in size_fields.items():
                if key in matched_file.stem and size_col in df.columns:
                    total_storage = round(df[size_col].fillna(0).astype(float).sum() / div, 2)
        except Exception as e:
            print(f"Skipped {matched_file.name}: {e}")

    summary_rows.append({
        "Service": service_name,
        "Cost (USD)": round(cost, 2),
        "Resource Count": resource_count,
        "Total Storage (GB)": total_storage,
        "Regions Used": region_count,
        "Last Updated": last_updated
    })

# Final summary frame
summary_df = pd.DataFrame(summary_rows)
summary_df = summary_df.sort_values(by="Cost (USD)", ascending=False)

# Write output Excel
with pd.ExcelWriter("aws_inventory_report_enhanced.xlsx", engine="xlsxwriter") as writer:
    summary_df.to_excel(writer, sheet_name="Summary", index=False)

    for csv_file in Path(".").glob("*.csv"):
        if csv_file.name == "active_services_cost.csv":
            continue
        try:
            df = pd.read_csv(csv_file, on_bad_lines="skip")
            df.to_excel(writer, sheet_name=csv_file.stem[:31], index=False)
        except Exception as e:
            print(f"Failed to include {csv_file.name}: {e}")

print("âœ… Smart enhanced Excel report generated: aws_inventory_report_enhanced.xlsx")






------
-- Comprehensive EC2 Instance Inventory with EBS Volume Details and Cost Information
WITH ec2_service_costs AS (
  SELECT
    usage_type,
    resource_id,
    SUM(unblended_cost_amount) as cost_usd
  FROM
    aws_cost_by_service_usage_type_daily
  WHERE
    service = 'Amazon Elastic Compute Cloud - Compute'
    AND period_start >= current_date - interval '30 days'
    AND resource_id IS NOT NULL
    AND resource_id != ''
  GROUP BY
    usage_type, resource_id
),
ec2_tag_costs AS (
  SELECT
    resource_id,
    SUM(unblended_cost_amount) as cost_usd
  FROM
    aws_cost_by_tag
  WHERE
    service = 'Amazon Elastic Compute Cloud - Compute'
    AND period_start >= current_date - interval '30 days'
    AND resource_id IS NOT NULL
    AND resource_id != ''
  GROUP BY
    resource_id
)
SELECT
  -- EC2 Instance Details
  i.instance_id,
  i.instance_type,
  i.instance_state,
  i.availability_zone,
  i.region,
  i.tags,
  i.launch_time,
  i.private_ip_address,
  i.public_ip_address,
  i.vpc_id,
  i.subnet_id,
  
  -- Instance Usage and Last Activity
  i.state_transition_time as last_state_change,
  i.cpu_options_core_count as cpu_cores,
  i.cpu_options_threads_per_core as threads_per_core,
  i.metadata_options_http_endpoint as metadata_endpoint,
  
  -- EBS Volume Details
  v.volume_id,
  v.volume_type,
  v.size as volume_size_gb,
  v.iops,
  v.throughput,
  v.encrypted as volume_encrypted,
  v.state as volume_state,
  v.create_time as volume_create_time,
  v.tags as volume_tags,
  
  -- Cost information (last 30 days)
  COALESCE(usage_costs.cost_usd, 0) as instance_cost_by_usage_30_days,
  COALESCE(tag_costs.cost_usd, 0) as instance_cost_by_tag_30_days,
  (COALESCE(usage_costs.cost_usd, 0) + COALESCE(tag_costs.cost_usd, 0)) as total_estimated_cost_30_days
FROM
  aws_ec2_instance as i
LEFT JOIN
  aws_ebs_volume as v
  ON v.attachments @> ANY(ARRAY[jsonb_build_object('InstanceId', i.instance_id)])
LEFT JOIN
  ec2_service_costs usage_costs
  ON usage_costs.resource_id = i.instance_id
LEFT JOIN
  ec2_tag_costs tag_costs
  ON tag_costs.resource_id = i.instance_id
WHERE
  i.region = 'us-east-1'  -- Change this to your desired region
ORDER BY
  total_estimated_cost_30_days DESC,
  i.instance_id, v.volume_id;


-------
Option 1: Using CloudTrail Events

Tracks last interaction with EC2 instances.

select
  recipient_account_id,
  event_name,
  event_time,
  user_identity ->> 'arn' as user,
  resources -> 0 ->> 'ARN' as resource_arn
from
  aws_cloudtrail_event
where
  event_source = 'ec2.amazonaws.com'
  and event_name in ('StartInstances', 'StopInstances', 'RebootInstances', 'TerminateInstances')
order by
  event_time desc
limit 50;
ðŸ” What you get:
Most recent EC2 actions per account
Good for admin-triggered events
âœ… Option 2: Using CloudWatch Metrics

Tracks actual usage like CPU or network activity.

select
  namespace,
  metric_name,
  resource_id,
  max(datapoints -> 0 ->> 'Timestamp') as last_active_time
from
  aws_cloudwatch_metric_statistics_set
where
  namespace = 'AWS/EC2'
  and metric_name = 'CPUUtilization'
group by
  namespace, metric_name, resource_id
order by
  last_active_time desc
limit 50;




-----------
select
  instance_id,
  instance_type,
  region,
  availability_zone,
  state,
  launch_time,
  private_ip_address,
  public_ip_address,
  tags
from
  aws_ec2_instance
where
  region = 'us-east-1'
limit 10;

------------------

import pandas as pd
from pathlib import Path

# Define the CSV files
cost_summary_file = Path("active_services_cost.csv")
service_files = list(Path(".").glob("*.csv"))
service_files.remove(cost_summary_file)  # remove summary from service list

# Load cost summary
cost_df = pd.read_csv(cost_summary_file)

# Start writing Excel report
with pd.ExcelWriter("aws_inventory_report.xlsx", engine="xlsxwriter") as writer:
    # Write cost summary first
    cost_df.to_excel(writer, sheet_name="Cost Summary", index=False)

    # Write each service file into its own sheet
    for csv_file in service_files:
        try:
            df = pd.read_csv(csv_file)
            sheet_name = csv_file.stem[:31]  # Excel limit
            df.to_excel(writer, sheet_name=sheet_name, index=False)
        except Exception as e:
            print(f"Failed to write {csv_file}: {e}")

print("âœ… Excel report generated: aws_inventory_report.xlsx")

# Generate basic HTML version of the cost summary
html_content = cost_df.to_html(index=False, border=0, classes="table table-striped", justify="center")

# Wrap it in some minimal styling
html_final = f"""
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>AWS Inventory Cost Summary</title>
  <style>
    body {{ font-family: sans-serif; margin: 40px; }}
    .table {{ width: 80%; margin: auto; border-collapse: collapse; }}
    .table td, .table th {{ padding: 8px 12px; border: 1px solid #ddd; }}
    .table th {{ background-color: #f4f4f4; text-align: center; }}
    .table-striped tr:nth-child(even) {{ background-color: #f9f9f9; }}
  </style>
</head>
<body>
  <h2 style="text-align:center;">AWS Inventory Cost Summary (Last 30 Days)</h2>
  {html_content}
</body>
</html>
"""

Path("aws_inventory_summary.html").write_text(html_final)
print("âœ… HTML summary generated: aws_inventory_summary.html")




--------------
import subprocess
import pandas as pd
import json
from pathlib import Path

# === Step 1: Get active services from Steampipe cost table (final query) ===
print("Fetching active services with cost > 0...")
cost_query = """
select
  service,
  sum(unblended_cost_amount) as cost_usd
from
  aws_cost_by_service_monthly
where
  period_start >= current_date - interval '30 days'
group by
  service
having sum(unblended_cost_amount) > 0
order by
  cost_usd desc;
"""

result = subprocess.run(
    ["steampipe", "query", "--output=json"],
    input=cost_query,
    text=True,
    capture_output=True
)

if result.returncode != 0:
    print("Failed to fetch cost data:", result.stderr)
    exit(1)

cost_data = json.loads(result.stdout)
if "rows" not in cost_data:
    print("Error: No 'rows' in Steampipe output.")
    exit(1)

rows = cost_data["rows"]
if len(rows) == 0:
    print("No active services with cost > 0 found.")
    exit(0)

cost_df = pd.DataFrame(rows)
cost_df.to_csv("active_services_cost.csv", index=False)
print("âœ… Saved active services with cost > 0 to active_services_cost.csv")

# === Step 2: Query inventory per service across known regions ===
regions = ["us-east-1", "eu-west-1", "eu-west-2"]

# Expanded service mappings
service_table_map = {
    "EC2 - Other": "aws_ec2_instance",
    "Amazon Elastic Compute Cloud - Compute": "aws_ec2_instance",
    "Amazon Virtual Private Cloud": "aws_vpc",
    "Amazon CloudWatch": "aws_cloudwatch_log_group",
    "Amazon Simple Storage Service": "aws_s3_bucket",
    "Amazon Relational Database Service": "aws_rds_db_instance",
    "Amazon DynamoDB": "aws_dynamodb_table",
    "AWS Lambda": "aws_lambda_function",
    "AWS Key Management Service": "aws_kms_key",
    "AWS Secrets Manager": "aws_secretsmanager_secret",
    "Elastic Load Balancing": "aws_elbv2_load_balancer",
    "Amazon SNS": "aws_sns_topic",
    "Amazon SQS": "aws_sqs_queue",
    "Amazon Route 53": "aws_route53_zone",
    "AWS Directory Service": "aws_directory_service_directory",
    "AWS Config": "aws_config_configuration_recorder",
    "AWS CloudFormation": "aws_cloudformation_stack",
    "AWS Identity and Access Management": "aws_iam_role",
    "Amazon EC2 Container Registry (ECR)": "aws_ecr_repository",
    "Amazon Elastic Container Service": "aws_ecs_cluster",
    "Amazon GuardDuty": "aws_guardduty_detector"
}

queried_services = []

for _, row in cost_df.iterrows():
    service = row["service"]
    if service not in service_table_map:
        continue

    table = service_table_map[service]

    for region in regions:
        sql = f"select * from {table} where region = '{region}' limit 1000;"

        out_file = f"{table.replace('aws_', '')}_{region}.csv"
        print(f"Querying {service} ({table}) in {region}...")

        result = subprocess.run(
            ["steampipe", "query", "--output=csv"],
            input=sql,
            text=True,
            capture_output=True
        )

        if result.returncode == 0:
            Path(out_file).write_text(result.stdout)
            queried_services.append((service, table, region))
        else:
            print(f"Failed to query {table} in {region}: {result.stderr}")

# === Step 3: Done ===
print("âœ… All done. Queried services:")
for s, t, r in queried_services:
    print(f"  - {s} ({t}) in {r}")




-------
cost_data = json.loads(result.stdout)

# Validate the structure
if not cost_data or "columns" not in cost_data[0] or "rows" not in cost_data[0]:
    print("Error: Unexpected or empty response from Steampipe.")
    print("Raw output:", result.stdout)
    exit(1)

columns = cost_data[0]["columns"]
rows = cost_data[0]["rows"]

# Build DataFrame safely
if len(rows) == 0:
    print("No active services with cost > 0 found.")
    exit(0)

cost_df = pd.DataFrame(rows, columns=columns)
cost_df.to_csv("active_services_cost.csv", index=False)
print("âœ… Saved active services with cost > 0 to active_services_cost.csv")




-------------
import subprocess
import pandas as pd
import json
from pathlib import Path

# === Step 1: Get active services from Steampipe cost table (final query) ===
print("Fetching active services with cost > 0...")
cost_query = """
select
  service,
  sum(unblended_cost_amount) as cost_usd
from
  aws_cost_by_service_monthly
where
  period_start >= current_date - interval '30 days'
group by
  service
having sum(unblended_cost_amount) > 0
order by
  cost_usd desc;
"""

result = subprocess.run(
    ["steampipe", "query", "--output=json"],
    input=cost_query,
    text=True,
    capture_output=True
)

if result.returncode != 0:
    print("Failed to fetch cost data:", result.stderr)
    exit(1)

cost_data = json.loads(result.stdout)
cost_df = pd.DataFrame(cost_data)
cost_df.to_csv("active_services_cost.csv", index=False)
print("Saved active services with cost > 0 to active_services_cost.csv")

# === Step 2: Query inventory per service across known regions ===
regions = ["us-east-1", "eu-west-1", "eu-west-2"]

service_table_map = {
    "Amazon EC2": "aws_ec2_instance",
    "Amazon S3": "aws_s3_bucket",
    "Amazon RDS": "aws_rds_db_instance",
    "Amazon DynamoDB": "aws_dynamodb_table",
    "AWS Lambda": "aws_lambda_function",
    "AWS Key Management Service": "aws_kms_key",
    "AWS Secrets Manager": "aws_secretsmanager_secret",
    "Elastic Load Balancing": "aws_elbv2_load_balancer",
    "Amazon CloudWatch": "aws_cloudwatch_log_group",
    "Amazon SNS": "aws_sns_topic",
    "Amazon SQS": "aws_sqs_queue",
    "AWS CloudFormation": "aws_cloudformation_stack",
    "AWS Identity and Access Management": "aws_iam_role"
}

queried_services = []

for _, row in cost_df.iterrows():
    service = row["service"]
    if service not in service_table_map:
        continue

    table = service_table_map[service]

    for region in regions:
        sql = f"select * from {table} where region = '{region}' limit 1000;"

        out_file = f"{table.replace('aws_', '')}_{region}.csv"
        print(f"Querying {service} ({table}) in {region}...")

        result = subprocess.run(
            ["steampipe", "query", "--output=csv"],
            input=sql,
            text=True,
            capture_output=True
        )

        if result.returncode == 0:
            Path(out_file).write_text(result.stdout)
            queried_services.append((service, table, region))
        else:
            print(f"Failed to query {table} in {region}: {result.stderr}")

# === Step 3: Done ===
print("âœ… All done. Queried services:")
for s, t, r in queried_services:
    print(f"  - {s} ({t}) in {r}")





----------
select
  service,
  round(sum(unblended_cost), 2) as cost_usd
from
  aws_cost_by_service_monthly
where
  start_time >= current_date - interval '30 days'
group by
  service
having sum(unblended_cost) > 0
order by
  cost_usd desc;



----
Run the queries via Steampipe:
steampipe query --output csv --file queries.sql
This will create multiple .csv files in your folder.
Generate the final report:
python generate_report.py
You'll get:
aws_inventory_report.xlsx â€” for Excel users
aws_inventory_report.html â€” viewable in any browser




------
Access Verification
 Verify access to new GitHub repositories (UI + CLI).
 Request missing access if needed.
CI/CD Service Account Setup
 Verify existing service accounts have access to new repos.
 Create/update service accounts or tokens as needed.
Update References
 Update pipeline configurations (Jenkins, GitHub Actions, Azure DevOps, etc.) with new repo URLs.
 Update Git URLs in tools like Jenkins, ArgoCD, FluxCD, etc.
 Update any local .git/config or workspace references (for developers or tools).
 Update any scripts or IaC templates that use hardcoded Git URLs.
Validation
 Trigger CI/CD pipelines end-to-end.
 Confirm that pipeline stages can fetch the repo successfully.
 Verify ArgoCD/git-sync-based tools are deploying as expected.
Cleanup (Optional)
 Remove references to old GitHub URLs.
 Archive or decommission old repositories if applicable.

------
âœ… Step 1: Export the Vault namespace
export VAULT_NAMESPACE="Caas-Parent/A10003579-Harness"
export VAULT_ADDR="https://your-vault-url"
export VAULT_TOKEN="your-admin-or-root-token"
âœ… Step 2: Enable the AppRole auth method (if not already)
vault auth enable approle
If it's already enabled, youâ€™ll get a warning which can be ignored.

âœ… Step 3: Enable KV secret engine (if not already)
vault secrets enable -path=kv kv-v2
Verify with:

vault secrets list
âœ… Step 4: Create a policy
Create a file harness-policy.hcl:

path "kv/data/harness/*" {
  capabilities = ["read", "create", "update", "delete", "list"]
}

path "kv/metadata/harness/*" {
  capabilities = ["list", "delete"]
}
Apply it:

vault policy write harness-policy harness-policy.hcl
âœ… Step 5: Create the AppRole and bind the policy
vault write auth/approle/role/harness-role \
    token_ttl=8760h token_max_ttl=8760h \
    secret_id_ttl=8760h \
    token_policies="harness-policy"
8760h = 1 year
Check role ID:

vault read auth/approle/role/harness-role/role-id
Get secret ID:

vault write -f auth/approle/role/harness-role/secret-id
âœ… Step 6: Test by logging in via AppRole
vault write auth/approle/login \
    role_id="<copied-role-id>" \
    secret_id="<copied-secret-id>"
This gives you a Vault token valid for 1 year.

Save this securely, as this token is what Harness can use.

âœ… Step 7: Use in Harness
In Harness, go to:

Project â†’ Connectors â†’ Secrets Manager â†’ Add New Vault

Vault URL: https://your-vault-url
Auth Method: AppRole
Role ID: <from Step 5>
Secret ID: <from Step 5>
Vault Namespace: Caas-Parent/A10003579-Harness (if using Enterprise)
KV Version: v2
Secret Path Prefix: kv/harness
Save and test the connection.

ðŸ§ª (Optional) Step 8: Store a sample secret in Vault
vault kv put kv/harness/sample-secret \
    username="admin" password="p@ssw0rd"


--------
LDAP Configuration for Keycloak in CIO Environment

Hi Casey & Rajesh,

Hope you're both doing well.

Weâ€™re approaching the testing phase of Keycloak in our new CIO environment, and I had a quick query regarding the LDAP configuration.

Currently, in our EDA Management account, we are using ldap://worldpay.local. However, since we donâ€™t have access to this LDAP from our new CIO AWS account, weâ€™ll need to connect to an alternative.

Could you please advise on the preferred Active Directory LDAP that we should connect to from the CIO environment? Below is the list I received from the LDAP teamâ€”please let me know if any of these would be suitable, or if there's a recommended option we should use for our Ethos LDAP setup.

Thank you!

Best regards,
Kiran

-------
GitHub Migration Plan: github.worldpay.com â†’ FIS GitHub Enterprise

Current Status & Activities
License Procurement
License request for FIS GitHub Enterprise has been raised with the respective teams.
Procurement is currently in progress.
Access Provisioning
Access request for FIS GitHub has been submitted.
This is currently being processed.
Migration Coordination
Engaged with Kailash and team for detailed planning.
A ServiceNow ticket has been raised with the GitHub migration team for both DTS and EDP organization migrations.
DTS Organization Migration
Current State: Hosted on github.worldpay.com.
Migration Type: Straightforward, as itâ€™s still within the same platform.
Repository Count: ~405 repositories total.
POC Plan:

Initial Scope: 20 representative repositories selected for a Proof of Concept (POC).
Timeline: Migration team committed to delivering these 20 repos in FIS GitHub by early next week.
Next Step:
Validate the POC repositories once delivered.
Provide confirmation/feedback.
Upon approval, the migration team will proceed with the remaining ~385 repositories.
Final migration timeline: To be confirmed based on POC outcome.
EDP Organization Migration
Current State: Already migrated by Worldpay to their GitHub Cloud instance a week ago.
Decision Pending: Need to confirm the source of truth for EDP migration:
Should the migration be from:
github.worldpay.com (legacy), or
Worldpay GitHub Cloud (new)?
Next Step:
Await confirmation from the application team.
Once confirmed, the migration team will finalize the migration plan and timeline accordingly.

----
echo "<+secrets.getValue(\"KEYCLOAK_ADMIN\")>" | sed -E 's/.*value: (.*)}/\1/' | base64 > /tmp/kc_admin.txt

echo "{value: keycloakadmin}" | awk -F'value: ' '{print $2}' | tr -d '}'


echo "<+secrets.getValue(\"KEYCLOAK_ADMIN\")>" | grep -oP '(?<=value: ).*' > /tmp/kc_admin.txt
cat /tmp/kc_admin.txt



---
replicaCount: 1

image:
  repository: my.jfrog.internal/keycloak/keycloakx
  tag: "21.1.2"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8080

ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    cert-manager.io/cluster-issuer: cio-ca-issuer
    nginx.ingress.kubernetes.io/rewrite-target: /
  hosts:
    - host: keycloak.test.internal
      paths:
        - path: /keycloak
          pathType: Prefix
    - host: <your-nlb-dns>.elb.amazonaws.com
      paths:
        - path: /keycloak
          pathType: Prefix
    - host: <your-alb-dns>.elb.amazonaws.com
      paths:
        - path: /keycloak
          pathType: Prefix
  tls:
    - secretName: keycloakx-tls
      hosts:
        - keycloak.test.internal
        - <your-nlb-dns>.elb.amazonaws.com
        - <your-alb-dns>.elb.amazonaws.com

extraEnv: |
  - name: KEYCLOAK_ADMIN
    value: <+secrets.getValue("keycloak-admin-username")>
  - name: KEYCLOAK_ADMIN_PASSWORD
    value: <+secrets.getValue("keycloak-admin-password")>
  - name: KC_DB
    value: postgres
  - name: KC_DB_URL_HOST
    value: <+secrets.getValue("keycloak-db-host")>
  - name: KC_DB_URL_DATABASE
    value: keycloak
  - name: KC_DB_USERNAME
    value: <+secrets.getValue("keycloak-db-username")>
  - name: KC_DB_PASSWORD
    value: <+secrets.getValue("keycloak-db-password")>
  - name: KC_PROXY
    value: edge
  - name: KC_HTTP_ENABLED
    value: "true"
  - name: KC_HOSTNAME_STRICT
    value: "false"
  - name: KC_HOSTNAME_STRICT_HTTPS
    value: "false"

resources:
  requests:
    cpu: 250m
    memory: 512Mi
  limits:
    cpu: 500m
    memory: 1Gi



-----------
replicaCount: 1

image:
  repository: httpd
  tag: 2.4-alpine
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: cio-ca-issuer
  hosts:
    - host: sample.test.internal
      paths:
        - path: /sample
          pathType: Prefix
  tls:
    - secretName: sample-app-tls
      hosts:
        - sample.test.internal




-------
curl -k https://<NLB-DNS> -H "Host: sample.test.internal"


----
controller:
  replicaCount: 2

  service:
    enabled: true
    externalTrafficPolicy: Local

    annotations:
      service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
      service.beta.kubernetes.io/aws-load-balancer-internal: "true"
      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
      service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"

  ingressClassResource:
    name: nginx
    enabled: true
    default: true

  metrics:
    enabled: true

  admissionWebhooks:
    enabled: true

defaultBackend:
  enabled: true



-----
git add ${CHART_NAME}/chart || true

echo "Checking if there are any staged changes..."
if ! git diff --cached --quiet; then
  git commit -m "Overlay Sync: ${CHART_NAME} ${CHART_VERSION} from upstream main"
  git push https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git HEAD:${OVERLAY_FEATURE_BRANCH}
else
  echo "No changes to commit. Skipping push."
fi


----
stage('Sync Chart (Preserve Overlay Customizations - Option B)') {
    sh '''
    mkdir -p overlays/${CHART_NAME}/chart

    echo "Backing up overlay-managed files (values-*.yaml, custom-values.yaml, patches/, overlays/)..."
    mkdir -p tmp-preserve
    cp -a overlays/${CHART_NAME}/chart/values-*.yaml tmp-preserve/ || true
    cp -a overlays/${CHART_NAME}/chart/custom-values.yaml tmp-preserve/ || true
    cp -a overlays/${CHART_NAME}/chart/patches tmp-preserve/ || true
    cp -a overlays/${CHART_NAME}/chart/overlays tmp-preserve/ || true

    echo "Removing old chart/ contents..."
    rm -rf overlays/${CHART_NAME}/chart/*

    echo "Copying upstream chart into overlays..."
    cp -a upstream/mirror/${CHART_NAME}/${CHART_VERSION}/${CHART_NAME}/* overlays/${CHART_NAME}/chart/

    echo "Restoring preserved overlay custom files..."
    cp -a tmp-preserve/* overlays/${CHART_NAME}/chart/ || true
    cp -a tmp-preserve/patches overlays/${CHART_NAME}/chart/ || true
    cp -a tmp-preserve/overlays overlays/${CHART_NAME}/chart/ || true
    rm -rf tmp-preserve
    '''
}



---
@Library(['common-lib@c3-stable']) _

parameters {
    string(name: 'CHART_NAME', description: 'Helm chart name (e.g. nginx, keycloak)')
    string(name: 'CHART_VERSION', description: 'Helm chart version (e.g. 1.21.0)')
    string(name: 'OVERLAY_FEATURE_BRANCH', description: 'Feature branch name to create/push in hydra-helm-overlays')
}

def label = "hydra-overlay-sync-${UUID.randomUUID().toString()}"
String podTemplateString = '''...''' // keep your pod spec as-is (aws container)

podTemplate(
    label: label,
    yaml: podTemplateString,
    serviceAccount: 'jenkins',
    runAsUser: '1000'
) {
    node(label) {
        environment {
            UPSTREAM_GIT_URL = "https://bitbucket.fis.dev/scm/~lc5736691/hydra-helm-upstreams.git"
            OVERLAYS_GIT_URL = "https://bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git"
            OVERLAYS_MAIN_BRANCH = 'develop'
        }

        stage('Clone Repositories') {
            withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                sh '''
                rm -rf upstream overlays

                echo "Cloning upstream repo (read-only)..."
                git clone --branch main https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-upstreams.git upstream

                echo "Cloning overlays repo (develop)..."
                git clone https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git overlays
                '''
            }
        }

        stage('Create or Checkout Feature Branch') {
            dir('overlays') {
                withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                    sh '''
                    git checkout ${OVERLAYS_MAIN_BRANCH}
                    git checkout -b ${OVERLAY_FEATURE_BRANCH} || git checkout ${OVERLAY_FEATURE_BRANCH}
                    '''
                }
            }
        }

        stage('Sync Chart (Preserve Overlay Customizations)') {
            sh '''
            mkdir -p overlays/${CHART_NAME}/chart

            echo "Syncing upstream chart into overlays/${CHART_NAME}/chart/..."
            rsync -av --delete \
              --exclude 'values-prod.yaml' \
              --exclude 'values-dev.yaml' \
              --exclude 'custom-values.yaml' \
              --exclude 'patches/' \
              --exclude 'overlays/' \
              upstream/mirror/${CHART_NAME}/${CHART_VERSION}/${CHART_NAME}/ \
              overlays/${CHART_NAME}/chart/
            '''
        }

        stage('Commit and Push Feature Branch') {
            dir('overlays') {
                withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                    sh '''
                    git config user.name "${BB_USER}"
                    git config user.email "${BB_USER}@yourorg.com"

                    git add ${CHART_NAME}/chart
                    git commit -m "Overlay Sync: ${CHART_NAME} ${CHART_VERSION} from upstream"
                    git push https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git HEAD:${OVERLAY_FEATURE_BRANCH}
                    '''
                }
            }
        }
    }
}


-------
@Library(['common-lib@c3-stable']) _

parameters {
    string(name: 'CHART_NAME', description: 'Helm chart name (e.g. nginx, keycloak)')
    string(name: 'CHART_VERSION', description: 'Helm chart version (e.g. 1.21.0)')
    string(name: 'OVERLAY_FEATURE_BRANCH', description: 'Feature branch name to create/push in hydra-helm-overlays')
}

def label = "hydra-helm-pod-${UUID.randomUUID().toString()}"
String podTemplateString = '''...''' // leave your existing podTemplate string here

podTemplate(
    label: label,
    yaml: podTemplateString,
    serviceAccount: 'jenkins',
    runAsUser: '1000'
) {
    node(label) {
        environment {
            UPSTREAM_GIT_URL = "https://bitbucket.fis.dev/scm/~lc5736691/hydra-helm-upstreams.git"
            OVERLAYS_GIT_URL = "https://bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git"
            OVERLAYS_MAIN_BRANCH = 'develop'
        }

        stage('Clone Repositories using HTTPS') {
            environment {
                GIT_SSL_NO_VERIFY = "true"
            }

            withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                sh '''
                rm -rf upstream overlays

                echo "Cloning upstream (read-only) repo..."
                git clone --branch main https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-upstreams.git upstream

                echo "Cloning overlays repo..."
                git clone https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git overlays
                '''
            }
        }

        stage('Create/Checkout Overlay Feature Branch') {
            dir('overlays') {
                withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                    sh '''
                    git checkout ${OVERLAYS_MAIN_BRANCH}
                    git checkout -b ${OVERLAY_FEATURE_BRANCH} || git checkout ${OVERLAY_FEATURE_BRANCH}
                    '''
                }
            }
        }

        stage('Sync Chart Folder from Upstream Repo') {
            sh '''
            rm -rf overlays/${CHART_NAME}/chart
            mkdir -p overlays/${CHART_NAME}/chart

            cp -R upstream/mirror/${CHART_NAME}/${CHART_VERSION}/${CHART_NAME}/* overlays/${CHART_NAME}/chart/
            '''
        }

        stage('Commit & Push to Overlay Feature Branch') {
            dir('overlays') {
                withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
                    sh '''
                    git config user.name "${BB_USER}"
                    git config user.email "${BB_USER}@yourorg.com"

                    git add ${CHART_NAME}/chart
                    git commit -m "Overlay Sync: ${CHART_NAME} ${CHART_VERSION} from upstream main"
                    git push https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/hydra-helm-overlays.git HEAD:${OVERLAY_FEATURE_BRANCH}
                    '''
                }
            }
        }
    }
}




-----
stage('Push Helm Chart to Bitbucket (versioned + latest)') {
  withCredentials([usernamePassword(credentialsId: 'kiran-creds', usernameVariable: 'BB_USER', passwordVariable: 'BB_TOKEN')]) {
    container('jdk11') {
      sh '''
        set -eo pipefail

        git config --global user.email "kiran.gonela@fisglobal.com"
        git config --global user.name "Kiran G"

        echo "Cloning target Bitbucket repo: ${TARGET_BITBUCKET_REPO}"
        git clone https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/${TARGET_BITBUCKET_REPO}.git target-repo
        cd target-repo

        CHART_DIR="mirror/${SRC_CHART_NAME}/${SRC_CHART_VERSION}"
        LATEST_DIR="mirror/${SRC_CHART_NAME}/latest"

        echo "Preparing chart directories..."
        mkdir -p ${CHART_DIR}
        rm -rf ${LATEST_DIR}
        mkdir -p ${LATEST_DIR}

        echo "Copying chart contents into versioned and latest directories..."
        cp -R ../${SRC_CHART_NAME}/* ${CHART_DIR}/
        cp -R ../${SRC_CHART_NAME}/* ${LATEST_DIR}/

        echo "Committing and pushing changes..."
        git add mirror/${SRC_CHART_NAME}
        git commit -m "Mirror: ${SRC_CHART_NAME} ${SRC_CHART_VERSION} from ${SRC_REPO_URL}"
        git push https://${BB_USER}:${BB_TOKEN}@bitbucket.fis.dev/scm/~lc5736691/${TARGET_BITBUCKET_REPO}.git HEAD
      '''
    }
  }
}



-------
Subject: Request to Configure Vault Kubernetes Auth Backend or Grant Temporary Admin Policy
Hi [Vault Admin / Platform Team],

I'm currently working on integrating our EKS workloads with HashiCorp Vault, specifically to enable the Vault Agent Injector to pull secrets (like Keycloak admin credentials and internal TLS certificates) using the Kubernetes authentication method.

However, I've hit a blocker while trying to configure the Kubernetes auth backend from the CLI on our bastion host using my Vault token.

ðŸ” Problem
Attempts to run the following fail with 403 Forbidden errors:

vault auth enable -path=kubernetes -max-lease-ttl=24h kubernetes

vault write auth/kubernetes/config \
  token_reviewer_jwt="<EKS_JWT_TOKEN>" \
  kubernetes_host="<EKS_API_URL>" \
  kubernetes_ca_cert="<CLUSTER_CA_CERT>" \
  issuer="https://kubernetes.default.svc"
The token Iâ€™m using is authenticated under namespace:
CaaS-Parent/A10003579-Keycloak

Even after switching to:
CaaS-Parent (where the auth/kubernetes path is enabled), the same error persists â€” confirming itâ€™s a permissions issue.

âœ… What We Need
Please help us with one of the following:

Option 1: You configure Kubernetes auth on our behalf

Run these (we will supply JWT, CA, and API server values as needed):

vault write auth/kubernetes/config \
  token_reviewer_jwt="<...>" \
  kubernetes_host="https://..." \
  kubernetes_ca_cert="-----BEGIN CERTIFICATE-----..." \
  issuer="https://kubernetes.default.svc"
And ensure the following role exists:

vault write auth/kubernetes/role/k8s-general \
  bound_service_account_names="*" \
  bound_service_account_namespaces="*" \
  policies="k8s-read-eks-apps" \
  ttl="24h"
Option 2: Grant my token temporary access to configure Kubernetes auth

Please assign my token (or identity) the following policy within CaaS-Parent namespace:

# Enable/Configure Kubernetes auth backend
path "sys/auth/kubernetes" {
  capabilities = ["create", "update"]
}
path "auth/kubernetes/config" {
  capabilities = ["create", "update"]
}

# Manage Vault roles under Kubernetes auth
path "auth/kubernetes/role/*" {
  capabilities = ["create", "update", "read", "delete", "list"]
}
Once this is done, I can finish setting up the Vault integration for the EKS workloads securely.

Please let me know if you'd prefer me to provide the JWT and CA cert values for you to configure directly. Iâ€™m happy to assist with validation after the setup.

Thanks in advance!

Best regards,

---
vault token create \
  -namespace="CaaS-Parent/A10003579-Keycloak" \
  -orphan \
  -policy="default" \
  -policy="k8s-read-eks-apps" \
  -period=24h


---
Steps to Set Up a Dedicated Namespace

kubectl create namespace vault-auth
kubectl create serviceaccount vault-auth-setup -n vault-auth
Then retrieve the values for Vault:

JWT_TOKEN=$(kubectl create token vault-auth-setup -n vault-auth)
K8S_CA_CERT=$(kubectl get configmap kube-root-ca.crt -n vault-auth -o jsonpath="{.data.ca\.crt}")
K8S_HOST=$(kubectl config view --minify -o jsonpath="{.clusters[0].cluster.server}")


---
Updated Steps (Child Namespace Scope)

Hereâ€™s what you need to do next:

1. âœ… Enable Kubernetes Auth within your namespace
vault auth enable -path=kubernetes kubernetes
This enables the Kubernetes auth backend scoped to your namespace.

2. âœ… Configure the Kubernetes Auth Backend
Now the previous vault write should work:

vault write auth/kubernetes/config \
  token_reviewer_jwt="${JWT_TOKEN}" \
  kubernetes_host="${K8S_HOST}" \
  kubernetes_ca_cert="${K8S_CA_CERT}" \
  issuer="https://kubernetes.default.svc"
âœ… This will succeed now because youâ€™re operating inside your own namespace.

3. âœ… Continue with the Role + Secret Setup
vault write auth/kubernetes/role/k8s-general \
  bound_service_account_names="*" \
  bound_service_account_namespaces="*" \
  policies="k8s-read-eks-apps" \
  ttl="24h"

vault kv put eks-apps/prod/keycloak/admin username="admin" password="supersecret123"

---
Update your Vault policy to include this:

# file: vault-admin-k8s-setup.hcl

# For setting up Kubernetes auth backend
path "auth/kubernetes/config" {
  capabilities = ["create", "update"]
}

# For creating roles that map K8s SAs to policies
path "auth/kubernetes/role/*" {
  capabilities = ["create", "update", "read", "delete", "list"]
}
Then apply:

vault policy write vault-admin-k8s-setup vault-admin-k8s-setup.hcl

----
#!/bin/bash

set -e

VAULT_ADDR="${VAULT_ADDR:-http://127.0.0.1:8200}"
VAULT_NAMESPACE="${VAULT_NAMESPACE:-root}"

ENGINE_PATH="eks-apps"
POLICY_NAME="k8s-read-eks-apps"
ROLE_NAME="k8s-general"

echo "ðŸ” [1/6] Enabling new KV v2 secrets engine at path: ${ENGINE_PATH}/"

vault secrets enable -path="${ENGINE_PATH}" -version=2 kv || echo "ðŸŸ¡ Already enabled"

echo "ðŸ“œ [2/6] Writing Vault policy: ${POLICY_NAME}"

cat <<EOF | vault policy write ${POLICY_NAME} -
path "${ENGINE_PATH}/data/*" {
  capabilities = ["read"]
}
EOF

echo "ðŸ”‘ [3/6] Extracting Kubernetes auth details from cluster..."

JWT_TOKEN=$(kubectl get secret $(kubectl get sa default -n default -o jsonpath="{.secrets[0].name}") -n default -o jsonpath="{.data.token}" | base64 -d)
K8S_HOST=$(kubectl config view --minify -o jsonpath="{.clusters[0].cluster.server}")
K8S_CA_CERT=$(kubectl get secret $(kubectl get sa default -n default -o jsonpath="{.secrets[0].name}") -n default -o jsonpath="{.data['ca\.crt']}" | base64 -d)

echo "âš™ï¸ [4/6] Configuring Vault Kubernetes auth backend"

vault write auth/kubernetes/config \
  token_reviewer_jwt="${JWT_TOKEN}" \
  kubernetes_host="${K8S_HOST}" \
  kubernetes_ca_cert="${K8S_CA_CERT}" \
  issuer="https://kubernetes.default.svc"

echo "ðŸ”— [5/6] Creating Vault role: ${ROLE_NAME} (bound to all SAs & namespaces)"

vault write auth/kubernetes/role/${ROLE_NAME} \
  bound_service_account_names="*" \
  bound_service_account_namespaces="*" \
  policies="${POLICY_NAME}" \
  ttl="24h"

echo "ðŸ” [6/6] Storing a sample secret at: ${ENGINE_PATH}/prod/keycloak/admin"

vault kv put ${ENGINE_PATH}/prod/keycloak/admin username="admin" password="supersecret123"

echo "âœ… Vault Kubernetes integration complete!"




--------
Quick Test Plan â€” Certificate Issuance

ðŸ“„ Step 1: Create a Test Certificate Resource
Hereâ€™s a simple manifest that will request a TLS cert using your cio-ca-issuer.

# test-certificate.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: test-tls
  namespace: default
spec:
  secretName: test-tls-secret
  duration: 2160h # 90 days
  renewBefore: 360h # 15 days
  commonName: test.keycloak.internal
  dnsNames:
    - test.keycloak.internal
  issuerRef:
    name: cio-ca-issuer
    kind: ClusterIssuer
Apply it:

kubectl apply -f test-certificate.yaml
ðŸ” Step 2: Validate Certificate Status
Check if cert-manager successfully issued it:

kubectl get certificate test-tls -n default
Expected output:

NAME        READY   SECRET             AGE
test-tls    True    test-tls-secret    1m
If READY is not True, inspect events:

kubectl describe certificate test-tls -n default
kubectl get events -n default --sort-by=.metadata.creationTimestamp
ðŸ” Step 3: Inspect the Generated Secret
kubectl get secret test-tls-secret -n default -o yaml
Should contain:

tls.crt â†’ Certificate
tls.key â†’ Private key
Optional: Decode and inspect the certificate:

kubectl get secret test-tls-secret -n default -o jsonpath='{.data.tls\.crt}' | base64 -d | openssl x509 -noout -text
Check:

CN = test.keycloak.internal
Validity period = 90 days
Issuer = cio.internal

-----
chart.yaml
---

apiVersion: v2
name: cert-bootstrap
description: Bootstrap internal self-signed CA and ClusterIssuer for cert-manager
type: application
version: 0.1.0
appVersion: "1.0"


vaules.yaml
-----

bootstrapCerts:
  enabled: true
  caCommonName: "cio.internal"
  duration: "8760h"         # 1 year
  renewBefore: "720h"       # 30 days
  caSecretName: "cio-internal-ca-secret"
  clusterIssuerName: "cio-ca-issuer"
  certificateName: "cio-internal-ca"
  namespace: "cert-manager"




-----

---
selfsigned-cluster-issuer.yaml
---
{{- if .Values.bootstrapCerts.enabled }}
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-cluster-issuer
spec:
  selfSigned: {}
{{- end }}


------

----
cio-ca-certificate.yaml
----

{{- if .Values.bootstrapCerts.enabled }}
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: cio-internal-ca
  namespace: cert-manager
spec:
  isCA: true
  commonName: "{{ .Values.bootstrapCerts.caCommonName }}"
  secretName: "{{ .Values.bootstrapCerts.caSecretName }}"
  duration: {{ .Values.bootstrapCerts.duration | quote }}
  renewBefore: {{ .Values.bootstrapCerts.renewBefore | quote }}
  issuerRef:
    name: selfsigned-cluster-issuer
    kind: ClusterIssuer
{{- end }}


------

----
cio-ca-issuer.yaml
----

{{- if .Values.bootstrapCerts.enabled }}
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: {{ .Values.bootstrapCerts.clusterIssuerName }}
spec:
  ca:
    secretName: "{{ .Values.bootstrapCerts.caSecretName }}"
{{- end }}

