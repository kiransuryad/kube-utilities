resource "aws_iam_policy" "dynamodb_restore_policy" {
  name        = "DynamoDBRestorePolicy"
  description = "Allows S3 read and DynamoDB BatchWriteItem for restoring table data"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid    = "S3Read"
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:ListBucket"
        ]
        Resource = [
          "arn:aws:s3:::<your-export-bucket>",
          "arn:aws:s3:::<your-export-bucket>/*"
        ]
      },
      {
        Sid    = "DynamoRestore"
        Effect = "Allow"
        Action = [
          "dynamodb:BatchWriteItem",
          "dynamodb:PutItem",
          "dynamodb:DescribeTable",
          "dynamodb:Scan"
        ]
        Resource = [
          "arn:aws:dynamodb:us-east-1:<acct>:table/ethos_uat_main_table",
          "arn:aws:dynamodb:us-east-1:<acct>:table/ethos_uat_main_table/*"
        ]
      }
    ]
  })
}



-----------------------------------
echo "=== Checking S3 Access ==="

aws s3 ls s3://<your-export-bucket> --region us-east-1
if [ $? -ne 0 ]; then
  echo "âŒ ERROR: Harness does NOT have S3 list/get permissions"
  exit 1
else
  echo "âœ” S3 list allowed"
fi


echo "=== Checking DynamoDB DescribeTable ==="

aws dynamodb describe-table \
  --table-name ethos_uat_main_table \
  --region us-east-1 > /dev/null 2>&1

if [ $? -ne 0 ]; then
  echo "âŒ ERROR: Harness does NOT have DynamoDB DescribeTable permissions"
  exit 1
else
  echo "âœ” DynamoDB DescribeTable allowed"
fi


echo "=== Checking DynamoDB BatchWriteItem Permission ==="

aws dynamodb batch-write-item \
  --region us-east-1 \
  --request-items '{}'  > /dev/null 2>&1

if [ $? -ne 0 ]; then
  echo "âŒ ERROR: Harness does NOT have BatchWriteItem permission"
  exit 1
else
  echo "âœ” DynamoDB BatchWriteItem allowed"
fi

echo
echo "ðŸŽ‰ All required permissions available. Restore is safe to proceed."


---------------------------------
SECRET_ID="your-secret-name"
REGION="us-east-1"

SECRET=$(aws secretsmanager get-secret-value \
  --secret-id "$SECRET_ID" \
  --region "$REGION" \
  --query SecretString \
  --output text)

PASSWORD=$(echo "$SECRET" | sed -n 's/.*"password":"\([^"]*\)".*/\1/p')

echo "Password = $PASSWORD"
echo "export DB_PASSWORD=$PASSWORD" >> $HARNESS_ENV_EXPORTS




------------------------
###############################################################################
# UPLOAD DIRECTORY FROM REPO â†’ S3 BUCKET
###############################################################################

locals {
  # Recursively collect all files in repo/export-data/
  dynamo_export_files = fileset("${path.module}/export-data", "**")
}

resource "aws_s3_object" "dynamo_export_upload" {
  for_each = local.dynamo_export_files

  bucket = module.ethos_artifacts_uat_s3.new_bucket["id"]
  key    = "dynamodb-import/${each.value}"

  source = "${path.module}/export-data/${each.value}"
  etag   = filemd5("${path.module}/export-data/${each.value}")
}


----


###############################################################################
# REFERENCE EXISTING DYNAMODB TABLE
###############################################################################

data "aws_dynamodb_table" "existing" {
  name = var.existing_table_name
}

###############################################################################
# IMPORT DATA INTO EXISTING DYNAMODB TABLE
###############################################################################

resource "aws_dynamodb_table_import" "dynamo_import" {
  depends_on = [
    aws_s3_object.dynamo_export_upload
  ]

  table_arn = data.aws_dynamodb_table.existing.arn

  s3_bucket_source {
    bucket     = module.ethos_artifacts_uat_s3.new_bucket["id"]
    key_prefix = "dynamodb-import/"
  }

  input_format = "DYNAMODB_JSON"
}
