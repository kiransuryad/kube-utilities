
#!/bin/bash
set -euo pipefail

########################################
# CONFIG
########################################
TABLE_NAME="ethos_uat_main_table"
BUCKET="ethos-artifacts-uat-s3"
PREFIX="dynamodb-import/01764087639925-ec20cfa4"
REGION="us-east-1"

# --- DRY RUN FLAG ---
# true  = count items only
# false = restore into DynamoDB
DRY_RUN=true

echo "=============================================="
echo " DynamoDB Restore Utility"
echo " Table     : $TABLE_NAME"
echo " S3 Source : s3://$BUCKET/$PREFIX"
echo " Region    : $REGION"
echo " Dry Run   : $DRY_RUN"
echo "=============================================="
echo

########################################
# INSTALL TOOLS
########################################
echo "Installing jq..."
yum install -y jq >/dev/null

########################################
# 1) SYNC EXPORT FILES FROM S3
########################################
echo "Syncing DynamoDB export files from S3..."
aws s3 sync "s3://$BUCKET/$PREFIX/data" ./data --region "$REGION"

echo "Files downloaded:"
ls -1 data
echo

########################################
# 2) DECOMPRESS .gz FILES
########################################
echo "Decompressing *.json.gz files..."
find data -name '*.gz' -print -exec gunzip {} \;

echo "Decompressed JSON files:"
ls -1 data
echo

########################################
# 3) DRY RUN MODE: COUNT RECORDS ONLY
########################################
if [ "$DRY_RUN" = true ]; then
  echo "=============================================="
  echo " DRY RUN MODE ENABLED â€” NO DATA WILL BE WRITTEN"
  echo " Counting records in S3 export..."
  echo "=============================================="
  echo

  TOTAL=0
  for file in data/*.json; do
    COUNT=$(wc -l < "$file")
    echo "File: $(basename "$file") â†’ $COUNT records"
    TOTAL=$((TOTAL + COUNT))
  done

  echo "----------------------------------------------"
  echo " TOTAL RECORDS IN BACKUP: $TOTAL"
  echo "----------------------------------------------"

  echo "Dry run complete."
  exit 0
fi

########################################
# 4) RESTORE MODE (ONLY IF DRY_RUN=false)
########################################

echo "=============================================="
echo " RESTORE MODE ACTIVE â€” WRITING TO DYNAMODB"
echo "=============================================="

BATCH_FILE=$(mktemp)
COUNT=0
TOTAL_WRITTEN=0

flush_batch() {
  if [ "$COUNT" -eq 0 ]; then
    return
  fi

  echo "Flushing batch of $COUNT items..."

  cat "$BATCH_FILE" \
    | jq -s --arg table "$TABLE_NAME" '{ ($table): [ .[] | { PutRequest: { Item: .Item } } ] }' \
    > batch.json

  # Retry unprocessed items
  while true; do
    RESP=$(aws dynamodb batch-write-item \
      --region "$REGION" \
      --request-items file://batch.json)

    UNPROCESSED=$(echo "$RESP" | jq '.UnprocessedItems | length')

    if [ "$UNPROCESSED" -eq 0 ]; then
      break
    fi

    echo "UnprocessedItems: $UNPROCESSED â€“ retrying..."
    echo "$RESP" | jq '.UnprocessedItems' > batch.json
    sleep 1
  done

  TOTAL_WRITTEN=$((TOTAL_WRITTEN + COUNT))
  COUNT=0
  : > "$BATCH_FILE"
}

: > "$BATCH_FILE"

echo "Starting full restore into: $TABLE_NAME"

for file in data/*.json; do
  echo "Processing file: $file"

  while IFS= read -r line; do
    echo "$line" >> "$BATCH_FILE"
    COUNT=$((COUNT + 1))

    if [ "$COUNT" -eq 25 ]; then
      flush_batch
    fi
  done < "$file"
done

flush_batch

echo
echo "=============================================="
echo " RESTORE COMPLETE â€” TOTAL ITEMS WRITTEN: $TOTAL_WRITTEN"
echo "=============================================="

# Verify table count
SCAN_COUNT=$(aws dynamodb scan \
  --table-name "$TABLE_NAME" \
  --select COUNT \
  --region "$REGION" \
  | jq '.Count')

echo "DynamoDB current item count: $SCAN_COUNT"
echo "Restore step finished successfully."




---------------------------------------------------------------------------
TABLE_NAME="ethos_uat_09876_main_table"
S3_BUCKET="s3://my-export-bucket/exports/09876/"
IAM_ROLE_ARN="arn:aws:iam::<acct>:role/dynamodb-import-role"
REGION="us-east-1"

aws dynamodb import-table \
  --table-name "$TABLE_NAME" \
  --s3-bucket-source S3Bucket=$S3_BUCKET \
  --input-format DYNAMODB_JSON \
  --input-compression-type GZIP \
  --table-creation-parameters '{
      "AttributeDefinitions": [
          {"AttributeName": "pk", "AttributeType": "S"},
          {"AttributeName": "sk", "AttributeType": "S"}
      ],
      "KeySchema": [
          {"AttributeName": "pk", "KeyType": "HASH"},
          {"AttributeName": "sk", "KeyType": "RANGE"}
      ],
      "BillingMode": "PAY_PER_REQUEST"
  }' \
  --region "$REGION"



----------------------------
resource "aws_iam_policy" "dynamodb_restore_policy" {
  name        = "DynamoDBRestorePolicy"
  description = "Allows S3 read and DynamoDB BatchWriteItem for restoring table data"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid    = "S3Read"
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:ListBucket"
        ]
        Resource = [
          "arn:aws:s3:::<your-export-bucket>",
          "arn:aws:s3:::<your-export-bucket>/*"
        ]
      },
      {
        Sid    = "DynamoRestore"
        Effect = "Allow"
        Action = [
          "dynamodb:BatchWriteItem",
          "dynamodb:PutItem",
          "dynamodb:DescribeTable",
          "dynamodb:Scan"
        ]
        Resource = [
          "arn:aws:dynamodb:us-east-1:<acct>:table/ethos_uat_main_table",
          "arn:aws:dynamodb:us-east-1:<acct>:table/ethos_uat_main_table/*"
        ]
      }
    ]
  })
}



-----------------------------------
echo "=== Checking S3 Access ==="

aws s3 ls s3://<your-export-bucket> --region us-east-1
if [ $? -ne 0 ]; then
  echo "âŒ ERROR: Harness does NOT have S3 list/get permissions"
  exit 1
else
  echo "âœ” S3 list allowed"
fi


echo "=== Checking DynamoDB DescribeTable ==="

aws dynamodb describe-table \
  --table-name ethos_uat_main_table \
  --region us-east-1 > /dev/null 2>&1

if [ $? -ne 0 ]; then
  echo "âŒ ERROR: Harness does NOT have DynamoDB DescribeTable permissions"
  exit 1
else
  echo "âœ” DynamoDB DescribeTable allowed"
fi


echo "=== Checking DynamoDB BatchWriteItem Permission ==="

aws dynamodb batch-write-item \
  --region us-east-1 \
  --request-items '{}'  > /dev/null 2>&1

if [ $? -ne 0 ]; then
  echo "âŒ ERROR: Harness does NOT have BatchWriteItem permission"
  exit 1
else
  echo "âœ” DynamoDB BatchWriteItem allowed"
fi

echo
echo "ðŸŽ‰ All required permissions available. Restore is safe to proceed."


---------------------------------
SECRET_ID="your-secret-name"
REGION="us-east-1"

SECRET=$(aws secretsmanager get-secret-value \
  --secret-id "$SECRET_ID" \
  --region "$REGION" \
  --query SecretString \
  --output text)

PASSWORD=$(echo "$SECRET" | sed -n 's/.*"password":"\([^"]*\)".*/\1/p')

echo "Password = $PASSWORD"
echo "export DB_PASSWORD=$PASSWORD" >> $HARNESS_ENV_EXPORTS




------------------------
###############################################################################
# UPLOAD DIRECTORY FROM REPO â†’ S3 BUCKET
###############################################################################

locals {
  # Recursively collect all files in repo/export-data/
  dynamo_export_files = fileset("${path.module}/export-data", "**")
}

resource "aws_s3_object" "dynamo_export_upload" {
  for_each = local.dynamo_export_files

  bucket = module.ethos_artifacts_uat_s3.new_bucket["id"]
  key    = "dynamodb-import/${each.value}"

  source = "${path.module}/export-data/${each.value}"
  etag   = filemd5("${path.module}/export-data/${each.value}")
}


----


###############################################################################
# REFERENCE EXISTING DYNAMODB TABLE
###############################################################################

data "aws_dynamodb_table" "existing" {
  name = var.existing_table_name
}

###############################################################################
# IMPORT DATA INTO EXISTING DYNAMODB TABLE
###############################################################################

resource "aws_dynamodb_table_import" "dynamo_import" {
  depends_on = [
    aws_s3_object.dynamo_export_upload
  ]

  table_arn = data.aws_dynamodb_table.existing.arn

  s3_bucket_source {
    bucket     = module.ethos_artifacts_uat_s3.new_bucket["id"]
    key_prefix = "dynamodb-import/"
  }

  input_format = "DYNAMODB_JSON"
}
