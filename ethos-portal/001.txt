#!/bin/bash

# Harness-ready S3 Content-Type fixer + CloudFront invalidation
# - Scans all objects in a bucket (optionally under PREFIX)
# - Guesses correct Content-Type by file extension
# - Rewrites metadata via copy-object with URL-encoded source keys
# - Retries on failure, skips but logs errors
# - Produces summary + text files listing fixed / unknown / failed keys
# - Triggers CloudFront invalidation at the end (if DISTRIBUTION_ID set)

set -uo pipefail   # don't use -e because we want to skip-on-error

# ----------------------------- CONFIG -----------------------------

: "${BUCKET:?Environment variable BUCKET is required}"
# CloudFront distribution (optional ‚Äì if empty, invalidation is skipped)
: "${DISTRIBUTION_ID:=}"
# Optional prefix within the bucket (e.g. _next/, static/, etc.)
: "${PREFIX:=}"

WORKDIR="${WORKDIR:-.}"
FIXED_LIST="${WORKDIR}/fixed_objects.txt"
UNKNOWN_LIST="${WORKDIR}/unknown_objects.txt"
FAILED_LIST="${WORKDIR}/failed_objects.txt"

: > "$FIXED_LIST"
: > "$UNKNOWN_LIST"
: > "$FAILED_LIST"

echo "============================================================"
echo "üîé Using S3 Bucket: $BUCKET"
[ -n "$PREFIX" ] && echo "üîé Prefix:        $PREFIX"
[ -n "$DISTRIBUTION_ID" ] && echo "üîé CloudFront:    $DISTRIBUTION_ID"
echo "============================================================"

# ------------------------ Helper functions -----------------------

urlencode() {
  # URL-encode a string for use in --copy-source
  python3 - "$1" << 'EOF'
import sys, urllib.parse
print(urllib.parse.quote(sys.argv[1], safe=''))
EOF
}

guess_mime_type() {
  local key="$1"
  local ext

  ext="${key##*.}"
  ext="$(echo "$ext" | tr '[:upper:]' '[:lower:]')"

  case "$ext" in
    html)  echo "text/html" ;;
    htm)   echo "text/html" ;;
    js)    echo "application/javascript" ;;
    mjs)   echo "application/javascript" ;;
    css)   echo "text/css" ;;
    json)  echo "application/json" ;;

    png)   echo "image/png" ;;
    jpg)   echo "image/jpeg" ;;
    jpeg)  echo "image/jpeg" ;;
    gif)   echo "image/gif" ;;
    webp)  echo "image/webp" ;;
    svg)   echo "image/svg+xml" ;;
    ico)   echo "image/x-icon" ;;

    eot)   echo "application/vnd.ms-fontobject" ;;
    ttf)   echo "font/ttf" ;;
    woff)  echo "font/woff" ;;
    woff2) echo "font/woff2" ;;

    map)   echo "application/json" ;;
    txt)   echo "text/plain" ;;
    *)     echo "" ;;
  esac
}

copy_with_retry() {
  local key="$1"
  local mime="$2"
  local encoded_key
  local attempts=0
  local max_attempts=3
  local rc=1
  local out

  encoded_key="$(urlencode "$key")"

  while [ "$attempts" -lt "$max_attempts" ]; do
    attempts=$((attempts + 1))
    out=$(aws s3api copy-object \
      --bucket "$BUCKET" \
      --copy-source "${BUCKET}/${encoded_key}" \
      --key "$key" \
      --metadata-directive REPLACE \
      --content-type "$mime" 2>&1)
    rc=$?

    if [ "$rc" -eq 0 ]; then
      return 0
    fi

    echo "‚ö†Ô∏è  Attempt ${attempts}/${max_attempts} failed for '${key}': $out" >&2
    sleep 2
  done

  echo "‚ùå Giving up on '${key}' after ${max_attempts} attempts." >&2
  return "$rc"
}

# --------------------- List all objects (paged) -------------------

TOTAL=0
FIXED=0
UNKNOWN=0
FAILED=0
UNCHANGED=0

echo "üìú Listing all objects in bucket..."
NEXT_TOKEN=""

while : ; do
  if [ -z "$NEXT_TOKEN" ]; then
    PAGE=$(aws s3api list-objects-v2 --bucket "$BUCKET" --prefix "$PREFIX")
  else
    PAGE=$(aws s3api list-objects-v2 --bucket "$BUCKET" --prefix "$PREFIX" --continuation-token "$NEXT_TOKEN")
  fi

  # No objects
  if [ "$(echo "$PAGE" | jq '.KeyCount')" -eq 0 ]; then
    break
  fi

  # Process keys in this page
  echo "$PAGE" | jq -r '.Contents[].Key' | while read -r KEY; do
    # Skip "folders"
    if [[ "$KEY" == */ && "$KEY" != *.* ]]; then
      continue
    fi

    TOTAL=$((TOTAL + 1))

    MIME=$(guess_mime_type "$KEY")
    if [ -z "$MIME" ]; then
      echo "‚ö†Ô∏è  UNKNOWN FILE TYPE: $KEY" | tee -a "$UNKNOWN_LIST"
      UNKNOWN=$((UNKNOWN + 1))
      continue
    fi

    echo "üõ†  Fixing: $KEY  ‚Üí  $MIME"

    # Optional optimisation: check current Content-Type and skip if already correct
    CURRENT_CT=$(aws s3api head-object --bucket "$BUCKET" --key "$KEY" \
                  | jq -r '.ContentType // empty')

    if [ "$CURRENT_CT" = "$MIME" ]; then
      echo "   ‚Ü≥ Skipping (already $MIME)"
      UNCHANGED=$((UNCHANGED + 1))
      echo "$KEY" >> "$FIXED_LIST"   # still log as processed
      continue
    fi

    if copy_with_retry "$KEY" "$MIME"; then
      FIXED=$((FIXED + 1))
      echo "$KEY" >> "$FIXED_LIST"
    else
      FAILED=$((FAILED + 1))
      echo "$KEY" >> "$FAILED_LIST"
    fi
  done

  NEXT_TOKEN=$(echo "$PAGE" | jq -r '.NextContinuationToken // empty')
  [ -z "$NEXT_TOKEN" ] && break
done

echo "============================================================"
echo "‚úÖ Finished S3 pass."
echo "  Total objects seen:     $TOTAL"
echo "  Fixed / updated:        $FIXED"
echo "  Already correct:        $UNCHANGED"
echo "  Unknown extensions:     $UNKNOWN"
echo "  Failed to update:       $FAILED"
echo
echo "  Fixed objects list:     $FIXED_LIST"
echo "  Unknown type list:      $UNKNOWN_LIST"
echo "  Failed objects list:    $FAILED_LIST"
echo "============================================================"

# --------------------- CloudFront invalidation --------------------

if [ -n "$DISTRIBUTION_ID" ]; then
  echo "üöÄ Creating CloudFront invalidation for /* ..."
  CF_OUT=$(aws cloudfront create-invalidation \
              --distribution-id "$DISTRIBUTION_ID" \
              --paths "/*" 2>&1)
  if [ "$?" -eq 0 ]; then
    INVALIDATION_ID=$(echo "$CF_OUT" | jq -r '.Invalidation.Id // empty')
    echo "‚úÖ CloudFront invalidation submitted. ID: $INVALIDATION_ID"
  else
    echo "‚ùå Failed to create CloudFront invalidation: $CF_OUT" >&2
  fi
else
  echo "‚ÑπÔ∏è  DISTRIBUTION_ID not set. Skipping CloudFront invalidation."
fi

echo "üéØ DONE."





-------------------------------
# ----- KMS Permissions for S3 SSE-KMS encrypted objects -----
statement {
  sid = "KMSS3CopyPermissions"

  actions = [
    "kms:Decrypt",
    "kms:Encrypt",
    "kms:ReEncryptFrom",
    "kms:ReEncryptTo",
    "kms:GenerateDataKey",
    "kms:DescribeKey"
  ]

  resources = [
    "arn:aws:kms:us-east-1:533267095208:key/584f9bc0-d83e-4c7b-9e63-3246c35c10f"
  ]
}


{
  "Sid": "AllowEKSOrHarnessRoleToUseKey",
  "Effect": "Allow",
  "Principal": {
    "AWS": "arn:aws:iam::533267095208:role/ethos-workers-eks-node-group-202407151047621190100000011"
  },
  "Action": [
    "kms:Decrypt",
    "kms:Encrypt",
    "kms:ReEncryptFrom",
    "kms:ReEncryptTo",
    "kms:GenerateDataKey",
    "kms:DescribeKey"
  ],
  "Resource": "*"
}




----------------------------------
#!/bin/bash

set -euo pipefail

# ================================================================
# CONFIGURATION (Harness will inject these)
# ================================================================
BUCKET="${BUCKET:-ethos-portal-originbucket-533267095208-cdn-uat}"
DISTRIBUTION_ID="${DISTRIBUTION_ID:-YOUR_CF_DIST_ID}"

echo "üîç Using S3 Bucket: $BUCKET"
echo "üîç CloudFront Distribution: $DISTRIBUTION_ID"
echo "============================================================"


# ================================================================
# Content-Type mapping function
# ================================================================
get_content_type() {
    case "$1" in
        html)  echo "text/html" ;;
        js)    echo "application/javascript" ;;
        css)   echo "text/css" ;;
        json)  echo "application/json" ;;
        png)   echo "image/png" ;;
        jpg|jpeg) echo "image/jpeg" ;;
        svg)   echo "image/svg+xml" ;;
        *)     echo "" ;;   # unknown filetype
    esac
}


# ================================================================
# Temporary files
# ================================================================
FIXED_LIST="fixed_files.log"
UNKNOWN_LIST="unknown_filetypes.log"

rm -f "$FIXED_LIST" "$UNKNOWN_LIST"
touch "$FIXED_LIST" "$UNKNOWN_LIST"


# ================================================================
# Fetch all S3 object keys
# ================================================================
echo "üì¶ Listing all objects in bucket..."
OBJECTS=$(aws s3api list-objects-v2 --bucket "$BUCKET" --query "Contents[].Key" --output text)

echo "Found $(echo "$OBJECTS" | wc -w) objects."
echo "------------------------------------------------------------"


# ================================================================
# Process each object
# ================================================================
for KEY in $OBJECTS; do
    EXT="${KEY##*.}"

    # Determine proper content type
    NEW_CT=$(get_content_type "$EXT")

    if [[ -z "$NEW_CT" ]]; then
        echo "‚ö†Ô∏è  UNKNOWN FILE TYPE: $KEY (.$EXT)" | tee -a "$UNKNOWN_LIST"
        continue
    fi

    echo "üîß Fixing: $KEY ‚Üí $NEW_CT"

    aws s3api copy-object \
        --bucket "$BUCKET" \
        --copy-source "$BUCKET/$KEY" \
        --key "$KEY" \
        --metadata-directive REPLACE \
        --content-type "$NEW_CT" >/dev/null

    echo "$KEY ‚Üí $NEW_CT" >> "$FIXED_LIST"
done

echo "------------------------------------------------------------"
echo "‚úî Done fixing known filetypes."
echo "üìÑ Fixed files saved in: $(pwd)/$FIXED_LIST"
echo "üìÑ Unknown filetypes saved in: $(pwd)/$UNKNOWN_LIST"
echo "------------------------------------------------------------"


# ================================================================
# CloudFront invalidation
# ================================================================
echo "üöÄ Creating CloudFront invalidation..."

aws cloudfront create-invalidation \
    --distribution-id "$DISTRIBUTION_ID" \
    --paths "/*"

echo "‚úî CloudFront invalidation submitted."
echo "============================================================"
echo "üéâ FINISHED ‚Äî All known filetypes corrected and cache invalidated."



-------------------------
variable "cdn_bucket_name" {
  type        = string
  default     = "ethos-portal-originbucket-533267095208-cdn-uat"
}

variable "cloudfront_distribution_id" {
  type        = string
  description = "CloudFront distribution to invalidate"
}


data "aws_iam_policy_document" "s3_cf_frontend" {
  # ---------------- S3 FULL OBJECT-LEVEL ACCESS ----------------
  statement {
    sid = "S3ObjectAccess"

    actions = [
      "s3:GetObject",
      "s3:GetObjectVersion",
      "s3:PutObject",
      "s3:DeleteObject",
      "s3:GetObjectTagging",
      "s3:PutObjectTagging",
      "s3:PutObjectAcl"
    ]

    resources = [
      "arn:aws:s3:::${var.cdn_bucket_name}/*"
    ]
  }

  # ---------------- S3 LIST BUCKET + READ METADATA -------------
  statement {
    sid = "S3ListBucket"

    actions = [
      "s3:ListBucket",
      "s3:GetBucketLocation"
    ]

    resources = [
      "arn:aws:s3:::${var.cdn_bucket_name}"
    ]
  }

  # ---------------- CloudFront INVALIDATION --------------------
  # CloudFront uses wildcard ARNs; AWS requires "*" here.
  statement {
    sid = "CloudFrontInvalidation"

    actions = [
      "cloudfront:CreateInvalidation",
      "cloudfront:GetInvalidation",
      "cloudfront:ListInvalidations",
      "cloudfront:GetDistribution",
      "cloudfront:GetDistributionConfig"
    ]

    resources = ["*"]
  }
}


resource "aws_iam_policy" "s3_cf_frontend" {
  name        = "s3-cloudfront-frontend-automation"
  description = "Allows metadata fixes on S3 CDN bucket + CloudFront invalidations"

  policy = data.aws_iam_policy_document.s3_cf_frontend.json
}








------------------------
#!/bin/bash

set -euo pipefail

# -----------------------------------------------------------------------------
# INPUTS (Harness variables or hard-coded)
# -----------------------------------------------------------------------------
BUCKET="${BUCKET:-your-bucket-name}"
KEY="${KEY:-path/index.html}"

echo "Checking metadata for S3 object:"
echo "  Bucket: $BUCKET"
echo "  Key:    $KEY"
echo "------------------------------------------------------"

# -----------------------------------------------------------------------------
# EXECUTE HEAD-OBJECT
# -----------------------------------------------------------------------------

HEAD_OUTPUT=$(aws s3api head-object \
  --bucket "$BUCKET" \
  --key "$KEY" 2>&1) || {
    echo "‚ùå ERROR: Unable to fetch object metadata."
    echo "$HEAD_OUTPUT"
    exit 1
}

echo "‚úÖ S3 Object Metadata:"
echo "$HEAD_OUTPUT" | jq .








-------------------------------------------
variable "invalidate" {
  type        = bool
  default     = false
  description = "Manually trigger a CloudFront invalidation"
}

locals {
  frontend_hash        = filesha256("${path.module}/frontend/build/index.html")
  lambda_redirect_hash = filesha256("${path.module}/lambda/redirect/index.js")
  lambda_response_hash = filesha256("${path.module}/lambda/response/index.js")

  # Change this local value if any of the hashes change OR invalidate=true
  invalidate_trigger = var.invalidate ? timestamp() : (
    "${local.frontend_hash}-${local.lambda_redirect_hash}-${local.lambda_response_hash}"
  )
}



# Your existing CF distribution
resource "aws_cloudfront_distribution" "portal" {
  # ... your existing config ...
}

# CloudFront invalidation via AWS CLI
resource "null_resource" "cloudfront_invalidation" {
  # Any change here will destroy + recreate this resource,
  # and that will re-run the provisioner below.
  triggers = {
    invalidate_trigger = local.invalidate_trigger
    distribution_id    = aws_cloudfront_distribution.portal.id
    paths              = "/*"                         # adjust if you want narrower invalidations
  }

  provisioner "local-exec" {
    # Requires AWS CLI configured where Terraform runs (env vars, profile, etc.)
    command = "aws cloudfront create-invalidation --distribution-id ${self.triggers.distribution_id} --paths \"${self.triggers.paths}\""
  }

  depends_on = [
    aws_cloudfront_distribution.portal
  ]
}





---------------------------------------
variable "api_v2_deploy_trigger" {
  type    = string
  default = "1"
}

locals {
  apigw_v2_access_log_format = jsonencode({
    requestId          = "$context.requestId"
    extendedRequestId  = "$context.extendedRequestId"
    requestTime        = "$context.requestTime"
    requestTimeEpoch   = "$context.requestTimeEpoch"

    httpMethod         = "$context.httpMethod"
    resourcePath       = "$context.resourcePath"
    path               = "$context.path"
    protocol           = "$context.protocol"

    sourceIp           = "$context.identity.sourceIp"
    userAgent          = "$context.identity.userAgent"

    status             = "$context.status"
    responseLength     = "$context.responseLength"
    responseLatency    = "$context.responseLatency"

    integrationStatus  = "$context.integrationStatus"
    integrationLatency = "$context.integrationLatency"
    integrationError   = "$context.integrationErrorMessage"

    errorMessage       = "$context.error.message"
    errorType          = "$context.error.responseType"
  })
}



resource "aws_cloudwatch_log_group" "api_gw_access_v2" {
  name              = "/aws/apigateway/${local.env_short_lower}-api-v2"
  retention_in_days = 30
}


resource "aws_iam_role" "apigw_cloudwatch_role_v2" {
  name = "${local.env_short_lower}-apigw-cloudwatch-role-v2"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = { Service = "apigateway.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy" "apigw_cloudwatch_policy_v2" {
  name = "${local.env_short_lower}-apigw-cloudwatch-policy-v2"
  role = aws_iam_role.apigw_cloudwatch_role_v2.id

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Action = [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:DescribeLogGroups",
        "logs:DescribeLogStreams",
        "logs:PutLogEvents",
        "logs:GetLogEvents",
        "logs:FilterLogEvents",
        "logs:PutRetentionPolicy"
      ],
      Resource = "*"
    }]
  })
}

resource "aws_api_gateway_account" "apigw_logging_v2" {
  cloudwatch_role_arn = aws_iam_role.apigw_cloudwatch_role_v2.arn
}

resource "aws_api_gateway_deployment" "v2_deployment" {
  rest_api_id = module.ethos-portal-uat-apigateway.rest_api_id

  description = "Deployment for api-v2 stage"

  triggers = {
    redeploy = var.api_v2_deploy_trigger
  }

  lifecycle {
    create_before_destroy = true
  }
}


resource "aws_api_gateway_stage" "api_v2" {
  stage_name    = "api-v2"
  rest_api_id   = module.ethos-portal-uat-apigateway.rest_api_id
  deployment_id = aws_api_gateway_deployment.v2_deployment.id

  xray_tracing_enabled = true

  access_log_settings {
    destination_arn = aws_cloudwatch_log_group.api_gw_access_v2.arn
    format          = local.apigw_v2_access_log_format
  }

  method_settings {
    resource_path       = "/*"
    http_method         = "*"
    logging_level       = "INFO"
    metrics_enabled     = true
    data_trace_enabled  = true
  }

  tags = {
    Environment = local.env_short_lower
    Stage       = "api-v2"
    ManagedBy   = "Terraform"
  }
}



--------------------
curl -v http://localhost:8080/
curl -v http://localhost:8080/health
curl -v http://localhost:8080/ready
curl -v http://localhost:8080/status
curl -v http://localhost:8080/api/health
curl -v http://localhost:8080/ping
curl -v http://localhost:8080/actuator/health     # if Spring
curl -v http://localhost:8080/q/health            # if Quarkus
curl -v http://localhost:8080/metrics



------------------
spec:
  serviceAccountName: {{ include "ethos-portal.serviceAccountName" . }}
  securityContext:
    {{- toYaml .Values.podSecurityContext | nindent 8 }}

  {{- with .Values.imagePullSecrets }}
  imagePullSecrets:
    {{- toYaml . | nindent 4 }}
  {{- end }}

  containers:
    - name: {{ .Chart.Name }}
      image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
      imagePullPolicy: {{ .Values.image.pullPolicy }}



------------
# ===========================
# GLOBAL IDENTITY
# ===========================
nameOverride: ""
fullnameOverride: ""

replicaCount: 2

image:
  repository: "<ARTIFACTORY_URL>/ethos-portal"
  tag: ""                     # defaults to .Chart.AppVersion
  pullPolicy: IfNotPresent

imagePullSecrets: []          # e.g. [{ name: "ecr-creds" }]

# ===========================
# SERVICE ACCOUNT / IRSA
# ===========================
serviceAccount:
  create: true
  name: ""
  annotations:
    {}

# ===========================
# POD SECURITY
# ===========================
podAnnotations: {}

podSecurityContext:
  fsGroup: 2000

securityContext:
  runAsUser: 1000
  runAsGroup: 1000
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true

# ===========================
# SERVICE
# ===========================
service:
  type: ClusterIP
  port: 8080

# ===========================
# INGRESS (DISABLED BY DEFAULT)
# Edge will use Akamai ‚Üí CloudFront ‚Üí API Gateway ‚Üí NLB ‚Üí ingress-nginx
# ===========================
ingress:
  enabled: false
  className: "nginx"
  annotations: {}
  hosts: []
  tls: []

# ===========================
# ENVIRONMENT VARIABLES
# ===========================
env:
  appEnv: "prod"
  tier: "backend"
  keycloakRealm: "ethos-portal"
  backendBaseUrl: ""
  corsAllowedOrigins: ""
  extra: []       # [{name: X, value: Y}]

# ===========================
# SECRETS & CONFIGMAPS
# ===========================
secrets:
  envFromSecrets: []     # ["portal-db", "portal-dynamodb"]

config:
  envFromConfigMaps: []  # ["portal-config"]

# ===========================
# PROBES
# ===========================
livenessProbe:
  httpGet:
    path: /actuator/health/liveness
    port: http
  initialDelaySeconds: 30
  timeoutSeconds: 5
  periodSeconds: 15
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /actuator/health/readiness
    port: http
  initialDelaySeconds: 15
  timeoutSeconds: 3
  periodSeconds: 10
  failureThreshold: 3

# ===========================
# RESOURCES
# ===========================
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 200m
    memory: 256Mi

# ===========================
# AUTOSCALING
# ===========================
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 6
  targetCPUUtilizationPercentage: 60
  targetMemoryUtilizationPercentage: 70

nodeSelector: {}
tolerations: []
affinity: {}



# ===========================
# TEST MODE ONLY
# Expose via ingress-nginx directly
# ===========================

ingress:
  enabled: true
  className: "nginx"

  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "60"

  hosts:
    - host: "ethos-portal.test.internal"
      paths:
        - path: /
          pathType: Prefix

  tls: []   # No TLS needed for now

env:
  backendBaseUrl: "http://ethos-portal.test.internal"
  corsAllowedOrigins: "*"




------------------------------------------------------------
@Library(['common-lib3-stable']) _

properties([
  parameters([
    string(name: 'IMAGE_NAME', defaultValue: 'ethos-portal-unauth', description: 'Image name'),
    string(name: 'IMAGE_TAG', defaultValue: '1.61.0-3-unauth-uat', description: 'Tag to migrate')
  ])
])

// POD TEMPLATE (your version)
String podTemplateString = """
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: aws
    image: caas-docker-release-local.docker.fis.dev/jenkins/ubi8-minimal-hel-aws:3.6.3-1
    tty: true
    command: ['cat']

  - name: buildah
    image: caas-docker-release-local.docker.fis.dev/mirror/buildah/stable:v1.23.1
    tty: true
    command: ['cat']

  imagePullSecrets:
  - name: caas-docker-release-local-ro

  serviceAccountName: jenkins
"""

def label = "ecr-artifactory-${UUID.randomUUID().toString()}"

podTemplate(label: label, yaml: podTemplateString, runAsUser: '1000') {

node(label) {

    env.OLD_AWS_ACCOUNT  = "588633075404"
    env.OLD_AWS_REGION   = "us-east-1"
    env.ARTIFACTORY_URL  = "artifactory.fis.dev"
    env.ARTIFACTORY_REPO = "dsdata-docker-snapshot-local"

    stage("Checkout Tools Repo") {
        checkout([
            $class: 'GitSCM',
            branches: [[name: "*/develop"]],
            userRemoteConfigs: [[
                url: "https://bitbucket.fis.dev/scm/dsdata/ethos-tools-devops.git",
                credentialsId: "ethos-portal-svc-acct"
            ]]
        ])
    }

    // -----------------------------------------
    // LOAD JFROG API KEY (Vault secret)
    // -----------------------------------------
    stage("Load JFrog API Key") {
        withCredentials([ string(credentialsId: 'svcacct-dsdata-caas-snapshot', variable: 'JFROG_API_KEY') ]) {
            env.JFROG_USER = "svcacct-dsdata-caas-snapshot"
        }
    }

    // -----------------------------------------
    // AWS STS LOGIN (aws container)
    // -----------------------------------------
    stage("ECR ‚Üí Export OCI from AWS") {
        container('aws') {

            withCredentials([
                string(credentialsId: 'aws_access_key_id',     variable: 'AWS_ACCESS_KEY_ID'),
                string(credentialsId: 'aws_secret_access_key', variable: 'AWS_SECRET_ACCESS_KEY'),
                string(credentialsId: 'aws_session_token',     variable: 'AWS_SESSION_TOKEN')
            ]) {

                sh """
                export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
                export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
                export AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN

                echo "Downloading OCI manifest and layers from ECR..."

                aws ecr batch-get-image \
                    --repository-name ${params.IMAGE_NAME} \
                    --image-ids imageTag=${params.IMAGE_TAG} \
                    --region ${env.OLD_AWS_REGION} \
                    --query 'images[].imageManifest' \
                    --output text > image_manifest.json

                mkdir -p oci/blobs oci/refs

                echo "\$(sha256sum image_manifest.json | cut -d ' ' -f1)" > oci/refs/latest

                # Create OCI layout
                cat > oci/layout.json << EOF
                {"imageLayoutVersion": "1.0.0"}
                EOF

                echo "Saved OCI manifest to workspace."
                """
            }
        }
    }

    // -----------------------------------------
    // IMPORT OCI INTO BUILDAH & PUSH
    // -----------------------------------------
    stage("Import OCI ‚Üí Push to Artifactory") {
        container('buildah') {

            withCredentials([ string(credentialsId: 'svcacct-dsdata-caas-snapshot', variable: 'JFROG_API_KEY') ]) {

                sh """
                echo "Logging into Artifactory..."
                buildah login -u ${env.JFROG_USER} -p ${JFROG_API_KEY} ${ARTIFACTORY_URL}

                echo "Importing OCI into buildah..."
                buildah pull oci:oci

                echo "Tagging for Artifactory..."
                buildah tag oci:latest ${ARTIFACTORY_URL}/${ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}

                echo "Pushing to Artifactory..."
                buildah push ${ARTIFACTORY_URL}/${ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}

                echo "SUCCESS: Image migrated!"
                """
            }
        }
    }

}}






----------------------------------------------
stage("Install AWS CLI in Buildah") {
  container('buildah') {
    sh """
      curl 'https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip' -o 'awscliv2.zip'
      unzip awscliv2.zip
      ./aws/install
      aws --version
    """
  }
}



stage("Migrate Image using Buildah") {
  container('buildah') {

    withCredentials([
      string(credentialsId: 'aws_access_key_id', variable: 'AWS_ACCESS_KEY_ID'),
      string(credentialsId: 'aws_secret_access_key', variable: 'AWS_SECRET_ACCESS_KEY'),
      string(credentialsId: 'aws_session_token',  variable: 'AWS_SESSION_TOKEN')
    ]) {

      withCredentials([string(credentialsId: 'svcacct-dsdata-caas-snapshot', variable: 'JFROG_API_KEY')]) {

        sh """
          # AWS env vars
          export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
          export AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN

          echo "Logging into ECR..."
          export ECR_PASSWORD=\$(aws ecr get-login-password --region ${env.OLD_AWS_REGION})

          buildah login -u AWS -p \$ECR_PASSWORD ${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com

          echo "Pulling from ECR..."
          buildah pull docker://${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com/${params.IMAGE_NAME}:${params.IMAGE_TAG}

          echo "Tagging for Artifactory..."
          buildah tag \
            docker://${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com/${params.IMAGE_NAME}:${params.IMAGE_TAG} \
            ${env.ARTIFACTORY_URL}/${env.ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}

          echo "Logging into Artifactory..."
          buildah login -u ${env.JFROG_USER} -p ${JFROG_API_KEY} ${env.ARTIFACTORY_URL}

          echo "Pushing to Artifactory..."
          buildah push ${env.ARTIFACTORY_URL}/${env.ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}
        """
      }
    }
  }
}



-=-=-=-=-=-=-=-=----=-=-=-=-=-=-
stage("Pull Image from ECR (AWS Container)") {
    container('aws') {

        withCredentials([
            string(credentialsId: 'aws_old_access_key', variable: 'AWS_ACCESS_KEY_ID'),
            string(credentialsId: 'aws_old_secret_key', variable: 'AWS_SECRET_ACCESS_KEY'),
            string(credentialsId: 'aws_old_session_token', variable: 'AWS_SESSION_TOKEN')
        ]) {

            sh """
                echo 'üîê Setting AWS credentials...'
                export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
                export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
                export AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}

                echo 'üîê Logging into AWS ECR...'
                aws sts get-caller-identity
                export ECR_PASSWORD=\$(aws ecr get-login-password --region ${env.OLD_AWS_REGION})

                docker login -u AWS -p \$ECR_PASSWORD \
                  ${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com

                echo '‚¨áÔ∏è Pulling image from ECR...'
                docker pull ${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com/${params.IMAGE_NAME}:${params.IMAGE_TAG}

                echo 'üì¶ Saving image to tar (docker-archive)...'
                docker save \
                  -o ${WORKSPACE}/image.tar \
                  ${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com/${params.IMAGE_NAME}:${params.IMAGE_TAG}

                echo 'üìÅ TAR file saved at: ${WORKSPACE}/image.tar'
            """
        }
    }
}


stage("Push to Artifactory (Buildah Container)") {
    container('buildah') {

        withCredentials([string(credentialsId: 'svcacct-dsdata-caas-snapshot', variable: 'JFROG_API_KEY')]) {

            sh """
                echo 'üîê Logging into Artifactory via Buildah...'
                buildah login -u svcacct-dsdata-caas-snapshot -p ${JFROG_API_KEY} ${env.ARTIFACTORY_URL}

                echo 'üì¶ Loading docker-archive (tar) into Buildah...'
                buildah pull docker-archive:${WORKSPACE}/image.tar

                echo 'üîÑ Tagging image for Artifactory...'
                buildah tag \
                  docker.io/library/${params.IMAGE_NAME}:${params.IMAGE_TAG} \
                  ${env.ARTIFACTORY_URL}/${env.ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}

                echo '‚¨ÜÔ∏è Pushing to Artifactory...'
                buildah push --tls-verify=false \
                  ${env.ARTIFACTORY_URL}/${env.ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}

                echo 'üéâ Image migration SUCCESSFUL!'
            """
        }
    }
}




--------------------------
@Library(['common-lib3-stable']) _

properties([
    parameters([
        string(name: 'IMAGE_NAME', defaultValue: 'ethos-portal-unauth', description: 'Image name'),
        string(name: 'IMAGE_TAG', defaultValue: '1.61.0-3-unauth-uat', description: 'Tag to migrate'),
    ])
])

// =====================
// POD TEMPLATE
// =====================
String podTemplateString = """
apiVersion: v1
kind: Pod
spec:
  containers:
    - name: aws
      image: caas-docker-release-local.docker.fis.dev/jenkins/ubi8-minimal-hel-aws:3.6.3-1
      tty: true
      command: ['cat']
      resources:
        limits:
          github.com/fuse: 1

    - name: buildah
      image: caas-docker-release-local.docker.fis.dev/mirror/buildah/stable:v1.23.1
      imagePullPolicy: IfNotPresent
      command: ['cat']
      tty: true
      resources:
        limits:
          github.com/fuse: 1

  imagePullSecrets:
    - name: caas-docker-release-local-ro

  serviceAccountName: jenkins
"""

def label = "ecr-artifactory-${UUID.randomUUID().toString()}"

podTemplate(label: label, yaml: podTemplateString, runAsUser: '1000') {

node(label) {

    // =======================
    // CONSTANTS
    // =======================
    env.OLD_AWS_ACCOUNT = "588633075404"
    env.OLD_AWS_REGION  = "us-east-1"
    env.ARTIFACTORY_URL = "artifactory.fis.dev"
    env.ARTIFACTORY_REPO = "dsdata-docker-snapshot-local"

    // =======================
    // LOAD JFROG API KEY
    // =======================
    stage("Load JFrog API Key") {
        withCredentials([string(credentialsId: 'svcacct-dsdata-caas-snapshot', variable: 'JFROG_API_KEY')]) {
            env.JFROG_USER = 'svcacct-dsdata-caas-snapshot'
        }
    }

    // =======================
    // AUTH AWS (3 variables)
    // =======================
    stage("Authenticate to old AWS ECR") {
        container('aws') {
            withCredentials([
                string(credentialsId: 'aws_old_access_key', variable: 'AWS_ACCESS_KEY_ID'),
                string(credentialsId: 'aws_old_secret_key', variable: 'AWS_SECRET_ACCESS_KEY'),
                string(credentialsId: 'aws_old_session_token', variable: 'AWS_SESSION_TOKEN')
            ]) {
                sh '''
                    echo "üîê Verifying AWS credentials..."
                    aws sts get-caller-identity

                    echo "üîê Logging into AWS ECR..."
                    export ECR_PASSWORD=$(aws ecr get-login-password --region ${OLD_AWS_REGION})
                    
                    echo "AWS authentication SUCCESSFUL"
                '''
            }
        }
    }

    // =======================
    // COPY IMAGE USING BUILDAH
    // =======================
    stage("Copy Image using Buildah") {
        container('buildah') {

            withCredentials([string(credentialsId: 'svcacct-dsdata-caas-snapshot', variable: 'JFROG_API_KEY')]) {
            withCredentials([
                string(credentialsId: 'aws_old_access_key', variable: 'AWS_ACCESS_KEY_ID'),
                string(credentialsId: 'aws_old_secret_key', variable: 'AWS_SECRET_ACCESS_KEY'),
                string(credentialsId: 'aws_old_session_token', variable: 'AWS_SESSION_TOKEN')
            ]) {

                sh '''
                    echo "üîê Setting AWS environment vars..."
                    export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
                    export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
                    export AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}

                    echo "üîê Logging into AWS ECR..."
                    export ECR_PASSWORD=$(aws ecr get-login-password --region ${OLD_AWS_REGION})

                    echo "üîì Logging into Artifactory..."
                    buildah login -u ${JFROG_USER} -p ${JFROG_API_KEY} ${ARTIFACTORY_URL}

                    echo "‚¨áÔ∏è Pulling from ECR..."
                    buildah pull --tls-verify=false \
                      "docker://${OLD_AWS_ACCOUNT}.dkr.ecr.${OLD_AWS_REGION}.amazonaws.com/${IMAGE_NAME}:${IMAGE_TAG}"

                    echo "üîÑ Tagging for Artifactory..."
                    buildah tag \
                      "docker://${OLD_AWS_ACCOUNT}.dkr.ecr.${OLD_AWS_REGION}.amazonaws.com/${IMAGE_NAME}:${IMAGE_TAG}" \
                      "${ARTIFACTORY_URL}/${ARTIFACTORY_REPO}/${IMAGE_NAME}:${IMAGE_TAG}"

                    echo "‚¨ÜÔ∏è Pushing to Artifactory..."
                    buildah push --tls-verify=false \
                      "${ARTIFACTORY_URL}/${ARTIFACTORY_REPO}/${IMAGE_NAME}:${IMAGE_TAG}"

                    echo "‚úÖ Buildah copy successful!"
                '''
            }}
        }
    }

    stage("Done") {
        echo "üéâ Image migrated successfully ‚Üí ${env.ARTIFACTORY_URL}/${env.ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}"
    }

} // node
} // podTemplate







------------------------------------------------------------------------
@Library(['common-lib3-stable']) _

properties([
  parameters([
    string(name: 'IMAGE_NAME', defaultValue: 'ethos-portal-unauth', description: 'Image name'),
    string(name: 'IMAGE_TAG',  defaultValue: '1.61.0-3-unauth-uat', description: 'Tag to migrate')
  ])
])

// ==========================
// POD TEMPLATE
// ==========================
String podTemplateString = """
apiVersion: v1
kind: Pod
spec:
  containers:
    - name: aws
      image: caas-docker-release-local.docker.fis.dev/jenkins/ubi8-minimal-hel-aws:3.6.3-1
      tty: true
      command: ['cat']
      resources:
        limits:
          github.com/fuse: 1

    - name: buildah
      image: caas-docker-release-local.docker.fis.dev/mirror/buildah/stable:v1.23.1
      tty: true
      command: ['cat']
      imagePullPolicy: IfNotPresent
      resources:
        limits:
          github.com/fuse: 1

  imagePullSecrets:
    - name: caas-docker-release-local-ro

  serviceAccountName: jenkins
"""

def label = "ecr-artifactory-${UUID.randomUUID().toString()}"
podTemplate(label: label, yaml: podTemplateString, runAsUser: '1000') {

node(label) {

    // ==========================
    // ENV COMMONS
    // ==========================
    env.OLD_AWS_ACCOUNT = "588633075404"
    env.OLD_AWS_REGION  = "us-east-1"

    env.ARTIFACTORY_URL  = "artifactory.fis.dev"
    env.ARTIFACTORY_REPO = "dsdata-docker-snapshot-local"

    stage("Checkout Tools Repo") {
        checkout([
          $class: 'GitSCM',
          branches: [[name: "*/develop"]],
          userRemoteConfigs: [[
              url: "https://bitbucket.fis.dev/scm/dsdata/ethos-tools-devops.git",
              credentialsId: "ethos-portal-svc-acct"
          ]]
        ])
    }

    // ==========================
    // LOAD JFROG SECRET FROM VAULT
    // ==========================
    stage("Load JFrog API Key") {
        withCredentials([string(credentialsId: 'svcacct-dsdata-caas-snapshot', variable: 'JFROG_API_KEY')]) {
            env.JFROG_USER = "svcacct-dsdata-caas-snapshot"
        }
    }

    // ==========================
    // AUTHENTICATE TO OLD ECR
    // ==========================
    stage("Authenticate to old AWS ECR") {
        withCredentials([[ $class: 'AmazonWebServicesCredentialsBinding',
                           credentialsId: 'ASIA...FWEDL' ]]) {

            container("aws") { sh """
                aws sts get-caller-identity
                export ECR_PASSWORD=\$(aws ecr get-login-password --region ${env.OLD_AWS_REGION})
                echo "AWS ECR login successful"
            """ }
        }
    }

// ==========================
// COPY IMAGE USING BUILDAH/PODMAN
// ==========================
stage("Copy Image using Buildah") {

    // JFrog API Key from Vault
    withCredentials([string(credentialsId: 'svcacct-dsdata-caas-snapshot', variable: 'JFROG_API_KEY')]) {

        container("buildah") {

            sh """
            echo "Logging into AWS ECR (using password from earlier)‚Ä¶"
            buildah login -u AWS -p "${env.ECR_PASSWORD}" \\
              ${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com

            echo "Logging into Artifactory‚Ä¶"
            buildah login -u ${env.JFROG_USER} -p $JFROG_API_KEY ${env.ARTIFACTORY_URL}

            echo "Pulling from ECR‚Ä¶"
            buildah pull --tls-verify=false \\
              docker://${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com/${params.IMAGE_NAME}:${params.IMAGE_TAG}

            echo "Tagging for Artifactory..."
            buildah tag \\
              docker://${env.OLD_AWS_ACCOUNT}.dkr.ecr.${env.OLD_AWS_REGION}.amazonaws.com/${params.IMAGE_NAME}:${params.IMAGE_TAG} \\
              ${env.ARTIFACTORY_URL}/${env.ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}

            echo "Pushing to Artifactory‚Ä¶"
            buildah push --tls-verify=false \\
              ${env.ARTIFACTORY_URL}/${env.ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}

            echo "‚úî Done! Buildah copy successful."
            """
        }
    }
}


    stage("Done") {
        echo "‚úî Image migrated successfully ‚Üí ${env.ARTIFACTORY_URL}/${env.ARTIFACTORY_REPO}/${params.IMAGE_NAME}:${params.IMAGE_TAG}"
    }
}}










-------------------------------------------------------
pipeline {
    agent any

    environment {
        OLD_ACCOUNT_ID = "588633075404"
        OLD_REGION     = "us-east-1"
        IMAGE_NAME     = "ethos-portal"
        IMAGE_TAG      = "1.61.0-3-unauth-uat"

        NEW_ARTIFACTORY_URL  = "artifactory.fis.dev/artifactory"
        NEW_ARTIFACTORY_REPO = "dsdata-docker-snapshot-local"
        NEW_ARTIFACTORY_IMAGE = "ethos-portal-unauth"

        // Username is static (not in Vault secret)
        JFROG_USER = "svcacct-dsdata-caas-snapshot"
    }

    stages {

        stage('Load JFrog API Key from Vault') {
            steps {
                withVault([
                    vaultSecrets: [
                        [
                            path: 'jenkins/artifactory-svc-acct-snapshot',
                            secretValues: [
                                [envVar: 'JFROG_API_KEY', vaultKey: 'svcacct-dsdata-caas-snapshot']
                            ]
                        ]
                    ]
                ]) {
                    sh '''
                        echo "JFrog user loaded = ${JFROG_USER}"
                        echo "JFrog API key length = ${#JFROG_API_KEY}"
                    '''
                }
            }
        }

        stage('Authenticate to AWS ECR') {
            steps {
                withCredentials([aws(
                    credentialsId: 'ASIAYSDKD53GLQBFWEDL',
                    accessKeyVariable: 'AWS_ACCESS_KEY_ID',
                    secretKeyVariable: 'AWS_SECRET_ACCESS_KEY'
                )]) {
                    sh '''
                        echo "üîê Logging into AWS ECR..."
                        export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
                        export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}

                        ECR_PASSWORD=$(aws ecr get-login-password --region ${OLD_REGION})
                        echo "${ECR_PASSWORD}" > ecr_pass.txt
                    '''
                }
            }
        }

        stage('Copy Image from ECR to Artifactory') {
            steps {
                sh '''
                    echo "üöÄ Copying image using Skopeo..."

                    skopeo copy \
                      --override-os linux \
                      --override-arch amd64 \
                      --src-creds "AWS:$(cat ecr_pass.txt)" \
                      --dest-creds "${JFROG_USER}:${JFROG_API_KEY}" \
                      docker://${OLD_ACCOUNT_ID}.dkr.ecr.${OLD_REGION}.amazonaws.com/${IMAGE_NAME}:${IMAGE_TAG} \
                      docker://${NEW_ARTIFACTORY_URL}/${NEW_ARTIFACTORY_REPO}/${NEW_ARTIFACTORY_IMAGE}:${IMAGE_TAG}

                    echo "üéâ Image migration completed successfully!"
                '''
            }
        }
    }
}





------------------------------------------------------------------
#!/bin/bash
set -e

###########################################
# CONFIGURATION
###########################################

# Old AWS Account Details (ECR Source)
OLD_ACCOUNT_ID="<OLD_ACCOUNT_ID>"
OLD_REGION="eu-west-2"

# Image you want to migrate
IMAGE_NAME="<IMAGE_NAME>"        # e.g. ethos-portal-auth
IMAGE_TAG="<IMAGE_TAG>"          # e.g. 1.0.0

# New Artifactory (Destination)
# Example: mycompany.jfrog.io/hydra-docker-virtual
NEW_ARTIFACTORY_URL="<ARTIFACTORY_URL>"
NEW_ARTIFACTORY_USER="<JFROG_USER>"
NEW_ARTIFACTORY_API_KEY="<JFROG_API_KEY>"

###########################################
# START
###########################################

echo "-----------------------------------------------------"
echo " üîê Logging into OLD ECR"
echo "-----------------------------------------------------"

aws ecr get-login-password --region $OLD_REGION \
  | docker login --username AWS --password-stdin \
    ${OLD_ACCOUNT_ID}.dkr.ecr.${OLD_REGION}.amazonaws.com

echo ""
echo "-----------------------------------------------------"
echo " ‚¨áÔ∏è Pulling image from OLD ECR"
echo "-----------------------------------------------------"

SOURCE_IMAGE="${OLD_ACCOUNT_ID}.dkr.ecr.${OLD_REGION}.amazonaws.com/${IMAGE_NAME}:${IMAGE_TAG}"

docker pull $SOURCE_IMAGE

echo ""
echo "-----------------------------------------------------"
echo " üîÑ Retagging for Artifactory"
echo "-----------------------------------------------------"

TARGET_IMAGE="${NEW_ARTIFACTORY_URL}/${IMAGE_NAME}:${IMAGE_TAG}"

docker tag $SOURCE_IMAGE $TARGET_IMAGE

echo ""
echo "-----------------------------------------------------"
echo " üîê Logging into Artifactory"
echo "-----------------------------------------------------"

docker login $NEW_ARTIFACTORY_URL \
  -u $NEW_ARTIFACTORY_USER \
  -p $NEW_ARTIFACTORY_API_KEY

echo ""
echo "-----------------------------------------------------"
echo " ‚¨ÜÔ∏è Pushing image to Artifactory"
echo "-----------------------------------------------------"

docker push $TARGET_IMAGE

echo ""
echo "-----------------------------------------------------"
echo " ‚úÖ Migration Complete!"
echo "-----------------------------------------------------"
echo "Old ECR Image : $SOURCE_IMAGE"
echo "New Artifactory: $TARGET_IMAGE"
echo "-----------------------------------------------------"



----------------------------------------------------------------------------------------
#!/usr/bin/env python3
# Proposed architecture for Ethos Portal (Akamai front + external Vault)
# Requires: pip install diagrams graphviz

from diagrams import Diagram, Cluster, Edge
from diagrams.aws.network import APIGateway, ELB
from diagrams.aws.compute import EKS, Lambda
from diagrams.aws.storage import S3
from diagrams.aws.database import RDS as RDSIcon, Aurora, Dynamodb
from diagrams.onprem.security import Vault as VaultIcon
from diagrams.generic.blank import Blank   # generic box we can label "Akamai"
from diagrams.onprem.client import Users

# ---- knobs ----
TITLE = "Ethos Portal ‚Äì Proposed Architecture (Akamai + Vault)"
USE_CLOUDFRONT = False  # set True to show CF as origin shield / multi-CDN

GRAPH_ATTR = dict(
    rankdir="LR", splines="ortho", nodesep="1.1", ranksep="1.2", pad="0.2"
)
NODE_ATTR  = dict(fontname="Helvetica", fontsize="12", margin="0.06,0.04")
EDGE_ATTR  = dict(arrowsize="0.9", penwidth="1.2", labelfontsize="10",
                  tailclip="true", headclip="true")

def draw():
    with Diagram(TITLE, filename="proposed_ethos_portal", show=False, outformat="png",
                 graph_attr=GRAPH_ATTR, node_attr=NODE_ATTR, edge_attr=EDGE_ATTR):

        user = Users("End-User / Browser")
        akamai = Blank("Akamai (DNS + CDN)")  # front door

        if USE_CLOUDFRONT:
            # Optional: CF as origin shield / AWS edge features (Lambda@Edge/OAC)
            from diagrams.aws.network import CloudFront
            cf = CloudFront("CloudFront (optional)")
            user >> Edge(label="HTTPS") >> akamai >> Edge(label="origin fetch") >> cf
            edge_source = cf
        else:
            user >> Edge(label="HTTPS") >> akamai
            edge_source = akamai

        # Dynamic path
        apigw = APIGateway("API Gateway (HTTP)")
        nlb   = ELB("NLB (TLS 443)")

        # EKS app cluster
        with Cluster("EKS Cluster (Private VPC)"):
            ingress = EKS("Ingress")
            backend = EKS("Backend Service")
            keycloak = EKS("Keycloak (Auth)")

        # Static/objects and async
        s3 = S3("S3 (static/assets/state)")
        lam = Lambda("Lambda (S3 events)")

        # Data stores
        rds = Aurora("Aurora (authorized FIS users)")  # change to RDSIcon(...) if needed
        ddb = Dynamodb("DynamoDB (external/unauth)")

        # External secrets
        vault = VaultIcon("HashiCorp Vault\n(FIS external)")

        # --- Flows ---
        # Dynamic requests
        edge_source >> Edge(label="dynamic HTTPS") >> apigw \
                    >> Edge(label="TLS 443") >> nlb \
                    >> Edge(label="ingress") >> ingress >> backend

        # Static content
        edge_source >> Edge(label="static HTTPS") >> s3

        # Auth in-cluster
        backend << Edge(label="OIDC/SAML") >> keycloak

        # Data write paths
        backend >> Edge(label="authorized (FIS)") >> rds
        backend >> Edge(style="dashed", label="external/unauth") >> ddb

        # S3 events
        backend >> Edge(label="read/write") >> s3
        s3 >> Edge(label="event") >> lam

        # Secrets from external Vault
        backend >> Edge(style="dotted", label="AppRole/JWT") >> vault
        keycloak >> Edge(style="dotted", label="AppRole/JWT") >> vault

if __name__ == "__main__":
    draw()
    print("Wrote proposed_ethos_portal.png")










------------------------------------------------------------------------------------------
def emit_mermaid_context(drawio_path="01_context_container.mmd", md_path=None):
    cf  = (cf_summary() or {})
    gw  = (apigw_v2_summary() or {})
    lb  = (nlb_summary() or {})
    eks = (eks_summary() or {})
    rds = rds_pick()
    ddb = ddb_pick()
    keycloak = detect_keycloak_name()
    s3label = "S3 (static/state)"

    cf_label  = f"CloudFront {cf.get('id','')}".strip()
    gw_label  = f"API Gateway {gw.get('id','')}".strip()
    lb_label  = f"NLB {lb.get('name','')}".strip()
    eks_label = (eks.get("name") or "EKS").strip()

    if rds and rds.get("type") == "aurora":
        rds_label = f"Aurora {rds.get('id','')}".strip()
    elif rds:
        rds_label = f"RDS {rds.get('id','')}".strip()
    else:
        rds_label = "RDS"

    ddb_label = (ddb or "DynamoDB").strip()

    # Use '-- text -->' (NOT the '|text|' form) for maximum draw.io compatibility.
    lines = [
        "flowchart LR",
        "  U[User] --> R53[Route 53] --> CF[" + cf_label + "]",
        "  CF -- static --> S3[" + s3label + "]",
        "  CF -- dynamic --> GW[" + gw_label + "]",
        "  GW -- 443/TLS --> NLB[" + lb_label + "]",
        "  subgraph EKS[" + eks_label + "]",
        "    ING[Ingress] --> BE[Backend Service]",
        "    BE -- OIDC/SAML --> KC[" + keycloak + "]",
        "  end",
        "  NLB -- ingress --> ING",
        "  BE -- authorized (FIS) --> RDS[" + rds_label + "]",
        "  BE -. external/unauth .-> DDB[" + ddb_label + "]",
        "  BE -- read/write --> S3",
    ]
    mermaid_raw = "\n".join(lines)

    (OUTDIR / drawio_path).write_text(mermaid_raw)
    if md_path:
        (OUTDIR / md_path).write_text("```mermaid\n" + mermaid_raw + "\n```")








-----------------------------------------------------------------------------------------------------------
def emit_mermaid_context(drawio_path="01_context_container.mmd", md_path=None):
    cf  = cf_summary() or {}
    gw  = apigw_v2_summary() or {}
    lb  = nlb_summary() or {}
    eks = eks_summary() or {}
    rds = rds_pick()
    ddb = ddb_pick()
    keycloak = detect_keycloak_name()
    s3label = "S3 (static/state)"

    # Build labels (no backslashes/newlines inside {} for safety)
    cf_label  = f"CloudFront {cf.get('id','')}".strip()
    gw_label  = f"API Gateway {gw.get('id','')}".strip()
    lb_label  = f"NLB {lb.get('name','')}".strip()
    eks_label = (eks.get("name") or "EKS").strip()

    if rds and rds.get("type") == "aurora":
        rds_label = f"Aurora {rds.get('id','')}".strip()
    elif rds:
        rds_label = f"RDS {rds.get('id','')}".strip()
    else:
        rds_label = "RDS"

    ddb_label = (ddb or "DynamoDB").strip()

    lines = [
        "flowchart LR",
        "  U[User] --> R53[Route 53] --> CF[" + cf_label + "]",
        "  CF -->|static| S3[" + s3label + "]",
        "  CF -->|dynamic| GW[" + gw_label + "]",
        "  GW -->|443/TLS| NLB[" + lb_label + "]",
        "  subgraph EKS[" + eks_label + "]",
        "    ING[Ingress] --> BE[Backend Service]",
        "    BE -- OIDC/SAML --> KC[" + keycloak + "]",
        "  end",
        "  NLB -->|ingress| ING",
        "  BE -->|authorized (FIS)| RDS[" + rds_label + "]",
        "  BE -.->|external/unauth| DDB[" + ddb_label + "]",
        "  BE -->|read/write| S3",
    ]
    mermaid_raw = "\n".join(lines)

    # Write raw Mermaid for draw.io
    (OUTDIR / drawio_path).write_text(mermaid_raw)

    # Optional: also write a Markdown-flavored copy with fences (for Confluence/GitHub)
    if md_path:
        (OUTDIR / md_path).write_text("```mermaid\n" + mermaid_raw + "\n```")









---------------------------------------------------
def emit_mermaid_context(path):
    cf  = cf_summary() or {}
    gw  = apigw_v2_summary() or {}
    lb  = nlb_summary() or {}
    eks = eks_summary() or {}
    rds = rds_pick()
    ddb = ddb_pick()
    keycloak = detect_keycloak_name()
    s3label = "S3 (static/state)"

    # --- Build labels OUTSIDE of {} to avoid backslashes inside f-string expressions ---
    cf_label  = f"CloudFront\\n{cf.get('id','')}"
    gw_label  = f"API Gateway\\n{gw.get('id','')}"
    lb_label  = f"NLB\\n{lb.get('name','')}"
    eks_label = f"{eks.get('name','') or 'EKS'}"

    rds_title = "Aurora" if (rds and rds.get("type") == "aurora") else "RDS"
    rds_id    = rds.get("id") if rds else ""
    rds_label = rds_title + (f"\\n{rds_id}" if rds_id else "")

    ddb_label = ddb or "DynamoDB"

    m = f"""
flowchart LR
  U[User] --> R53[Route 53] --> CF[{cf_label}]
  CF -->|static| S3[{s3label}]
  CF -->|dynamic| GW[{gw_label}]
  GW -->|443/TLS| NLB[{lb_label}]
  subgraph EKS[{eks_label}]
    ING[Ingress] --> BE[Backend Service]
    BE -- OIDC/SAML --> KC[{keycloak}]
  end
  NLB -->|ingress| ING
  BE -->|authorized (FIS)| RDS[{rds_label}]
  BE -.->|external/unauth| DDB[{ddb_label}]
  BE -->|read/write| S3
"""
    (OUTDIR / path).write_text("```mermaid\n" + m.strip() + "\n```")







--------------
GRAPH_ATTR = dict(rankdir="LR", splines="ortho", nodesep="1.1", ranksep="1.2", pad="0.2", concentrate="false")
NODE_ATTR  = dict(fontname="Helvetica", fontsize="12", margin="0.06,0.04")
EDGE_ATTR  = dict(arrowsize="0.9", penwidth="1.2", labelfontsize="10", tailclip="true", headclip="true")

with Diagram("Ethos Portal ‚Äì Context & Containers",
             filename=str(OUTDIR/"01_context_container"),
             show=False, outformat="svg",         # SVG = crisp + editable
             graph_attr=GRAPH_ATTR, node_attr=NODE_ATTR, edge_attr=EDGE_ATTR):

    # ‚Ä¶
    cf_i >> Edge(label="dynamic", minlen="2") >> gw_i \
        >> Edge(label="443/TLS", minlen="2") >> nlb_i \
        >> Edge(label="ingress", minlen="2") >> ingress

    # Add an invisible ‚Äúknee‚Äù if a line crosses an icon:
    from diagrams.custom import Custom
    SPACER = "data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/>"
    elbow = Custom("", SPACER)   # 1x1 transparent node

    backend >> Edge(minlen="2") >> elbow >> Edge(minlen="2") >> rds_i


----

def emit_mermaid_context(path):
    cf = cf_summary(); gw = apigw_v2_summary(); lb = nlb_summary(); eks = eks_summary()
    rds = rds_pick(); ddb = ddb_pick()
    keycloak = detect_keycloak_name()
    s3label = "S3 (static/state)"
    m = f"""
flowchart LR
  U[User] --> R53[Route 53] --> CF[CloudFront\\n{(cf or {}).get('id','')}]
  CF -->|static| S3[{s3label}]
  CF -->|dynamic| GW[API Gateway\\n{(gw or {}).get('id','')}]
  GW -->|443/TLS| NLB[NLB\\n{(lb or {}).get('name','')}]
  subgraph EKS[{(eks or {}).get('name','')}]
    ING[Ingress] --> BE[Backend Service]
    BE -- OIDC/SAML --> KC[{keycloak}]
  end
  NLB -->|ingress| ING
  BE -->|authorized (FIS)| RDS[{('Aurora' if (rds and rds['type']=='aurora') else 'RDS') + '\\n' + (rds['id'] if rds else '')}]
  BE -.->|external/unauth| DDB[{ddb or 'DynamoDB'}]
  BE -->|read/write| S3
"""
    (OUTDIR / path).write_text("```mermaid\n" + m.strip() + "\n```")

# call it:
emit_mermaid_context("01_context_container.mmd")













------------------------------------------------------------------------------------------
#!/usr/bin/env python3
"""
Ethos Portal ‚Äì diagram generator (SVG) from AWS CLI JSON exports.

Outputs (to ./diagrams):
  - 01_context_container.svg                 (clean context & containers)
  - 01b_context_container_network.svg        (context + AWS/VPC/Subnets/AZ frame)
  - 02_request_path.svg                      (API request path)
  - 03_topology.svg                          (compact VPC topology)

Author: you + ChatGPT
"""

import json
import pathlib
import re
from contextlib import suppress

# diagrams (mingrammer)
from diagrams import Diagram, Cluster, Edge
from diagrams.aws.network import (
    Route53, CloudFront, APIGateway, ELB,
    InternetGateway, PrivateSubnet, PublicSubnet
)
from diagrams.aws.compute import EKS, Lambda
from diagrams.aws.storage import S3
from diagrams.aws.database import RDS as RDSIcon, Aurora, Dynamodb
from diagrams.aws.security import SecretsManager

# Optional VPC Endpoint icon (not in all versions of diagrams)
try:
    from diagrams.aws.network import VPCEndpointInterface as VPCEndpointIcon
except Exception:
    VPCEndpointIcon = None

# ----------------------------- config & helpers -----------------------------

EXPORTS = pathlib.Path("as-is/exports")
OUTDIR = pathlib.Path("diagrams")
OUTDIR.mkdir(exist_ok=True, parents=True)

# Graphviz polish
GRAPH_ATTR = dict(rankdir="LR", splines="ortho", nodesep="1.0", ranksep="1.2", pad="0.2")
NODE_ATTR = dict(fontname="Helvetica", fontsize="12", margin="0.05,0.03")
EDGE_ATTR = dict(arrowsize="0.8", penwidth="1.2")

def load_json(name):
    p = EXPORTS / name
    if not p.exists():
        return None
    with open(p) as f:
        return json.load(f)

def first(lst, default=None):
    return lst[0] if lst else default

def find_first_file(glob_pattern: str):
    files = sorted(EXPORTS.glob(glob_pattern))
    return files[0] if files else None

# ----------------------------- light parsers -----------------------------

def cf_summary():
    lst = (load_json("cf_distributions.json") or {}).get("DistributionList", {}).get("Items", [])
    if not lst:
        return None
    did = lst[0]["Id"]
    conf = (load_json(f"cf_{did}.json") or {}).get("Distribution", {}).get("DistributionConfig", {})
    aliases = conf.get("Aliases", {}).get("Items", []) or []
    return {"id": did, "aliases": aliases}

def apigw_v2_summary():
    items = (load_json("apigw_v2_apis.json") or {}).get("Items", [])
    if not items:
        return None
    api = items[0]
    api_id = api.get("ApiId")
    name = api.get("Name")
    integ = load_json(f"apigw_v2_{api_id}_integrations.json")
    nlb_links = []
    if integ and "Items" in integ:
        for it in integ["Items"]:
            uri = (it.get("IntegrationUri") or it.get("IntegrationUriArn") or "")
            if "elasticloadbalancing" in uri:
                nlb_links.append(uri)
    return {"id": api_id, "name": name, "nlb_links": nlb_links}

def nlb_summary():
    nlbs = load_json("elbv2_nlbs.json") or []
    if not nlbs:
        lbs = (load_json("elbv2_lbs.json") or {}).get("LoadBalancers", [])
        nlbs = [lb for lb in lbs if lb.get("Type") == "network"]
    if not nlbs:
        return None
    lb = nlbs[0]
    # Try to find its listeners (from export files like elbv2_<LBID>_listeners.json)
    listeners_file = find_first_file("elbv2_*_listeners.json")
    port_label = ""
    if listeners_file:
        try:
            j = json.load(open(listeners_file))
            ports = [str(li.get("Port")) for li in j.get("Listeners", []) if li.get("Port")]
            if ports:
                port_label = f" :{','.join(sorted(set(ports)))}"
        except Exception:
            pass
    return {
        "name": lb.get("LoadBalancerName"),
        "dns": lb.get("DNSName"),
        "port_label": port_label,
        "vpc": lb.get("VpcId"),
    }

def eks_summary():
    names = (load_json("eks_clusters.json") or {}).get("clusters", [])
    if not names:
        return None
    return {"name": names[0]}

def detect_keycloak_name():
    for fname in EXPORTS.glob("eks_*_deploys.yaml"):
        try:
            txt = open(fname).read().lower()
            m = re.search(r'name:\s*keycloak[^\n]*', txt)
            if m:
                return "Keycloak (EKS)"
        except Exception:
            pass
    return "Keycloak"

def rds_pick():
    clusters = (load_json("rds_clusters.json") or {}).get("DBClusters", [])
    insts = (load_json("rds_instances.json") or {}).get("DBInstances", [])
    pickc = first([c for c in clusters if re.search(r'ethos|portal|keycloak', c.get("DBClusterIdentifier",""), re.I)], default=first(clusters))
    if pickc:
        return {"type": "aurora", "id": pickc.get("DBClusterIdentifier")}
    picki = first([i for i in insts if re.search(r'ethos|portal|keycloak', i.get("DBInstanceIdentifier",""), re.I)], default=first(insts))
    if picki:
        return {"type": "rds", "id": picki.get("DBInstanceIdentifier")}
    return None

def ddb_pick():
    names = (load_json("ddb_tables.json") or {}).get("TableNames", [])
    return first([n for n in names if re.search(r'ethos|portal', n, re.I)], default=first(names))

def secrets_summary():
    secrets = (load_json("secrets_list.json") or {}).get("SecretList", [])
    picks = [s for s in secrets if re.search(r'ethos|portal|eks|keycloak', (s.get("Name") or ""), re.I)]
    picks = picks or secrets[:1]
    return [s.get("Name") for s in picks[:2]] if picks else []

def pick_vpc():
    vpcs = load_json("vpcs.json") or {}
    return first(vpcs.get("Vpcs", []), default={"VpcId": "vpc-unknown"})

def list_subnets(vpc_id):
    subnets = (load_json("subnets.json") or {}).get("Subnets", [])
    subs = [s for s in subnets if s.get("VpcId") == vpc_id]
    pubs = [s for s in subs if s.get("MapPublicIpOnLaunch")]
    pris = [s for s in subs if not s.get("MapPublicIpOnLaunch")]
    def by_az(xs):
        d = {}
        for s in xs:
            d.setdefault(s.get("AvailabilityZone", "AZ-?"), []).append(s)
        return d
    return by_az(pubs), by_az(pris)

# ----------------------------- tiny layout helper -----------------------------

from diagrams.custom import Custom
SPACER_ICON = "data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/>"
def spacer(label=""):
    """A 1x1 transparent node that forces neat elbows when needed."""
    return Custom(label, SPACER_ICON)

# ----------------------------- diagram 1 (clean) -----------------------------

def draw_context_container_clean():
    cf = cf_summary(); gw = apigw_v2_summary(); lb = nlb_summary(); eks = eks_summary()
    rds = rds_pick(); ddb = ddb_pick(); secrets = secrets_summary()
    keycloak_name = detect_keycloak_name()

    with Diagram("Ethos Portal ‚Äì Context & Containers",
                 filename=str(OUTDIR/"01_context_container"),
                 show=False, outformat="svg",
                 graph_attr=GRAPH_ATTR, node_attr=NODE_ATTR, edge_attr=EDGE_ATTR):

        # Edge & CDN
        user = Route53("User")
        r53 = Route53("Route 53")
        cf_i = CloudFront(f"CloudFront\n{(cf or {}).get('id','')}")
        gw_i = APIGateway(f"API Gateway\n{(gw or {}).get('id','')}")
        nlb_i = ELB(f"NLB{(lb or {}).get('port_label','')}\n{(lb or {}).get('name','')}")
        s3_i = S3("S3 (static/state)")
        lam_i = Lambda(first((load_json("lambda_functions.json") or {}).get("Functions", []), {"FunctionName":"Lambda"})["FunctionName"])

        # EKS cluster & app containers
        with Cluster(f"EKS {(eks or {}).get('name','')}"):
            ingress = EKS("Ingress")
            backend = EKS("Backend Service")
            keycloak = EKS(keycloak_name)

        # Data stores & secrets
        if rds and rds["type"] == "aurora":
            rds_i = Aurora(f"Aurora\n{rds['id']}")
        elif rds:
            rds_i = RDSIcon(f"RDS\n{rds['id']}")
        else:
            rds_i = RDSIcon("RDS")

        ddb_i = Dynamodb(ddb or "DynamoDB")
        secnodes = [SecretsManager(n) for n in (secrets or ["Secrets Manager"])]

        # Flow
        user >> Edge(label="HTTPS", minlen="2") >> r53 >> Edge(label="Alias", minlen="2") >> cf_i
        cf_i  >> Edge(label="static", minlen="2") >> s3_i
        cf_i  >> Edge(label="dynamic", minlen="2") >> gw_i \
              >> Edge(label="443/TLS", minlen="2") >> nlb_i \
              >> Edge(label="ingress", minlen="2") >> ingress >> backend

        # Auth & storage decisions
        elbow = spacer()  # neat elbow for RDS branch
        backend << Edge(label="OIDC/SAML", minlen="2") >> keycloak
        backend >> Edge(label="authorized (FIS)", minlen="2") >> elbow >> Edge(minlen="2") >> rds_i
        backend >> Edge(style="dashed", label="external/unauth", minlen="2") >> ddb_i

        # Events & secrets use
        backend >> Edge(label="read/write", minlen="2") >> s3_i
        s3_i >> Edge(label="event", minlen="2") >> lam_i
        for s in secnodes:
            backend >> Edge(style="dotted", label="IRSA", minlen="2") >> s

# ----------------------------- diagram 1b (with AWS/VPC/AZ frame) -----------------------------

def draw_context_container_with_network_frame():
    cf = cf_summary(); gw = apigw_v2_summary(); lb = nlb_summary(); eks = eks_summary()
    rds = rds_pick(); ddb = ddb_pick(); secrets = secrets_summary()
    keycloak_name = detect_keycloak_name()

    vpc_id = pick_vpc().get("VpcId", "vpc-unknown")
    pubs_by_az, pris_by_az = list_subnets(vpc_id)

    with Diagram("Ethos Portal ‚Äì Context & Containers (with AWS/VPC frame)",
                 filename=str(OUTDIR/"01b_context_container_network"),
                 show=False, outformat="svg",
                 graph_attr=GRAPH_ATTR, node_attr=NODE_ATTR, edge_attr=EDGE_ATTR):

        user = Route53("User")
        r53 = Route53("Route 53")

        with Cluster("AWS Account"):
            with Cluster(f"VPC {vpc_id}"):
                # AZ clusters (two max for clarity)
                az_names = sorted(set(list(pubs_by_az.keys()) + list(pris_by_az.keys())))[:2]
                az_nodes = []
                for az in az_names:
                    with Cluster(f"{az}"):
                        pub = PublicSubnet("public")
                        pri = PrivateSubnet("private")
                        az_nodes.append((pub, pri))

                cf_i = CloudFront(f"CloudFront\n{(cf or {}).get('id','')}")
                gw_i = APIGateway(f"API Gateway\n{(gw or {}).get('id','')}")
                nlb_i = ELB(f"NLB{(lb or {}).get('port_label','')}\n{(lb or {}).get('name','')}")
                s3_i = S3("S3 (static/state)")
                lam_i = Lambda(first((load_json("lambda_functions.json") or {}).get("Functions", []), {"FunctionName":"Lambda"})["FunctionName"])

                with Cluster(f"EKS {(eks or {}).get('name','')}"):
                    ingress = EKS("Ingress")
                    backend = EKS("Backend Service")
                    keycloak = EKS(keycloak_name)

                # Data stores & secrets
                if rds and rds["type"] == "aurora":
                    rds_i = Aurora(f"Aurora\n{rds['id']}")
                elif rds:
                    rds_i = RDSIcon(f"RDS\n{rds['id']}")
                else:
                    rds_i = RDSIcon("RDS")

                ddb_i = Dynamodb(ddb or "DynamoDB")
                secnodes = [SecretsManager(n) for n in (secrets or ["Secrets Manager"])]

        # Edges across frame
        user >> Edge(label="HTTPS", minlen="2") >> r53 >> Edge(label="Alias", minlen="2") >> cf_i
        cf_i  >> Edge(label="static", minlen="2") >> s3_i
        cf_i  >> Edge(label="dynamic", minlen="2") >> gw_i \
              >> Edge(label="443/TLS", minlen="2") >> nlb_i \
              >> Edge(label="ingress", minlen="2") >> ingress >> backend

        backend << Edge(label="OIDC/SAML", minlen="2") >> keycloak
        backend >> Edge(label="authorized (FIS)", minlen="2") >> rds_i
        backend >> Edge(style="dashed", label="external/unauth", minlen="2") >> ddb_i
        backend >> Edge(label="read/write", minlen="2") >> s3_i
        s3_i >> Edge(label="event", minlen="2") >> lam_i
        for s in secnodes:
            backend >> Edge(style="dotted", label="IRSA", minlen="2") >> s

# ----------------------------- diagram 2 (request path) -----------------------------

def draw_request_path():
    cf = cf_summary(); gw = apigw_v2_summary(); lb = nlb_summary(); eks = eks_summary()

    with Diagram("Ethos Portal ‚Äì API Request Path",
                 filename=str(OUTDIR/"02_request_path"),
                 show=False, outformat="svg",
                 graph_attr=GRAPH_ATTR, node_attr=NODE_ATTR, edge_attr=EDGE_ATTR):

        user = Route53("Browser")
        r53 = Route53("Route 53")
        cf_i = CloudFront(f"CloudFront\n{(cf or {}).get('id','')}")
        gw_i = APIGateway(f"API GW\n{(gw or {}).get('id','')}")
        lb_i = ELB(f"NLB{(lb or {}).get('port_label','')}\n{(lb or {}).get('name','')}")

        with Cluster(f"EKS {(eks or {}).get('name','')}"):
            ingress = EKS("Ingress")
            svc = EKS("Backend")
            keycloak = EKS("Keycloak")

        user >> Edge(label="HTTPS", minlen="2") >> r53 >> Edge(label="Alias", minlen="2") >> cf_i
        cf_i >> Edge(label="HTTPS", minlen="2") >> gw_i \
             >> Edge(label="TLS 443", minlen="2") >> lb_i \
             >> Edge(label="TCP 443", minlen="2") >> ingress >> svc
        svc << Edge(label="OIDC", minlen="2") >> keycloak

# ----------------------------- diagram 3 (topology schematic) -----------------------------

def draw_topology_schematic():
    vpc_id = pick_vpc().get("VpcId", "vpc-unknown")
    pubs_by_az, pris_by_az = list_subnets(vpc_id)

    with Diagram(f"Ethos Portal ‚Äì VPC Topology (schematic)",
                 filename=str(OUTDIR/"03_topology"),
                 show=False, outformat="svg",
                 graph_attr=GRAPH_ATTR, node_attr=NODE_ATTR, edge_attr=EDGE_ATTR):

        igw = InternetGateway("IGW")

        with Cluster(f"VPC {vpc_id}"):
            # two AZs for readability
            az_names = sorted(set(list(pubs_by_az.keys()) + list(pris_by_az.keys())))[:2]

            per_az = []
            for az in az_names:
                with Cluster(az):
                    pub = PublicSubnet("public")
                    pri = PrivateSubnet("private")
                    nlb_eni = ELB("NLB ENI")
                    per_az.append((pub, pri, nlb_eni))

            s3e = VPCEndpointIcon("S3 Endpoint") if VPCEndpointIcon else ELB("S3 Endpoint")

        # Wiring
        with suppress(Exception):
            if per_az:
                igw >> per_az[0][0]
                if len(per_az) > 1:
                    igw >> per_az[1][0]
        for (_, pri, nlb_eni) in per_az:
            nlb_eni - pri
            s3e - pri

# ----------------------------- entrypoint -----------------------------

if __name__ == "__main__":
    draw_context_container_clean()
    draw_context_container_with_network_frame()
    draw_request_path()
    draw_topology_schematic()
    print(f"SVGs written to {OUTDIR.resolve()}")








-------------------------------------------------------------------------------------------------------------------------------------------
#!/usr/bin/env bash
set -euo pipefail

# === Config ===
OUT="${OUT:-as-is/exports}"                 # override with OUT=/path ./export_data_tier.sh
REGION="${AWS_REGION:-${REGION:-eu-west-1}}"# override with REGION=us-east-1 ./export_data_tier.sh
FILTER_NAME="${FILTER_NAME:-}"              # optional: grep filter for names/ids (e.g., ethos|portal|keycloak)

mkdir -p "$OUT"
echo "Exporting data-tier metadata to $OUT (region: $REGION)"
have_jq() { command -v jq >/dev/null 2>&1; }

# ---------- RDS / Aurora ----------
aws rds describe-db-instances --region "$REGION" > "$OUT/rds_instances.json"
aws rds describe-db-clusters  --region "$REGION" > "$OUT/rds_clusters.json"

if have_jq; then
  echo "Tagging RDS instances‚Ä¶"
  jq -r '.DBInstances[].DBInstanceArn' "$OUT/rds_instances.json" 2>/dev/null \
    | { if [ -n "$FILTER_NAME" ]; then grep -Ei "$FILTER_NAME" || true; else cat; fi; } \
    | while read -r ARN; do
        safe="$(echo "$ARN" | sed 's#.*:db:##')"
        aws rds list-tags-for-resource --resource-name "$ARN" --region "$REGION" > "$OUT/rds_tags_${safe}.json" || true
      done

  echo "Tagging RDS clusters‚Ä¶"
  jq -r '.DBClusters[].DBClusterArn' "$OUT/rds_clusters.json" 2>/dev/null \
    | { if [ -n "$FILTER_NAME" ]; then grep -Ei "$FILTER_NAME" || true; else cat; fi; } \
    | while read -r ARN; do
        safe="$(echo "$ARN" | sed 's#.*:cluster:##')"
        aws rds list-tags-for-resource --resource-name "$ARN" --region "$REGION" > "$OUT/rds_cluster_tags_${safe}.json" || true
      done
else
  echo "jq not found; skipping per-resource tag exports." >&2
fi

# ---------- DynamoDB ----------
aws dynamodb list-tables --region "$REGION" > "$OUT/ddb_tables.json"
if have_jq; then
  jq -r '.TableNames[]' "$OUT/ddb_tables.json" 2>/dev/null \
    | { if [ -n "$FILTER_NAME" ]; then grep -Ei "$FILTER_NAME" || true; else cat; fi; } \
    | while read -r T; do
        aws dynamodb describe-table --table-name "$T" --region "$REGION" > "$OUT/ddb_${T}.json" || true
      done
else
  echo "jq not found; export contains list only (no per-table describe)." >&2
fi

# ---------- Secrets Manager (metadata only) ----------
aws secretsmanager list-secrets --region "$REGION" > "$OUT/secrets_list.json"
if have_jq; then
  jq -r '.SecretList[].ARN' "$OUT/secrets_list.json" 2>/dev/null \
    | { if [ -n "$FILTER_NAME" ]; then grep -Ei "$FILTER_NAME" || true; else cat; fi; } \
    | while read -r SARN; do
        base="$(basename "$SARN")"
        aws secretsmanager get-resource-policy --secret-id "$SARN" --region "$REGION" > "$OUT/secret_policy_${base}.json" 2>/dev/null || true
      done
fi

echo "Done."



--------
# ----- RDS / Aurora -----
aws rds describe-db-instances --region "$REGION" > "$OUT/rds_instances.json"
aws rds describe-db-clusters  --region "$REGION" > "$OUT/rds_clusters.json"
# (optional) tags for identification
jq -r '.DBInstances[].DBInstanceArn' "$OUT/rds_instances.json" 2>/dev/null | while read -r ARN; do
  aws rds list-tags-for-resource --resource-name "$ARN" --region "$REGION" > "$OUT/rds_tags_$(echo "$ARN" | sed 's#.*:db:##').json" || true
done
jq -r '.DBClusters[].DBClusterArn' "$OUT/rds_clusters.json" 2>/dev/null | while read -r ARN; do
  aws rds list-tags-for-resource --resource-name "$ARN" --region "$REGION" > "$OUT/rds_cluster_tags_$(echo "$ARN" | sed 's#.*:cluster:##').json" || true
done

# ----- DynamoDB -----
aws dynamodb list-tables --region "$REGION" > "$OUT/ddb_tables.json"
jq -r '.TableNames[]' "$OUT/ddb_tables.json" 2>/dev/null | while read -r T; do
  aws dynamodb describe-table --table-name "$T" --region "$REGION" > "$OUT/ddb_${T}.json" || true
done

# ----- Secrets Manager (metadata only) -----
aws secretsmanager list-secrets --region "$REGION" > "$OUT/secrets_list.json"
# (optional) resource policies for posture evidence
jq -r '.SecretList[].ARN' "$OUT/secrets_list.json" 2>/dev/null | while read -r SARN; do
  aws secretsmanager get-resource-policy --secret-id "$SARN" --region "$REGION" > "$OUT/secret_policy_$(basename "$SARN").json" 2>/dev/null || true
done


---------

#!/usr/bin/env python3
import json, os, pathlib, re
from contextlib import suppress

# diagrams
from diagrams import Diagram, Cluster, Edge
from diagrams.aws.network import Route53, CloudFront, APIGateway, ELB, InternetGateway, PrivateSubnet, PublicSubnet
from diagrams.aws.compute import EKS, Lambda
from diagrams.aws.storage import S3
from diagrams.aws.database import RDS as RDSIcon, Aurora, Dynamodb
from diagrams.aws.security import SecretsManager

# Optional endpoint icon (not present in all versions)
try:
    from diagrams.aws.network import VPCEndpointInterface as VPCEndpointIcon
except Exception:
    VPCEndpointIcon = None

EXPORTS = pathlib.Path("as-is/exports")
OUTDIR  = pathlib.Path("diagrams")
OUTDIR.mkdir(exist_ok=True, parents=True)

def load_json(name):
    p = EXPORTS / name
    if not p.exists(): return None
    with open(p) as f: return json.load(f)

def first(lst, default=None):
    return lst[0] if lst else default

# ---------- Parsers ----------
def cf_summary():
    lst = (load_json("cf_distributions.json") or {}).get("DistributionList", {}).get("Items", [])
    if not lst: return None
    did = lst[0]["Id"]
    conf = (load_json(f"cf_{did}.json") or {}).get("Distribution", {}).get("DistributionConfig", {})
    aliases = conf.get("Aliases", {}).get("Items", []) or []
    return {"id": did, "aliases": aliases}

def apigw_v2_summary():
    items = (load_json("apigw_v2_apis.json") or {}).get("Items", [])
    if not items: return None
    api = items[0]; api_id = api.get("ApiId"); name = api.get("Name")
    integ = load_json(f"apigw_v2_{api_id}_integrations.json")
    nlb_links = []
    if integ and "Items" in integ:
        for it in integ["Items"]:
            uri = (it.get("IntegrationUri") or it.get("IntegrationUriArn") or "")
            if "elasticloadbalancing" in uri:
                nlb_links.append(uri)
    return {"id": api_id, "name": name, "nlb_links": nlb_links}

def nlb_summary():
    nlbs = load_json("elbv2_nlbs.json") or []
    if not nlbs:
        lbs = (load_json("elbv2_lbs.json") or {}).get("LoadBalancers", [])
        nlbs = [lb for lb in lbs if lb.get("Type") == "network"]
    if not nlbs: return None
    lb = nlbs[0]
    return {"name": lb.get("LoadBalancerName"), "dns": lb.get("DNSName")}

def eks_summary():
    names = (load_json("eks_clusters.json") or {}).get("clusters", [])
    if not names: return None
    return {"name": names[0]}

def detect_keycloak_name():
    # Look for 'keycloak' in k8s exports; fall back to generic
    for fname in EXPORTS.glob("eks_*_deploys.yaml"):
        try:
            txt = open(fname).read().lower()
            m = re.search(r'name:\s*keycloak[^\n]*', txt)
            if m: return "Keycloak (EKS)"
        except Exception:
            pass
    return "Keycloak"

def rds_pick():
    # Prefer clusters (Aurora) else instances; pick one matching 'ethos' if possible
    clusters = (load_json("rds_clusters.json") or {}).get("DBClusters", [])
    insts    = (load_json("rds_instances.json") or {}).get("DBInstances", [])
    pick = first([c for c in clusters if re.search(r'ethos|portal|keycloak', c.get("DBClusterIdentifier",""), re.I)], default=first(clusters))
    if pick:
        return {"type": "aurora", "id": pick.get("DBClusterIdentifier")}
    picki = first([i for i in insts if re.search(r'ethos|portal|keycloak', i.get("DBInstanceIdentifier",""), re.I)], default=first(insts))
    if picki:
        return {"type": "rds", "id": picki.get("DBInstanceIdentifier")}
    return None

def ddb_pick():
    names = (load_json("ddb_tables.json") or {}).get("TableNames", [])
    pick = first([n for n in names if re.search(r'ethos|portal', n, re.I)], default=first(names))
    return pick

def secrets_summary():
    secrets = (load_json("secrets_list.json") or {}).get("SecretList", [])
    # Only show 1-2 to keep diagram clean
    picks = [s for s in secrets if re.search(r'ethos|portal|eks|keycloak', (s.get("Name") or ""), re.I)]
    picks = picks or secrets[:1]
    return [s.get("Name") for s in picks[:2]]

# ---------- Diagrams ----------
GRAPH_STYLE = dict(
    rankdir="LR", splines="ortho", nodesep="1", ranksep="1.1", fontname="Helvetica", fontsize="12"
)

def draw_context_container():
    cf = cf_summary(); gw = apigw_v2_summary(); lb = nlb_summary(); eks = eks_summary()
    s3label = "S3 (static/state)"
    lambdas = (load_json("lambda_functions.json") or {}).get("Functions", [])
    lam = first([x["FunctionName"] for x in lambdas], "Lambda")

    rds = rds_pick()
    ddb = ddb_pick()
    secrets = secrets_summary()
    keycloak_name = detect_keycloak_name()

    with Diagram("Ethos Portal ‚Äì Context & Containers", filename=str(OUTDIR/"01_context_container"),
                 show=False, outformat="png", graph_attr=GRAPH_STYLE):
        # Edge & CDN
        user = Route53("User")
        r53  = Route53("Route 53")
        cf_i = CloudFront(f"CloudFront\n{(cf or {}).get('id','')}")
        gw_i = APIGateway(f"API Gateway\n{(gw or {}).get('id','')}")
        nlb_i = ELB(f"NLB\n{(lb or {}).get('name','')}")
        s3_i = S3(s3label)
        lam_i = Lambda(lam)

        # EKS cluster & app containers
        with Cluster(f"EKS {(eks or {}).get('name','')}"):
            ingress = EKS("Ingress")
            backend = EKS("Backend Service")
            keycloak = EKS(keycloak_name)

        # Data stores & secrets
        if rds and rds["type"] == "aurora":
            rds_i = Aurora(f"Aurora\n{rds['id']}")
        elif rds:
            rds_i = RDSIcon(f"RDS\n{rds['id']}")
        else:
            rds_i = RDSIcon("RDS")

        ddb_i = Dynamodb(ddb or "DynamoDB")
        secnodes = [SecretsManager(n) for n in (secrets or ["Secrets Manager"])]

        # Flow
        user >> r53 >> cf_i
        cf_i >> Edge(label="static") >> s3_i
        cf_i >> Edge(label="dynamic") >> gw_i >> Edge(label="443/TLS") >> nlb_i >> Edge(label="ingress") >> ingress >> backend

        # Auth & storage decisions
        backend << Edge(label="OIDC/SAML") >> keycloak
        backend >> Edge(label="authorized (FIS)") >> rds_i
        backend >> Edge(style="dashed", label="external/unauth") >> ddb_i

        # Events & secrets use
        backend >> Edge(label="read/write") >> s3_i
        s3_i >> Edge(label="event") >> lam_i
        for s in secnodes:
            backend >> Edge(style="dotted", label="IRSA") >> s

def draw_request_path():
    cf = cf_summary(); gw = apigw_v2_summary(); lb = nlb_summary(); eks = eks_summary()
    with Diagram("Ethos Portal ‚Äì API Request Path", filename=str(OUTDIR/"02_request_path"),
                 show=False, outformat="png", graph_attr=GRAPH_STYLE):
        user = Route53("Browser")
        r53  = Route53("Route 53")
        cf_i = CloudFront(f"CloudFront\n{(cf or {}).get('id','')}")
        gw_i = APIGateway(f"API GW\n{(gw or {}).get('id','')}")
        lb_i = ELB(f"NLB\n{(lb or {}).get('name','')}")
        with Cluster(f"EKS {(eks or {}).get('name','')}"):
            ingress = EKS("Ingress")
            svc     = EKS("Backend")
            keycloak = EKS("Keycloak")

        user >> Edge(label="HTTPS") >> r53 >> Edge(label="Alias") >> cf_i
        cf_i  >> Edge(label="HTTPS") >> gw_i >> Edge(label="TLS 443") >> lb_i >> Edge(label="TCP 443") >> ingress >> svc
        svc   << Edge(label="OIDC") >> keycloak

def draw_topology():
    with Diagram("Ethos Portal ‚Äì VPC Topology (schematic)", filename=str(OUTDIR/"03_topology"),
                 show=False, outformat="png", graph_attr=GRAPH_STYLE):
        igw = InternetGateway("IGW")
        with Cluster("VPC (Ethos Portal)"):
            with Cluster("AZ-a"):
                pub_a = PublicSubnet("public-a")
                pri_a = PrivateSubnet("private-a")
                nlb_a = ELB("NLB ENI-a")

            with Cluster("AZ-b"):
                pub_b = PublicSubnet("public-b")
                pri_b = PrivateSubnet("private-b")
                nlb_b = ELB("NLB ENI-b")

            s3e = VPCEndpointIcon("S3 Endpoint") if VPCEndpointIcon else ELB("S3 Endpoint")

        igw >> pub_a
        igw >> pub_b
        nlb_a - pri_a; nlb_b - pri_b
        s3e - pri_a;   s3e - pri_b

if __name__ == "__main__":
    draw_context_container()
    draw_request_path()
    draw_topology()
    print(f"Wrote diagrams to {OUTDIR.resolve()}")

















----------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------
# macOS (Homebrew) / Linux
brew install graphviz || sudo apt-get install -y graphviz
python3 -m venv venv && source venv/bin/activate
pip install diagrams pyyaml

---------

#!/usr/bin/env python3
import json, os, pathlib, re
from contextlib import suppress

# diagrams (mingrammer)
from diagrams import Diagram, Cluster, Edge
from diagrams.aws.network import Route53, CloudFront, APIGateway, ELB, VPC, InternetGateway, VPCEndpoint
from diagrams.aws.network import PrivateSubnet, PublicSubnet
from diagrams.aws.compute import EKS, Lambda
from diagrams.aws.storage import S3

EXPORTS = pathlib.Path("as-is/exports")
OUTDIR  = pathlib.Path("diagrams")
OUTDIR.mkdir(exist_ok=True, parents=True)

def load_json(path):
    p = EXPORTS / path
    if not p.exists(): return None
    with open(p) as f:
        return json.load(f)

def first(items, key=None, default=None):
    if not items: return default
    return items[0] if key is None else next((i for i in items if key(i)), default)

# ---------- Parsers (best-effort) ----------
def pick_vpc():
    vpcs = load_json("vpcs.json") or {}
    arr = vpcs.get("Vpcs", [])
    return first(arr, default={"VpcId": "vpc-unknown"})

def list_subnets(vpc_id):
    subnets = (load_json("subnets.json") or {}).get("Subnets", [])
    # Filter by VPC
    subs = [s for s in subnets if s.get("VpcId")==vpc_id]
    # Decide public vs private: MapPublicIpOnLaunch is a decent heuristic
    pubs = [s for s in subs if s.get("MapPublicIpOnLaunch")]
    pris = [s for s in subs if not s.get("MapPublicIpOnLaunch")]
    # group by AZ (up to 2 for aesthetics)
    def by_az(xs): 
        d={}
        for s in xs:
            d.setdefault(s["AvailabilityZone"], []).append(s)
        return d
    return by_az(pubs), by_az(pris)

def cf_summary():
    dist = load_json("cf_distributions.json") or {}
    items = dist.get("DistributionList", {}).get("Items", [])
    if not items: 
        return None
    did = items[0]["Id"]
    full = load_json(f"cf_{did}.json") or {}
    conf = full.get("Distribution", {}).get("DistributionConfig", {})
    origins = [o.get("DomainName") for o in conf.get("Origins", {}).get("Items", [])]
    aliases = conf.get("Aliases", {}).get("Items", [])
    return {"id": did, "origins": origins, "aliases": aliases}

def apigw_v2_summary():
    apis = load_json("apigw_v2_apis.json") or {}
    items = apis.get("Items", [])
    if not items: return None
    api = items[0]
    api_id = api.get("ApiId"); name = api.get("Name")
    # Try to find integration to ELB/NLB
    integ = load_json(f"apigw_v2_{api_id}_integrations.json")
    tgts = []
    if integ and "Items" in integ:
        for it in integ["Items"]:
            uri = (it.get("IntegrationUri") or it.get("IntegrationUriArn") or "")
            if "elasticloadbalancing" in uri:
                tgts.append(uri)
    return {"id": api_id, "name": name, "nlb_links": tgts}

def nlb_summary():
    nlbs = load_json("elbv2_nlbs.json") or []
    if not nlbs: 
        # fallback: filter from full LB list
        lbs = (load_json("elbv2_lbs.json") or {}).get("LoadBalancers", [])
        nlbs = [lb for lb in lbs if lb.get("Type") == "network"]
    if not nlbs: 
        return None
    lb = nlbs[0]
    return {
        "arn": lb.get("LoadBalancerArn"),
        "name": lb.get("LoadBalancerName"),
        "dns": lb.get("DNSName"),
        "vpc": lb.get("VpcId"),
        "scheme": lb.get("Scheme"),
        "azs": [z.get("ZoneName") for z in lb.get("AvailabilityZones", [])],
    }

def eks_summary():
    cl = load_json("eks_clusters.json") or {}
    names = cl.get("clusters", [])
    if not names: return None
    name = names[0]
    desc = load_json(f"eks_{name}.json") or {}
    ver  = desc.get("cluster", {}).get("version")
    return {"name": name, "version": ver}

def s3_brief():
    b = load_json("s3_buckets.json") or {}
    names = [x["Name"] for x in b.get("Buckets", [])]
    return names[:3]  # keep the diagram clean

def lambda_brief():
    f = load_json("lambda_functions.json") or {}
    items = f.get("Functions", [])
    return [x.get("FunctionName") for x in items[:2]]  # max two icons on diagram

# ---------- Diagram 1: Context/Container ----------
def draw_context_container():
    cf = cf_summary()
    apigw = apigw_v2_summary()
    nlb = nlb_summary()
    eks = eks_summary()
    s3s = s3_brief()
    lambdas = lambda_brief()

    title = "Ethos Portal ‚Äì Context & Containers"
    filename = str(OUTDIR / "01_context_container")

    with Diagram(title, filename=filename, show=False, outformat="png"):
        user = Route53("User via DNS")  # Just to keep icon variety; we‚Äôll add real R53 next line
        r53  = Route53("Route 53")
        cf_i = CloudFront(f"CloudFront\n{cf['id'] if cf else ''}")
        apigw_i = APIGateway(f"API Gateway\n{apigw['id'] if apigw else ''}")
        nlb_i = ELB(f"NLB\n{(nlb or {}).get('name','')}")
        eks_i = EKS(f"EKS\n{(eks or {}).get('name','')}")
        s3_i  = S3("S3 (static/state)")
        # Keep Lambdas compact
        l_nodes = [Lambda(n) for n in lambdas] if lambdas else [Lambda("Lambda")]

        user >> r53 >> cf_i
        cf_i >> Edge(label="static") >> s3_i
        cf_i >> Edge(label="dynamic") >> apigw_i >> Edge(label="443/TLS") >> nlb_i >> Edge(label="ingress") >> eks_i
        # side paths
        eks_i >> Edge(label="read/write") >> s3_i
        s3_i >> Edge(label="event") >> l_nodes[0]

# ---------- Diagram 2: Request Path (icons w/ arrows) ----------
def draw_request_path():
    cf = cf_summary(); apigw = apigw_v2_summary(); nlb = nlb_summary(); eks = eks_summary()

    title = "Ethos Portal ‚Äì API Request Path"
    filename = str(OUTDIR / "02_request_path")

    with Diagram(title, filename=filename, show=False, outformat="png"):
        user = Route53("Browser")
        r53  = Route53("Route 53")
        cf_i = CloudFront(f"CloudFront\n{cf['id'] if cf else ''}")
        gw   = APIGateway(f"API GW\n{(apigw or {}).get('id','')}")
        lb   = ELB(f"NLB\n{(nlb or {}).get('name','')}")
        ing  = EKS(f"Ingress (EKS)\n{(eks or {}).get('name','')}")

        user >> Edge(label="HTTPS") >> r53 >> Edge(label="Alias") >> cf_i
        cf_i >> Edge(label="HTTPS") >> gw >> Edge(label="TLS 443") >> lb >> Edge(label="TCP 443") >> ing

# ---------- Diagram 3: Network Topology ----------
def draw_topology():
    vpc = pick_vpc().get("VpcId")
    pubs_by_az, pris_by_az = list_subnets(vpc)
    nlb = nlb_summary()

    title = f"Ethos Portal ‚Äì VPC Topology ({vpc})"
    filename = str(OUTDIR / "03_topology")

    with Diagram(title, filename=filename, show=False, outformat="png"):
        igw = InternetGateway("IGW")
        with Cluster(f"VPC {vpc}"):
            # draw at most two AZs to keep legible
            az_names = sorted(set(list(pubs_by_az.keys()) + list(pris_by_az.keys())))[:2]
            az_clusters = []
            for az in az_names:
                with Cluster(f"{az}"):
                    # Public subnets
                    pub_nodes = []
                    for s in pubs_by_az.get(az, [])[:1]:
                        pub_nodes.append(PublicSubnet(f"{s['SubnetId']}"))
                    # Private subnets
                    pri_nodes = []
                    for s in pris_by_az.get(az, [])[:1]:
                        pri_nodes.append(PrivateSubnet(f"{s['SubnetId']}"))
                    # Drop an NLB ENI per AZ (symbolically)
                    nlb_node = ELB(f"NLB ENI\n{(nlb or {}).get('name','')}")
                    az_clusters.append((pub_nodes, pri_nodes, nlb_node))

            # S3 Gateway/Interface Endpoint (symbolic)
            s3e = VPCEndpoint("S3 VPC Endpoint")

        # Connect IGW to first public subnets (if any)
        with suppress(Exception):
            if az_clusters and az_clusters[0][0]:
                igw >> az_clusters[0][0][0]
            if len(az_clusters) > 1 and az_clusters[1][0]:
                igw >> az_clusters[1][0][0]

        # NLB sits in private subnets here (common pattern with Ingress)
        for (_, pri_nodes, nlb_node) in az_clusters:
            if pri_nodes:
                nlb_node - pri_nodes[0]
        # VPC endpoint links
        for (_, pri_nodes, _) in az_clusters:
            if pri_nodes:
                s3e - pri_nodes[0]

# ---------- Run ----------
if __name__ == "__main__":
    draw_context_container()
    draw_request_path()
    draw_topology()
    print(f"Diagrams written to {OUTDIR.resolve()}")




----

python generate_ethos_diagrams.py
# -> diagrams/01_context_container.png
# -> diagrams/02_request_path.png
# -> diagrams/03_topology.png





----------------
#!/usr/bin/env bash
set -euo pipefail

# ===== config =====
OUT="as-is/exports"
REGION="${AWS_REGION:-$(aws configure get region || echo eu-west-1)}"
mkdir -p "$OUT"

echo "Exporting Ethos Portal as-is to $OUT (region: $REGION)"

# ----- Identity & scope -----
aws sts get-caller-identity > "$OUT/whoami.json"
aws ec2 describe-vpcs --region "$REGION" > "$OUT/vpcs.json"
aws ec2 describe-subnets --region "$REGION" > "$OUT/subnets.json"
aws ec2 describe-vpc-endpoints --region "$REGION" > "$OUT/vpce.json"
aws ec2 describe-security-groups --region "$REGION" > "$OUT/sgs.json"
aws ec2 describe-route-tables --region "$REGION" > "$OUT/routes.json"

# ----- Edge + DNS + CDN -----
aws route53 list-hosted-zones > "$OUT/r53_zones.json"
# loop zones to fetch records
jq -r '.HostedZones[].Id' "$OUT/r53_zones.json" | sed 's#/hostedzone/##' | while read -r HZID; do
  aws route53 list-resource-record-sets --hosted-zone-id "$HZID" > "$OUT/r53_records_${HZID}.json"
done

aws cloudfront list-distributions > "$OUT/cf_distributions.json"
# fetch full config per distribution (origins/behaviors/TLS)
jq -r '.DistributionList.Items[].Id' "$OUT/cf_distributions.json" 2>/dev/null | while read -r DID; do
  aws cloudfront get-distribution --id "$DID" > "$OUT/cf_${DID}.json"
done

# ----- API layer (v2 + v1) -----
aws apigatewayv2 get-apis --region "$REGION" > "$OUT/apigw_v2_apis.json"
# Optional: domain names (custom domains)
aws apigatewayv2 get-domain-names --region "$REGION" > "$OUT/apigw_v2_domains.json"
# loop v2 apis for stages/integrations/routes/authorizers
jq -r '.Items[].ApiId' "$OUT/apigw_v2_apis.json" 2>/dev/null | while read -r API; do
  aws apigatewayv2 get-stages --api-id "$API" --region "$REGION" > "$OUT/apigw_v2_${API}_stages.json"
  aws apigatewayv2 get-integrations --api-id "$API" --region "$REGION" > "$OUT/apigw_v2_${API}_integrations.json"
  aws apigatewayv2 get-routes --api-id "$API" --region "$REGION" > "$OUT/apigw_v2_${API}_routes.json"
  aws apigatewayv2 get-authorizers --api-id "$API" --region "$REGION" > "$OUT/apigw_v2_${API}_authorizers.json"
done

aws apigateway get-rest-apis --region "$REGION" > "$OUT/apigw_v1_apis.json"
aws apigateway get-domain-names --region "$REGION" > "$OUT/apigw_v1_domains.json"
# lightweight per-REST API (full per-method integrations are very heavy; skip for one-time doc)
jq -r '.items[].id' "$OUT/apigw_v1_apis.json" 2>/dev/null | while read -r RID; do
  aws apigateway get-stages --rest-api-id "$RID" --region "$REGION" > "$OUT/apigw_v1_${RID}_stages.json"
  aws apigateway get-resources --rest-api-id "$RID" --region "$REGION" > "$OUT/apigw_v1_${RID}_resources.json"
done

# ----- Load balancer (focus on NLBs) -----
aws elbv2 describe-load-balancers --region "$REGION" > "$OUT/elbv2_lbs.json"
# Filter NLBs for convenience (still keep the full file above)
aws elbv2 describe-load-balancers --region "$REGION" \
  --query 'LoadBalancers[?Type==`network`]' > "$OUT/elbv2_nlbs.json"

# loop NLB ARNs for listeners, attributes, and target groups
jq -r '.[].LoadBalancerArn' "$OUT/elbv2_nlbs.json" 2>/dev/null | while read -r LARN; do
  LID=$(basename "$LARN")
  aws elbv2 describe-listeners --load-balancer-arn "$LARN" --region "$REGION" > "$OUT/elbv2_${LID}_listeners.json"
  aws elbv2 describe-load-balancer-attributes --load-balancer-arn "$LARN" --region "$REGION" > "$OUT/elbv2_${LID}_attrs.json"
  # target groups for this LB
  aws elbv2 describe-target-groups --load-balancer-arn "$LARN" --region "$REGION" > "$OUT/elbv2_${LID}_tgs.json"
  jq -r '.TargetGroups[].TargetGroupArn' "$OUT/elbv2_${LID}_tgs.json" 2>/dev/null | while read -r TGARN; do
    TID=$(basename "$TGARN")
    aws elbv2 describe-target-health --target-group-arn "$TGARN" --region "$REGION" > "$OUT/elbv2_tg_${TID}_health.json"
    aws elbv2 describe-target-group-attributes --target-group-arn "$TGARN" --region "$REGION" > "$OUT/elbv2_tg_${TID}_attrs.json"
  done
done

# ----- EKS control plane & k8s ingress/services -----
aws eks list-clusters --region "$REGION" > "$OUT/eks_clusters.json"
jq -r '.clusters[]' "$OUT/eks_clusters.json" 2>/dev/null | while read -r CL; do
  aws eks describe-cluster --name "$CL" --region "$REGION" > "$OUT/eks_${CL}.json"
  # kubeconfig + cluster scrape (requires your IAM/kubectl can auth to EKS)
  aws eks update-kubeconfig --name "$CL" --region "$REGION" >/dev/null
  kubectl get ingressclass -A -o yaml > "$OUT/eks_${CL}_ingressclass.yaml" || true
  kubectl get ingress -A -o yaml       > "$OUT/eks_${CL}_ingress.yaml" || true
  kubectl get svc -A -o yaml           > "$OUT/eks_${CL}_services.yaml" || true
  kubectl get deploy -A -o yaml        > "$OUT/eks_${CL}_deploys.yaml" || true
done

# ----- Lambda & S3 (PCI-relevant) -----
aws lambda list-functions --region "$REGION" > "$OUT/lambda_functions.json"

aws s3api list-buckets > "$OUT/s3_buckets.json"
jq -r '.Buckets[].Name' "$OUT/s3_buckets.json" | while read -r B; do
  aws s3api get-bucket-encryption --bucket "$B" > "$OUT/s3_${B}_enc.json" 2>/dev/null || true
  aws s3api get-public-access-block --bucket "$B" > "$OUT/s3_${B}_pab.json" 2>/dev/null || true
  aws s3api get-bucket-versioning --bucket "$B" > "$OUT/s3_${B}_ver.json" 2>/dev/null || true
  aws s3api get-bucket-policy-status --bucket "$B" > "$OUT/s3_${B}_policy_status.json" 2>/dev/null || true
  aws s3api get-bucket-ownership-controls --bucket "$B" > "$OUT/s3_${B}_ownership.json" 2>/dev/null || true
done

echo "Done. JSON exports in $OUT"
