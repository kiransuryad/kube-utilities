Context:

Our EKS cluster, initially provisioned on July 3, 2019, has not received upgrades or patches since its deployment. Over the years, the infrastructure, including the EDP Jenkins environment, has required significant manual interventions due to breakages and the lack of automated maintenance. AWS has recently discontinued support for EKS version 1.21 and automatically migrated our cluster to version 1.22. This forced upgrade has introduced potential risks, particularly around application stability upon restart.

Given these developments, it is critical that we perform a planned upgrade to EKS version 1.23. This upgrade is necessary to ensure the continued stability, security, and supportability of our infrastructure, especially for those stakeholders who still rely on this cluster during our broader migration efforts to new clusters.

Current State:

Cluster: EKS Cluster
Region: us-west-2
Current EKS Version: 1.22 (after AWS's automatic upgrade)
Upgrade Target: EKS Version 1.23
Risks and Challenges:

As with any major upgrade, there are several risks and challenges that we must anticipate and manage carefully:

Incompatibility Issues:
Potential Impact: Upgrading the Kubernetes version can introduce breaking changes that may be incompatible with existing applications or configurations.
Mitigation: This could necessitate additional adjustments or patches post-upgrade to ensure all applications function correctly. We plan to conduct thorough testing in a staging environment to identify and resolve these issues before the upgrade.
Downtime:
Potential Impact: The upgrade process could result in significant downtime, impacting all applications running within this cluster. Even with careful planning, some service interruptions are expected during the upgrade.
Mitigation: We will schedule the upgrade during off-peak hours to minimize disruption and will communicate the expected downtime well in advance to all stakeholders.
No Rollback Option:
Potential Impact: Once initiated, the upgrade process cannot be paused, canceled, or rolled back. Any issues encountered during the upgrade could result in prolonged outages or, in a worst-case scenario, permanent loss of applications.
Mitigation: We will have a dedicated team on standby to address any issues that arise immediately during the upgrade, minimizing the risk of extended outages.
Application Failures on Restart:
Potential Impact: Some applications may fail to restart correctly due to compatibility issues with the newer Kubernetes version. This could lead to service disruptions and potentially impact business operations.
Mitigation: By identifying and addressing these issues in a controlled staging environment before the upgrade, we aim to minimize this risk. Additionally, we will have rollback plans for individual applications where feasible, though a full rollback of the cluster is not possible.
Resource Constraints:
Potential Impact: The upgrade may place additional load on cluster resources, potentially leading to resource contention or performance degradation during the process.
Mitigation: We will monitor resource utilization closely throughout the upgrade and be prepared to adjust resource allocations as needed to ensure stability.
Data Integrity Risks:
Potential Impact: During the upgrade process, there is a small risk of data corruption or loss, particularly if any applications are not fully compatible with the new Kubernetes version.
Mitigation: We will perform comprehensive backups of all data and the cluster state prior to the upgrade, ensuring that we can restore any lost data if necessary.
Applications Impacted:

The following applications currently running in the cluster will be affected by the upgrade process:

Application Name	Status	Age
airflow-test	Active	4y21d
amazon-cloudwatch	Active	3y154d
awx-poc	Active	4y7d
cattle-prometheus	Active	4y74d
cert-manager	Active	4y224d
data-exporter	Active	4y224d
data-tools	Active	4y93d
dev-csi	Active	4y450d
ingress-nginx	Active	3y47d
jenkins-test	Active	4y219d
jmeter	Active	3y130d
kube-public	Active	5y53d
kube-system	Active	5y53d
monitoring	Active	4y81d
redshift-exporter	Active	4y292d
tekton	Active	4y31d
wc-test	Active	4y21d
Need for This Upgrade and Why Now:

AWS's recent forced upgrade to EKS version 1.22 has highlighted the urgent need to maintain an actively supported and stable environment. Some applications may fail on restart due to compatibility issues with the newer Kubernetes version. If we do not proactively upgrade to EKS version 1.23, we risk further instability, security vulnerabilities, and operational challenges.

Additionally, this upgrade ensures that we continue to provide a stable and supported environment for stakeholders who still depend on this cluster, allowing them to transition smoothly to alternative clusters at their own pace.

Why Not Retire the EKS Cluster Instead?

While retiring the EKS cluster and migrating services to a different platform is part of our long-term plan, it is not feasible to do so immediately due to ongoing dependencies by various teams and applications. The primary goal of this upgrade is to maintain a supported and stable environment for current users, ensuring that their operations are not disrupted during the transition to new clusters. This approach gives teams the necessary time to plan and execute their migrations effectively.

Action Plan:

Pre-Upgrade Preparations:
Backup: Complete a full backup of the current cluster state and all application data before starting the upgrade.
Testing: Perform thorough compatibility testing of all applications with EKS version 1.23 in a staging environment to identify and address potential issues.
Communication: Notify all stakeholders and end-users about the planned downtime and its potential impact on services.
Upgrade Execution:
Schedule the upgrade during off-peak hours to minimize disruption.
Monitor the upgrade process closely, with team members on standby to resolve any issues immediately.
Post-Upgrade Validation:
Verify the stability and functionality of all applications post-upgrade.
Conduct a detailed assessment to ensure that no data loss or application issues occurred during the upgrade.
Provide a comprehensive post-upgrade report to all stakeholders.
Anticipated Outcomes:

A successful upgrade to EKS version 1.23 will bring the cluster into a fully supported state, ensuring the continued receipt of critical patches and support from AWS. This upgrade will also lay the groundwork for future optimizations and provide a stable environment for users as they plan their migrations to new clusters.

What We Expect from Stakeholders:

Review Your Usage:
We ask all stakeholders to review their current usage of this EKS cluster. If your team requires additional time to migrate to a different environment, please notify us immediately. We are open to postponing the upgrade if necessary to accommodate your needs.
Provide Approval and Feedback:
We need your approval to proceed with the upgrade on the proposed schedule. If there are any concerns or if the timing conflicts with your teamâ€™s operations, please let us know as soon as possible so we can make necessary adjustments.
Request for Approval:

To ensure continued support and minimize risks, we request your approval to proceed with the upgrade to EKS version 1.23. Your prompt feedback and approval are crucial for scheduling and executing this upgrade with minimal impact on operations.




---
Subject: Urgent: EKS Cluster Upgrade Required

Context:

The EKS cluster in question has not been upgraded or patched since its initial provisioning on July 3, 2019. The infrastructure, including the EDP Jenkins environment, has experienced significant manual interventions over the years due to breakages and the lack of automated maintenance. Importantly, the current EKS version (1.21) is approaching its end of support from AWS, which will result in the cessation of updates and support services from AWS. Consequently, we must perform an upgrade to EKS version 1.23 to ensure continued stability, security, and supportability of our infrastructure.

Current State:

Cluster: EKS Cluster
Region: us-west-2
Current EKS Version: 1.21
Upgrade Target: EKS Version 1.23
Risks and Challenges:

Incompatibility Issues:
Upgrading the Kubernetes version can introduce breaking changes that may not be compatible with existing applications or configurations. This could potentially require additional adjustments or patches to the current environment post-upgrade.
Downtime:
The upgrade process could lead to significant downtime, impacting all applications running within this cluster. While we will make efforts to minimize this, some level of service interruption is expected during the upgrade.
No Rollback Option:
Once the upgrade is initiated, it cannot be paused, canceled, or rolled back. This irreversible process means that any issues arising during the upgrade could potentially result in a prolonged outage or even permanent loss of applications if not properly managed.
Applications Impacted:

The following applications currently running in the cluster will be affected by the upgrade process:

Application Name	Status	Age
airflow-test	Active	4y21d
amazon-cloudwatch	Active	3y154d
awx-poc	Active	4y7d
cattle-prometheus	Active	4y74d
cert-manager	Active	4y224d
data-exporter	Active	4y224d
data-tools	Active	4y93d
dev-csi	Active	4y450d
ingress-nginx	Active	3y47d
jenkins-test	Active	4y219d
jmeter	Active	3y130d
kube-public	Active	5y53d
kube-system	Active	5y53d
monitoring	Active	4y81d
redshift-exporter	Active	4y292d
tekton	Active	4y31d
wc-test	Active	4y21d
Action Plan:

Pre-Upgrade Preparations:
Backup: Ensure that a full backup of the current cluster state and all application data is completed before starting the upgrade.
Testing: Conduct a thorough assessment of application compatibility with EKS version 1.23 in a staging environment to identify potential issues in advance.
Communication: Notify all stakeholders and end-users about the expected downtime and its impact on services.
Upgrade Execution:
Proceed with the upgrade during off-peak hours to minimize disruption.
Monitor the upgrade process closely, with team members on standby to address any issues immediately.
Post-Upgrade Validation:
Verify the stability and functionality of all applications post-upgrade.
Perform a detailed assessment to ensure that no data loss or application issues occurred during the upgrade.
Provide a detailed post-upgrade report to all stakeholders.
Anticipated Outcomes:

The successful upgrade will bring the cluster into a supported state, ensuring continued receipt of critical patches and support from AWS. It will also lay the groundwork for future upgrades and optimizations.

Request for Approval:

We request approval to proceed with the upgrade process on the proposed schedule, given the outlined risks and the mitigations in place. Please review and confirm.






-----------
aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names <AutoScalingGroupName> --query "AutoScalingGroups[0].Instances[*].{InstanceID:InstanceId, Lifecycle:LifecycleState, InstanceType:InstanceType, LaunchTemplateConfiguration:LaunchConfigurationName, Version:LaunchTemplateVersion, AvailabilityZone:AvailabilityZone, HealthStatus:HealthStatus}" --output table


aws autoscaling describe-auto-scaling-instances --query "AutoScalingInstances[*].{InstanceID:InstanceId, Lifecycle:LifecycleState, InstanceType:InstanceType, LaunchTemplateConfiguration:LaunchConfigurationName, Version:LaunchTemplateVersion, AvailabilityZone:AvailabilityZone, HealthStatus:HealthStatus}" --output table



---
Good evening all,

I hope this message finds you well.

Could I kindly ask for your assistance in approving the following two Change Requests scheduled for tomorrow?

CH24001646853
CH24001658859
These CRs pertain to the renewal of AWS RDS Certificates, and they were recently transitioned to the assessment stage.

Your prompt attention to this would be greatly appreciated.

Thank you so much for your support.


-----
Good Morning All,

@Swapnil Rajgure, thank you for confirming the change state. I also appreciate your efforts in accepting the task and moving it to the accepted state.

We plan to proceed with the change this evening and will notify you once testing is complete.

Change Window:

Start: 2024-08-20, 16:00:00 BST
End: 2024-08-20, 18:30:00 BST




-----
Context:

Hello All,

Today is the planned date for the CR: CH24001658959, which involves renewing the AWS RDS certificates. The initial plan was to proceed with the renewal and have your team assist with validation to ensure there were no impacts on other services.

However, the Change Request (CR) was unexpectedly moved back to the "Assess" state. Iâ€™m not entirely sure of the reasons for this but plan to discuss it with the ServiceNow (SNOW) team later today.

Request:

To keep things on track, I have rescheduled the change for tomorrow at the same time. I kindly request your support in accepting and assisting with this task from your end.

Thank you in advance for your cooperation.


-----
Hi Chris,

Thank you for your email. I chose "None" as the correct answer regarding the type of service that could be impacted by this change due to the specific nature of the update being carried out.

This change involves updating the RDS (Relational Database Service) certificates, which is a back-end infrastructure update. It's focused solely on the RDS component and doesn't impact any application logic or processing services.

Given this, I believe "None" is the appropriate classification, as thereâ€™s no anticipated risk or impact on any services within the scope of this update. That said, I'm absolutely open to any suggestions or further discussions if you think there might be areas we should revisit.

Please feel free to reach out if you need any further clarification or if there's anything else you'd like to discuss.


----
I hope this message finds you well. We are in the process of the mandatory AWS RDS Certificate renewal and have successfully updated the certificates for our non-production and development environments earlier this week.

We are now preparing to proceed with the renewal of the production RDS certificates and kindly request your approval to move forward. Additionally, we would appreciate your insights on the following queries to ensure seamless coordination and compliance:

Has the client been informed of the upcoming renewal?
Could you provide a list of the Lines of Business (LOBs) that will be affected by this update?
Your prompt response will help us maintain the integrity and security of our services without disruption.

Thank you for your cooperation and support.


----
aws ec2 describe-instances --instance-id i-0123456789abcdef0 --query 'Reservations[*].Instances[*].IamInstanceProfile' --output json

aws ec2 disassociate-iam-instance-profile --association-id <association-id>


# Disassociate the old IAM role (if needed)
aws ec2 disassociate-iam-instance-profile --association-id <association-id>

# Associate the new IAM role
aws ec2 associate-iam-instance-profile --instance-id i-0123456789abcdef0 --iam-instance-profile Name=NewInstanceProfileName




----
Team,

We are undertaking a crucial upgrade of the EKS clusters within our EDP management AWS Account to enhance system integrity and security.

AWS has upgraded to version 1.22, which has introduced potential disruptions. Although some end users are currently utilizing these applications without reported issues, we must remain vigilant. To preemptively address any disruptions and ensure a seamless transition, we are planning a proactive migration to version 1.29. This upgrade is essential for maintaining compatibility with upcoming AWS updates and bolstering our overall system security.

While we anticipate that some applications may experience compatibility issues post-upgrade, we want to emphasize that resolving these issues may not always be within our control. The upgrade may result in certain applications breaking due to dependencies or compatibility problems. We will monitor the situation closely and address any issues that arise to the best of our abilities, but there may be limitations in our capacity to resolve all potential problems.





-----
We are initiating an upgrade of the EKS clusters within our EDP management AWS Account. This is a critical step to maintain the integrity and security of our infrastructure.

AWS has already upgraded the control plane to version 1.22, which has led to some potential application disruptions. To mitigate these issues and address security vulnerabilities, we are advancing to version 1.29. This upgrade is necessary to ensure compatibility with future AWS updates and to strengthen our overall system security.

During this upgrade, we will focus primarily on Jenkins and Keycloak. Our team is prepared to handle any issues that arise in these applications as part of the EKS upgrade process.

We are taking a proactive approach to resolve any disruptions and ensure a smooth transition. Your understanding and cooperation are appreciated as we work through these upgrades.


----
Thank you so much for your confirmation!

Could you please provide the details for our Prod deployment Readiness Checklist? I have prefilled some of the information and would appreciate your assistance in confirming and suggesting any necessary adjustments, especially regarding the impact on LOBs and their clients.

Your support is greatly appreciated.


---
Good afternoon Lokesh,

I hope this message finds you well.

As mentioned in the previous email, we have successfully updated the below RDS DB instance certificates.

To ensure seamless operations, please verify that your applications in the NonProd ARIC Accounts (DEVL-ARIC) are functioning as expected with the new certificates. If you could provide confirmation by this Friday, that would be greatly appreciated. In the absence of any issues, we will proceed with the assumption that everything is operating smoothly.

Additionally, we have scheduled the Prod Account (PPRD-ARIC) RDS Certificate renewals for early next week. Please note that if we do not hear back from you either positively or negatively by this Thursday, we will assume that as positive feedback and proceed with our changes from Monday.

Thank you for your cooperation and understanding. We are committed to providing you with the best support and ensuring minimal disruption.

Best regards,


---
Hi Team,

I am pleased to inform you that the SSL certificate upgrade for our AWS RDS instances has been successfully completed as of July 26, 2024. This upgrade ensures continued security and compliance for our database interactions.

What You Need to Do:

Please conduct a thorough test of your applications using the current RDS instances to confirm that they operate seamlessly with the updated certificates. This is crucial for ensuring that our applications maintain their functionality and performance standards.




----
Good morning all,

Thank you, Courtney and Casey, for your valuable feedback. Regarding your question about updating Keycloak to a newer version, we face a challenge due to our current deployment on the Keycloak 18.0.3-legacy version, which requires migration to a newer version rather than a straightforward update.

Rajesh, I would appreciate your insights and recommendations on migrating from Keycloak 18.0.3-legacy to the latest version, 25.0.2. Your expertise will be crucial in outlining our next steps.

Manuel, in light of our discussions, could you please share your thoughts on the feasibility of the Keycloak upgrade or suggest any alternative solutions that might better suit our needs?

Thanks for your cooperation.

Best regards,
Kiran

---
aws ec2 describe-instances \
    --query "Reservations[*].Instances[*].[InstanceId, Tags[?Key=='Name'].Value | [0], InstanceType]" \
    --filters "Name=instance-state-name,Values=running" \
    --output text



----
build_prompt() {
  # Load current AWS profile
  local current_profile=$AWS_PROFILE

  # Read the configuration file to get the account number, Kubernetes context, and environment type
  local config_line=$(awk -v profile="$current_profile" -F, '$1 == profile {print $0}' /path/to/aws_k8s_config.txt)

  local aws_account=$(echo $config_line | cut -d ',' -f2)
  local k8s_context=$(echo $config_line | cut -d ',' -f3)
  local env_type=$(echo $config_line | cut -d ',' -f4)

  # Your existing prompt components
  prompt_status
  prompt_context
  prompt_dir
  prompt_git
  prompt_bzr
  prompt_hg
  
  # Set AWS details in the prompt, change visual style based on environment type
  if [[ -n "$aws_account" ]]; then
    case "$env_type" in
      prod)
        prompt_segment red yellow "ðŸ›‘ AWS: ${current_profile} (${aws_account})"
        ;;
      nonprod)
        prompt_segment green black "ðŸŒ± AWS: ${current_profile} (${aws_account})"
        ;;
    esac
  fi

  # Set Kubernetes context in the prompt, if applicable
  if [[ -n "$k8s_context" ]]; then
    PS1+="$(kube_ps1) "
  else
    PS1+="[K8s context not applicable] "
  fi

  prompt_end
}

profile1,123456789012,cluster1,prod
profile2,234567890123,cluster2,nonprod
profile3,345678901234,cluster3,nonprod







--------
build_prompt() {
  # Your existing prompt components
  prompt_status
  prompt_context
  prompt_dir
  prompt_git
  prompt_bzr
  prompt_hg
  prompt_aws

  # Fetch AWS account number
  local aws_account=$(aws sts get-caller-identity --output text --query 'Account' 2>/dev/null)
  
  # Decide whether to show kube-ps1 based on AWS account
  case "$aws_account" in
    "123456789012")  # Suppose this is the account linked to your K8s context
      PS1+="$(kube_ps1) "
      ;;
    *)
      # Optionally, you can display nothing or some placeholder
      PS1+="[K8s context not applicable] "
      ;;
  esac

  prompt_end
}




# AWS Profile and Account:
# - display current AWS_PROFILE name and AWS account number
# - displays yellow on red if profile name contains 'production' or ends in '-prod'
# - displays black on green otherwise
prompt_aws() {
  [[ -z "$AWS_PROFILE" || "$SHOW_AWS_PROMPT" = false ]] && return

  # Fetch AWS account number
  local aws_account=$(aws sts get-caller-identity --output text --query 'Account' 2>/dev/null)

  # Check if aws_account is successfully fetched, otherwise show only AWS_PROFILE
  local aws_detail
  if [[ -n "$aws_account" ]]; then
    aws_detail="AWS: ${AWS_PROFILE} (${aws_account})"
  else
    aws_detail="AWS: ${AWS_PROFILE}"
  fi

  # Determine color scheme based on profile name
  case "$AWS_PROFILE" in
    *-prod|*production*) prompt_segment red yellow  "${aws_detail:gs/%/%%}" ;;
    *) prompt_segment green black "${aws_detail:gs/%/%%}" ;;
  esac
}



_kube_ps1_get_context_ns() {
  # Set the command time
  if [[ "$(_kube_ps1_shell_type)" == "bash" ]]; then
    if ((BASH_VERSINFO[0] >= 4 && BASH_VERSINFO[1] >= 2)); then
      _KUBE_PS1_LAST_TIME=$(printf '%(%s)T')
    else
      _KUBE_PS1_LAST_TIME=$(date +%s)
    fi
  elif [[ "$(_kube_ps1_shell_type)" == "zsh" ]]; then
    _KUBE_PS1_LAST_TIME=$EPOCHREALTIME
  fi

  KUBE_PS1_CONTEXT="${KUBE_PS1_CONTEXT:-N/A}"
  KUBE_PS1_NAMESPACE="${KUBE_PS1_NAMESPACE:-N/A}"
  KUBE_PS1_AWS_PROFILE="${KUBE_PS1_AWS_PROFILE:-N/A}"

  # Cache which cfgfiles we can read in case they change.
  local conf
  _KUBE_PS1_CFGFILES_READ_CACHE=
  for conf in $(_kube_ps1_split_config : "${KUBECONFIG:-${HOME}/.kube/config}"); do
    [[ -r $conf ]] && _KUBE_PS1_CFGFILES_READ_CACHE+=":$conf"
  done

  _kube_ps1_get_context
  _kube_ps1_get_ns
  _kube_ps1_get_aws_profile
}

_kube_ps1_get_aws_profile() {
  KUBE_PS1_AWS_PROFILE=$(kubectl config view --minify --output "jsonpath={.contexts[?(@.name=='$(kubectl config current-context)')].context.aws-profile}")
  if [[ -n "$KUBE_PS1_AWS_PROFILE" ]]; then
    export AWS_PROFILE=$KUBE_PS1_AWS_PROFILE
  fi
}

_kube_ps1_get_aws_profile() {
  KUBE_PS1_AWS_PROFILE=$(kubectl config view --minify --output "jsonpath={.users[?(@.name=='$(kubectl config current-context)')].user.exec.env[?(@.name=='AWS_PROFILE')].value}")
  if [[ -n "$KUBE_PS1_AWS_PROFILE" ]]; then
    export AWS_PROFILE=$KUBE_PS1_AWS_PROFILE
  fi
}
