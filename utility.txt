As part of the ongoing efforts to decommission AWS account WUAT-ARIC, we are executing Change Request CH24001690506, which involves pausing services prior to full decommissioning. This is a critical step to ensure smooth deactivation with minimal disruption to ongoing operations.

Key Activities:
Pausing AWS Services:
We have identified multiple services and resources across the WUAT-ARIC account. These include EC2 instances, EKS clusters, and AWS IAM Access Keys, among others.
As part of this CR, the following services have been paused or are scheduled for pausing:
EKS Clusters:
cdp-prod (us-east-1) and cdp-prod (eu-west-1) have been decommissioned as of now.
EC2 Instances:
Instances like P-SEC-PRI-QUALYS-EC2, kali-linux-ec2, PROD-HTTP-PROXY-ASG, Squid-proxy, among others, across regions eu-west-1, eu-west-2, and us-east-1 are still running. We are in the process of pausing them in a phased manner to ensure system integrity.
AWS IAM Access Keys:
Various IAM users with access keys that are still active, such as srv-cdp-tableau-prod, swc-cdp-accurate-control-hub-prod, and swc-WUAT-ARIC will be reviewed and deactivated systematically in the next phase.
High-Level Impact:
Service Availability:
Services linked to the EKS clusters and EC2 instances mentioned above will experience a controlled pause as part of the decommissioning process. Any active workloads will be halted according to the predefined recovery and rollback plans.
Security Measures:
Active IAM keys are being systematically deactivated to prevent unauthorized access. All security protocols are adhered to in this transition.
Next Steps:
Once services are paused, we will proceed with a final validation to ensure all essential resources have been successfully decommissioned.
Further communications will follow as we progress into the subsequent phases of decommissioning.
Timeline:
The service pausing will be completed by [insert expected date], after which full decommissioning tasks will follow.
We will provide updates as we complete each phase. Please feel free to reach out if you have any questions or concerns about the process.


Important Notes:
EC2 Instances:
The EC2 instances being paused as part of this decommissioning process may not be recoverable or may not start again after pausing. Please ensure that any critical data or configurations associated with these instances have been backed up, as their state post-decommissioning is not guaranteed.
EKS Clusters:
Both cdp-prod clusters have already been decommissioned and are no longer available.
IAM Access Keys:
IAM access keys for users across various regions will be systematically deactivated. Please note that once deactivated, these users will no longer have access to AWS services or resources within this account.
Next Steps:
Once the pausing and decommissioning tasks are completed, we will conduct a final review to ensure all services have been properly decommissioned. Further communications will follow as we progress through these phases.

If you have any questions or need assistance with specific resources, please don't hesitate to reach out.



-----
Task 1: Stop EC2 Instances and Deactivate AWS IAM Access Keys
Step-by-Step Technical Steps:

List and Stop EC2 Instances:
Identify Active EC2 Instances:
bash
Copy code
aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,Tags[?Key==`Name`].Value]' --output table
Stop EC2 Instances:
bash
Copy code
aws ec2 stop-instances --instance-ids <instance_id1> <instance_id2> ...
Replace <instance_id1> <instance_id2> with the instance IDs obtained from the above command.
Validate EC2 Instances are Stopped:
bash
Copy code
aws ec2 describe-instances --instance-ids <instance_id1> <instance_id2> ... --query 'Reservations[*].Instances[*].State.Name' --output text
Check that the status is stopped for each instance.
List and Deactivate AWS IAM Access Keys:
List IAM Users:
bash
Copy code
aws iam list-users --query 'Users[*].UserName' --output table
List Active Access Keys for Each User:
bash
Copy code
aws iam list-access-keys --user-name <username> --query 'AccessKeyMetadata[*].[AccessKeyId,Status]' --output table
Replace <username> with the IAM user name obtained from the above list.
Deactivate Access Keys (Mark as Inactive):
bash
Copy code
aws iam update-access-key --user-name <username> --access-key-id <access_key_id> --status Inactive
Replace <access_key_id> and <username> with appropriate values.
Validate Access Keys are Deactivated:
bash
Copy code
aws iam list-access-keys --user-name <username> --query 'AccessKeyMetadata[*].[AccessKeyId,Status]' --output table
Ensure the status is Inactive.
Task 2: Remove EKS Clusters
Step-by-Step Technical Steps:

List EKS Clusters:
Identify Existing EKS Clusters:
bash
Copy code
aws eks list-clusters --output table
This command lists all active EKS clusters.
Delete EKS Node Groups (if present):
List Node Groups for the Cluster:
bash
Copy code
aws eks list-nodegroups --cluster-name <cluster_name> --output table
Replace <cluster_name> with the name of the EKS cluster.
Delete Node Groups:
bash
Copy code
aws eks delete-nodegroup --cluster-name <cluster_name> --nodegroup-name <nodegroup_name>
Replace <cluster_name> and <nodegroup_name> with respective values.
Delete EKS Cluster:
Delete the EKS Cluster:
bash
Copy code
aws eks delete-cluster --name <cluster_name>
Replace <cluster_name> with the appropriate cluster name.
Clean Up Resources Associated with the EKS Cluster:
Delete Associated Load Balancers: List load balancers associated with the cluster and manually delete them if they do not auto-clean.
bash
Copy code
aws elb describe-load-balancers --query 'LoadBalancerDescriptions[*].[LoadBalancerName]' --output table
Then, delete using:
bash
Copy code
aws elb delete-load-balancer --load-balancer-name <load_balancer_name>
Delete Security Groups: Identify and delete security groups associated with the EKS cluster.
bash
Copy code
aws ec2 describe-security-groups --query 'SecurityGroups[*].[GroupId,GroupName]' --output table
Then delete:
bash
Copy code
aws ec2 delete-security-group --group-id <security_group_id>
Task 3: Technical Validation of the Decommissioning Process
Step-by-Step Validation:

Validate EC2 Instances are Stopped:
Check the Status of EC2 Instances:
bash
Copy code
aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name]' --output table
Ensure all instances are in the stopped state.
Validate IAM Keys are Deactivated:
List and Check Access Key Status:
bash
Copy code
aws iam list-access-keys --user-name <username> --query 'AccessKeyMetadata[*].[AccessKeyId,Status]' --output table
Ensure that all keys have a status of Inactive.
Validate EKS Clusters and Resources are Deleted:
Check for Remaining EKS Clusters:
bash
Copy code
aws eks list-clusters --output table
Ensure no clusters are listed.
Check for Remaining Node Groups:
bash
Copy code
aws eks list-nodegroups --cluster-name <cluster_name> --output table
Ensure no node groups are listed for the deleted cluster.
Check for Remaining Load Balancers:
bash
Copy code
aws elb describe-load-balancers --output table
Ensure no associated load balancers exist.
Check for Orphaned Security Groups:
bash
Copy code
aws ec2 describe-security-groups --query 'SecurityGroups[*].[GroupId,GroupName]' --output table
Ensure all unused security groups are deleted.
Monitor CloudWatch Logs for Anomalies:
Check for any unauthorized access or failures during decommissioning via CloudWatch logs:
bash
Copy code
aws logs describe-log-groups --query 'logGroups[*].logGroupName' --output table
This step-by-step approach covers the technical process for stopping EC2 instances, deactivating IAM keys, removing EKS clusters, and then validating the success of these actions to ensure a smooth decommissioning process.



----
High-Level Process for AWS Account Decommissioning
Stop EC2 Instances:
Identify Instances: List all active EC2 instances in the account.
Notify Stakeholders: Inform all relevant teams about the scheduled downtime.
Stop Instances: Use AWS CLI/console to stop EC2 instances.
Validate: Confirm that all instances have stopped by checking their status.
Deactivate AWS IAM Access Keys:
List Active Keys: Identify all active IAM access keys in the account.
Notify Users: Inform users of impending key deactivation.
Deactivate Keys: Use AWS CLI/console to deactivate (not delete) access keys.
Verify: Ensure that all access keys are marked as “Inactive”.
Remove EKS Clusters:
Identify Clusters: List all active EKS clusters.
Backup Data: If needed, back up critical data, configurations, and node groups.
Delete EKS Clusters: Use AWS CLI/console to delete the EKS clusters.
Cleanup Resources: Ensure associated resources like node groups, security groups, and load balancers are cleaned up.
Verify: Confirm that the clusters and resources have been removed.
Change Plan
Pre-Change Notifications:
Notify all stakeholders and AWS account users regarding the decommissioning process, including the date, time, and expected impact.
Change Execution:
Stop EC2 Instances: Confirm no critical services are running and stop all instances.
Deactivate IAM Keys: Ensure all access keys are properly deactivated without disrupting remaining resources.
Remove EKS Clusters: Ensure proper backup and data transfer, followed by EKS cluster deletion.
Communication Plan:
Establish communication channels (Slack, email) for immediate status updates during the process.
Post-Change Review:
Review the decommissioning steps to ensure everything was executed as planned.
Blackout/Recovery Plan
Blackout Period Planning:
Define a blackout period when the decommissioning will take place.
Ensure no major deployments, updates, or changes will occur in the account during this period.
Notify stakeholders about the blackout window.
Recovery Steps:
EC2 Instance Restart: If necessary, EC2 instances can be restarted by simply starting them from the AWS console/CLI.
Reactivating IAM Keys: Reactivate deactivated IAM keys if access is required by restoring the key status to "Active".
Restore EKS Cluster: If EKS clusters need to be re-created, ensure backed-up configurations and data are used for redeployment.
Backup & Failover Plans:
Maintain backups of important EKS configurations and EC2 snapshots to ensure a quick recovery in case of failure.
Technical Validation Plan
Pre-Validation:
Identify Critical Services: Ensure no critical services are running on the EC2 instances.
Validate IAM Key Usage: Ensure IAM keys being deactivated are not being actively used by critical processes.
EKS Dependencies: Check for any services that rely on the EKS clusters before deletion.
Post-Validation:
Validate EC2 Shutdown: Verify that no EC2 instances are running post-shutdown.
IAM Key Deactivation: Confirm that all IAM keys have been successfully deactivated and that no access is possible.
Confirm EKS Removal: Ensure that the EKS clusters are no longer available and no associated resources are left behind.
Monitoring:
Use CloudWatch to monitor for any unexpected errors, resource usage, or unauthorized access attempts following the decommissioning.


-----
output_file="iam_access_keys.txt"
echo "IAM Access Keys Information" > $output_file
echo "==========================" >> $output_file

aws iam list-users --query 'Users[*].UserName' --output text | tr '\t' '\n' | while read user; do
  echo "Access Keys for User: $user" >> $output_file
  aws iam list-access-keys --user-name $user --query 'AccessKeyMetadata[*].[AccessKeyId,Status,CreateDate]' --output table >> $output_file
  echo "" >> $output_file
done


---
aws iam list-users --query 'Users[*].UserName' --output text | tr '\t' '\n' | while read user; do
  echo "Access Keys for User: $user"
  aws iam list-access-keys --user-name $user --query 'AccessKeyMetadata[*].[AccessKeyId,Status,CreateDate]' --output table
done


---
Based on the image you provided, here are possible answers to the questions from the context of your incident:

What was the issue? (can you please help us in granular details)
The Data Load DAGs were failing at the AWSDataStaging step in the production environment. This failure was causing interruptions in the data pipeline, leading to incomplete or delayed data processing for critical production workloads.
What is the root cause? (can you please help us in granular details)
The root cause of the failure was a paused service in the dts-management EKS cluster (us-east-1 region) due to a change request (CR: CH24001687338). This led to connectivity issues for the Squid proxy and other services depending on this deployment, which directly affected the DAG executions.
What did we do to resolve this?
To resolve the issue, the team identified a working proxy URL (proxy-dts-services.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com) that had been deployed earlier in the dis-management-services EKS cluster. After switching to this new URL, services were restored, and the DAGs resumed their normal execution.
What change was done? What was the existing code/setup and what is it now (can you please help us in details)?
The change involved updating the proxy URL from proxy.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com (which was no longer functional due to the paused service in the dts-management cluster) to the new URL proxy-dts-services.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com. The new URL connects to a service deployed in the dis-management-services EKS cluster. No changes were made to the core infrastructure, except the URL update.
What is the log name that says it's successfully resolved?
The log file indicating successful resolution can be found under the AWSDataStaging step execution logs. Once the new proxy URL was in use, the DAG executions succeeded without errors, and this would be reflected in the specific DAG's success logs.
Did the issue arise due to a past change? If yes, any CR that was associated with this?
Yes, the issue arose due to Change Request (CR: CH24001687338), which paused the deployment of services in the dts-management EKS cluster, rendering the original proxy URL non-functional.
Is there any monitoring setup in place to identify these issues in future?
Yes, we have monitoring in place, including alarms set up in CloudWatch and Prometheus for service health checks and DAG failures. However, based on this incident, we plan to add an additional layer of monitoring for proxy connectivity and EKS service statuses to detect such issues earlier.
Let me know if you'd like any further adjustments or more details added!






You said:
Super, this is good
ChatGPT said:

ChatGPT
Glad to hear that! Feel free to reach out if you need any further assistance or clarifications. I'm here to help!






You said:
Ok we are only concerned and related to the proxy here and no way related to DAG and data load, can you tweak the answers and only provide inputs from our team if necessary and provide our view 
ChatGPT said:

ChatGPT
Got it! Here’s a revised version of the answers focusing solely on the proxy issue and removing any irrelevant details related to DAGs and data loads:

What was the issue? (can you please help us in granular details)
The issue was related to a paused service in the dts-management EKS cluster (us-east-1 region), which caused the proxy URL proxy.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com to stop working. This affected the ability to route traffic through the proxy in the production environment.
What is the root cause? (can you please help us in granular details)
The root cause of the issue was the pause in services within the dts-management EKS cluster (us-east-1 region), executed as part of Change Request (CR: CH24001687338). As a result, the proxy URL tied to that service became non-functional.
What did we do to resolve this?
To resolve the issue, we directed the traffic to a new proxy URL (proxy-dts-services.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com), which was previously deployed in the dis-management-services EKS cluster within the same AWS account. This restored proxy functionality.
What change was done? What was the existing code/setup and what is it now (can you please help us in details)?
The existing setup used the proxy URL proxy.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com, which was tied to the dts-management EKS cluster. After the pause of that service, the setup was updated to use the proxy URL proxy-dts-services.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com, which connects to the dis-management-services EKS cluster.
What is the log name that says it's successfully resolved?
The proxy connectivity logs, along with EKS service logs, confirm the successful switch to the new proxy URL. There is no specific log name from our side related to this issue beyond monitoring traffic routing through the new proxy URL.
Did the issue arise due to a past change? If yes, any CR that was associated with this?
Yes, the issue arose as a direct result of Change Request (CR: CH24001687338), which paused the deployment in the dts-management EKS cluster and caused the old proxy URL to become non-functional.
Is there any monitoring setup in place to identify these issues in future?
Yes, there is proxy monitoring in place, and alerts for proxy connectivity are integrated with our overall monitoring system. We will review our alerting setup to ensure timely detection of service pauses impacting proxy availability in the future.




----
The proxy URL proxy.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com is currently non-functional because we paused the service deployed in the dts-management EKS cluster (us-east-1 region) today as part of the Change Request (CR: CH24001687338). The new proxy URL, proxy-dts-services.cdp-pci-prod.us-east-1.edp.awscloud.worldpay.com, had been deployed some time ago in the dis-management-services EKS cluster (us-east-1 region) in the same AWS account, PPRD-ARIC. Since the services were paused today, this caused the connectivity issue you encountered. Now that you have switched to the new URL, the issue has been resolved, and everything should be functioning properly.

---
Log in to the AWS Management Console and locate the EKS Cluster by navigating to the Elastic Kubernetes Service (EKS).
Identify the Node Group and corresponding Auto Scaling Group (ASG) for the EKS cluster.
Open the EC2 Console to verify the correct ASG associated with the nodes (under Auto Scaling Groups).
Use the following AWS CLI command to set the Auto Scaling Group's desired, minimum, and maximum capacities to 0:
bash
Copy code
aws autoscaling update-auto-scaling-group \
    --auto-scaling-group-name <ASG-NAME> \
    --min-size 0 --max-size 0 --desired-capacity 0
After running the command, verify that the EC2 instances are terminating by checking the EC2 Instances in the AWS Console.
Go back to the EKS Console under the Nodes tab and confirm that the node group has no active nodes.
Optionally, delete the node group in EKS by selecting it and choosing Delete in the Node Groups tab.
Monitor the status in both the EC2 and EKS consoles to ensure the nodes have been fully scaled down or deleted as required.


---
Hi Casey,

As discussed earlier, below is the new URL for Keycloak in the non-prod environment:

[Insert URL here]

Please review the data and other configurations to ensure everything is as expected. If you notice any discrepancies, kindly let us know so we can address them promptly.

We are aware that the Keycloak theme still needs to be updated. Once your validation is complete, we will proceed with updating the theme as part of the next steps.

Additionally, we are continuing discussions regarding the SIEM logs in a parallel thread.

Thanks for your support, and I look forward to your feedback.

Best regards,



-------
Step-by-Step Tasks
1. List All Namespaces

First, list all the namespaces in your cluster to identify where your workloads are running.
Command: kubectl get namespaces
2. Scale Down Deployments

For each namespace, scale down all deployments to zero pods.

# Replace <namespace> with the actual namespace
command:
kubectl get deployments -n <namespace> | awk '{if(NR>1)print $1}' | xargs -I {} kubectl scale deployment {} --replicas=0 -n <namespace>

Repeat this for each namespace where you want to pause services.

3. Scale Down StatefulSets

Scale down all StatefulSets to zero pods.


# Replace <namespace> with the actual namespace
Command:
kubectl get statefulsets -n <namespace> | awk '{if(NR>1)print $1}' | xargs -I {} kubectl scale statefulset {} --replicas=0 -n <namespace>

Repeat for each relevant namespace.

4. Scale Down DaemonSets

DaemonSets typically run a pod on every node, so you might want to consider whether you need to scale these down, depending on your use case.

# Replace <namespace> with the actual namespace
Command:
kubectl get daemonsets -n <namespace> | awk '{if(NR>1)print $1}' | xargs -I {} kubectl scale daemonset {} --replicas=0 -n <namespace>

1. Verify Scaling

After scaling down, verify that there are no running pods left in the affected namespaces.

Command:
kubectl get pods -n <namespace>

Repeat the check for each namespace.

---
For EKS clusters, AWS-managed KMS keys can be used, but we can also use Customer Managed Keys (CMKs) if we need more control over the encryption keys. 
AWS-Managed KMS Keys: These are managed entirely by AWS and offer a simpler solution if we do not need to manage the lifecycle of the key directly. 
Customer Managed KMS Keys (CMKs): If we have specific compliance, security, or auditing requirements that require direct control over the encryption keys.
EKS does not have a hard requirement for using CMKs like EC2 may in certain situations, and we can use either AWS-managed keys or CMKs, depending on our needs. If compliance or strict key control is necessary, then using CMKs would be the way to go.

----


Here's your revised update with the additional section on pending tasks:

Keycloak Migration Feedback: Casey responded positively to the migration plans. He has approved making the necessary changes to the nonprod admin account as required.
SIEM Log Group: Casey mentioned he is only aware of the SIEM log group and does not have further information. We might need to contact the SIEM team for more details; please let me know if you have contact information for them.
URL Sharing: Casey has requested the new URL to be shared for testing in the nonprod environment once the data migration is complete. I confirmed that we would provide the URL for their testing before the official switch over.
Current Data Migration Tasks: I'm working on data migration in the nonprod environment and exploring two paths:
I have prepared export scripts and tried them in the dev environment, but now there seems to be an issue with the nonprod admin login from the backend.
I am currently creating a new Keycloak environment manually and running export scripts during startup from the restored and backup environment.
Additional Pending Tasks:
Update certificates for external access with key factor.
Confirm the current status of SIEM updates, as the existing SIEM does not seem to be functioning properly. Coordination with the SIEM team is needed.
Update Keycloak themes.
Update Keycloak DBA image.
Next Update: I will provide a more detailed update on the data migration process early next week.

---
We are pausing services in the following EKS clusters as part of the EKS decommissioning:

airflow2-cdp-pci-prod in AWS account 771019490822 (dts-transatlanticexpress-elementals) in region US EAST 1
cdp-pci-prod in AWS account 771019490822 (dts-transatlanticexpress-elementals) in region US EAST 1
fisid-dts-management in AWS account 561037286955 (PRRD-ARIC) in region US EAST 1
eks-cluster in AWS account 776419728769 (EDP-Management) in region EU WEST 2

---
Quick question — I noticed that the URL https://teleport.dts-management.wfps.worldpay.com/ isn’t accessible right now. Can you check if it’s down on your side too?

Also, it looks like the URL is hosted on dts-management (PPRD-ARIC). Any idea what the current use case is for the EC2 with Teleport? Just want to see if there’s another way to access it and confirm the setup.

--
I am writing to update you on a significant enhancement to our authentication system infrastructure, specifically concerning our ongoing efforts to upgrade our Keycloak service from version 18.0.2-legacy to 24.0.

Context & Progress: As part of our system improvements, we have successfully deployed the new Keycloak version 24.0 in our development environment. This update is a crucial step toward enhancing our system's reliability, security features, and performance.

Current Phase – Data Migration: To ensure that the new system reflects our operational environment accurately, we are planning to export data from our non-production environment and import it into the upgraded development instance. This procedure is pivotal for validating the new version under realistic data conditions.

Your Participation:

Approval for Data Export: Before proceeding, we need your approval to export the non-production data. Your confirmation will ensure compliance with our data governance policies and respect for the integrity of the data handling processes.
Testing and Validation: Once the data is migrated to the development environment, we crucially need your assistance in testing the updated system. Your insights are invaluable, and your approval on the migration's accuracy will be pivotal in moving forward with full confidence. Could you please confirm your availability and readiness to perform this testing?
Next Steps: Upon receiving your approval, we will initiate the data export and inform you once the import into the development environment is complete. We aim to provide all necessary support for you during the testing phase to ensure a smooth transition.

Your collaboration and timely responses are crucial to keeping this upgrade on track and minimizing any potential disruptions. Please let us know your decision at your earliest convenience.



----
AWS's recent forced upgrade of one of our EKS clusters to version 1.22 has highlighted the urgent need to maintain an actively supported and stable environment. We have already observed issues with some applications during restarts due to compatibility challenges with the new Kubernetes version. In light of these issues, we are proactively recommending an upgrade to EKS version 1.23 to mitigate further potential instability, security vulnerabilities, and operational challenges.

AWS's recent forced upgrade of our EKS cluster to version 1.22 has highlighted the importance of maintaining an actively supported and stable environment. As some applications may encounter restart issues due to compatibility with the new Kubernetes version, it is essential that we take proactive steps. Upgrading to EKS version 1.23 will help us avoid potential instability, security risks, and operational challenges.



Why Not Retire the EKS Cluster Instead?

While retiring the EKS cluster and migrating services to a different platform is part of our long-term plan, it is not feasible to do so immediately due to ongoing dependencies by various teams and applications. The primary goal of this upgrade is to maintain a supported and stable environment for current users, ensuring that their operations are not disrupted during the transition to new clusters. This approach gives teams the necessary time to plan and execute their migrations effectively.

Action Plan:

Pre-Upgrade Preparations:
Backup: Complete a full backup of the current cluster state and all application data before starting the upgrade.
Testing: Perform thorough compatibility testing of all applications with EKS version 1.23 in a staging environment to identify and address potential issues.
Communication: Notify all stakeholders and end-users about the planned downtime and its potential impact on services.
Upgrade Execution:
Schedule the upgrade during off-peak hours to minimize disruption.
Monitor the upgrade process closely, with team members on standby to resolve any issues immediately.
Post-Upgrade Validation:
Verify the stability and functionality of all applications post-upgrade.
Conduct a detailed assessment to ensure that no data loss or application issues occurred during the upgrade.
Provide a comprehensive post-upgrade report to all stakeholders.
Stakeholder Actions Needed:

We request all stakeholders to promptly notify us if your team is currently using this EKS cluster. If you require additional time to migrate your applications to a different environment, please inform us as soon as possible. This will allow us to postpone the upgrade if necessary to accommodate your needs.

If we do not receive timely notification, the upgrade will proceed as planned, and we will not be able to address any issues that may arise from the transition after the fact.

Next Steps:

Please respond at your earliest convenience if your team needs additional time or has concerns about this upgrade. Your feedback is crucial to ensure a smooth process and to avoid any unnecessary disruptions.


---
Context:

Our EKS cluster, initially provisioned on July 3, 2019, has not received upgrades or patches since its deployment. Over the years, the infrastructure, including the EDP Jenkins environment, has required significant manual interventions due to breakages and the lack of automated maintenance. AWS has recently discontinued support for EKS version 1.21 and automatically migrated our cluster to version 1.22. This forced upgrade has introduced potential risks, particularly around application stability upon restart.

Given these developments, it is critical that we perform a planned upgrade to EKS version 1.23. This upgrade is necessary to ensure the continued stability, security, and supportability of our infrastructure, especially for those stakeholders who still rely on this cluster during our broader migration efforts to new clusters.

Current State:

Cluster: EKS Cluster
Region: us-west-2
Current EKS Version: 1.22 (after AWS's automatic upgrade)
Upgrade Target: EKS Version 1.23
Risks and Challenges:

As with any major upgrade, there are several risks and challenges that we must anticipate and manage carefully:

Incompatibility Issues:
Potential Impact: Upgrading the Kubernetes version can introduce breaking changes that may be incompatible with existing applications or configurations.
Mitigation: This could necessitate additional adjustments or patches post-upgrade to ensure all applications function correctly. We plan to conduct thorough testing in a staging environment to identify and resolve these issues before the upgrade.
Downtime:
Potential Impact: The upgrade process could result in significant downtime, impacting all applications running within this cluster. Even with careful planning, some service interruptions are expected during the upgrade.
Mitigation: We will schedule the upgrade during off-peak hours to minimize disruption and will communicate the expected downtime well in advance to all stakeholders.
No Rollback Option:
Potential Impact: Once initiated, the upgrade process cannot be paused, canceled, or rolled back. Any issues encountered during the upgrade could result in prolonged outages or, in a worst-case scenario, permanent loss of applications.
Mitigation: We will have a dedicated team on standby to address any issues that arise immediately during the upgrade, minimizing the risk of extended outages.
Application Failures on Restart:
Potential Impact: Some applications may fail to restart correctly due to compatibility issues with the newer Kubernetes version. This could lead to service disruptions and potentially impact business operations.
Mitigation: By identifying and addressing these issues in a controlled staging environment before the upgrade, we aim to minimize this risk. Additionally, we will have rollback plans for individual applications where feasible, though a full rollback of the cluster is not possible.
Resource Constraints:
Potential Impact: The upgrade may place additional load on cluster resources, potentially leading to resource contention or performance degradation during the process.
Mitigation: We will monitor resource utilization closely throughout the upgrade and be prepared to adjust resource allocations as needed to ensure stability.
Data Integrity Risks:
Potential Impact: During the upgrade process, there is a small risk of data corruption or loss, particularly if any applications are not fully compatible with the new Kubernetes version.
Mitigation: We will perform comprehensive backups of all data and the cluster state prior to the upgrade, ensuring that we can restore any lost data if necessary.
Applications Impacted:

The following applications currently running in the cluster will be affected by the upgrade process:

Application Name	Status	Age
airflow-test	Active	4y21d
amazon-cloudwatch	Active	3y154d
awx-poc	Active	4y7d
cattle-prometheus	Active	4y74d
cert-manager	Active	4y224d
data-exporter	Active	4y224d
data-tools	Active	4y93d
dev-csi	Active	4y450d
ingress-nginx	Active	3y47d
jenkins-test	Active	4y219d
jmeter	Active	3y130d
kube-public	Active	5y53d
kube-system	Active	5y53d
monitoring	Active	4y81d
redshift-exporter	Active	4y292d
tekton	Active	4y31d
wc-test	Active	4y21d
Need for This Upgrade and Why Now:

AWS's recent forced upgrade to EKS version 1.22 has highlighted the urgent need to maintain an actively supported and stable environment. Some applications may fail on restart due to compatibility issues with the newer Kubernetes version. If we do not proactively upgrade to EKS version 1.23, we risk further instability, security vulnerabilities, and operational challenges.

Additionally, this upgrade ensures that we continue to provide a stable and supported environment for stakeholders who still depend on this cluster, allowing them to transition smoothly to alternative clusters at their own pace.

Why Not Retire the EKS Cluster Instead?

While retiring the EKS cluster and migrating services to a different platform is part of our long-term plan, it is not feasible to do so immediately due to ongoing dependencies by various teams and applications. The primary goal of this upgrade is to maintain a supported and stable environment for current users, ensuring that their operations are not disrupted during the transition to new clusters. This approach gives teams the necessary time to plan and execute their migrations effectively.

Action Plan:

Pre-Upgrade Preparations:
Backup: Complete a full backup of the current cluster state and all application data before starting the upgrade.
Testing: Perform thorough compatibility testing of all applications with EKS version 1.23 in a staging environment to identify and address potential issues.
Communication: Notify all stakeholders and end-users about the planned downtime and its potential impact on services.
Upgrade Execution:
Schedule the upgrade during off-peak hours to minimize disruption.
Monitor the upgrade process closely, with team members on standby to resolve any issues immediately.
Post-Upgrade Validation:
Verify the stability and functionality of all applications post-upgrade.
Conduct a detailed assessment to ensure that no data loss or application issues occurred during the upgrade.
Provide a comprehensive post-upgrade report to all stakeholders.
Anticipated Outcomes:

A successful upgrade to EKS version 1.23 will bring the cluster into a fully supported state, ensuring the continued receipt of critical patches and support from AWS. This upgrade will also lay the groundwork for future optimizations and provide a stable environment for users as they plan their migrations to new clusters.

What We Expect from Stakeholders:

Review Your Usage:
We ask all stakeholders to review their current usage of this EKS cluster. If your team requires additional time to migrate to a different environment, please notify us immediately. We are open to postponing the upgrade if necessary to accommodate your needs.
Provide Approval and Feedback:
We need your approval to proceed with the upgrade on the proposed schedule. If there are any concerns or if the timing conflicts with your team’s operations, please let us know as soon as possible so we can make necessary adjustments.
Request for Approval:

To ensure continued support and minimize risks, we request your approval to proceed with the upgrade to EKS version 1.23. Your prompt feedback and approval are crucial for scheduling and executing this upgrade with minimal impact on operations.




---
Subject: Urgent: EKS Cluster Upgrade Required

Context:

The EKS cluster in question has not been upgraded or patched since its initial provisioning on July 3, 2019. The infrastructure, including the EDP Jenkins environment, has experienced significant manual interventions over the years due to breakages and the lack of automated maintenance. Importantly, the current EKS version (1.21) is approaching its end of support from AWS, which will result in the cessation of updates and support services from AWS. Consequently, we must perform an upgrade to EKS version 1.23 to ensure continued stability, security, and supportability of our infrastructure.

Current State:

Cluster: EKS Cluster
Region: us-west-2
Current EKS Version: 1.21
Upgrade Target: EKS Version 1.23
Risks and Challenges:

Incompatibility Issues:
Upgrading the Kubernetes version can introduce breaking changes that may not be compatible with existing applications or configurations. This could potentially require additional adjustments or patches to the current environment post-upgrade.
Downtime:
The upgrade process could lead to significant downtime, impacting all applications running within this cluster. While we will make efforts to minimize this, some level of service interruption is expected during the upgrade.
No Rollback Option:
Once the upgrade is initiated, it cannot be paused, canceled, or rolled back. This irreversible process means that any issues arising during the upgrade could potentially result in a prolonged outage or even permanent loss of applications if not properly managed.
Applications Impacted:

The following applications currently running in the cluster will be affected by the upgrade process:

Application Name	Status	Age
airflow-test	Active	4y21d
amazon-cloudwatch	Active	3y154d
awx-poc	Active	4y7d
cattle-prometheus	Active	4y74d
cert-manager	Active	4y224d
data-exporter	Active	4y224d
data-tools	Active	4y93d
dev-csi	Active	4y450d
ingress-nginx	Active	3y47d
jenkins-test	Active	4y219d
jmeter	Active	3y130d
kube-public	Active	5y53d
kube-system	Active	5y53d
monitoring	Active	4y81d
redshift-exporter	Active	4y292d
tekton	Active	4y31d
wc-test	Active	4y21d
Action Plan:

Pre-Upgrade Preparations:
Backup: Ensure that a full backup of the current cluster state and all application data is completed before starting the upgrade.
Testing: Conduct a thorough assessment of application compatibility with EKS version 1.23 in a staging environment to identify potential issues in advance.
Communication: Notify all stakeholders and end-users about the expected downtime and its impact on services.
Upgrade Execution:
Proceed with the upgrade during off-peak hours to minimize disruption.
Monitor the upgrade process closely, with team members on standby to address any issues immediately.
Post-Upgrade Validation:
Verify the stability and functionality of all applications post-upgrade.
Perform a detailed assessment to ensure that no data loss or application issues occurred during the upgrade.
Provide a detailed post-upgrade report to all stakeholders.
Anticipated Outcomes:

The successful upgrade will bring the cluster into a supported state, ensuring continued receipt of critical patches and support from AWS. It will also lay the groundwork for future upgrades and optimizations.

Request for Approval:

We request approval to proceed with the upgrade process on the proposed schedule, given the outlined risks and the mitigations in place. Please review and confirm.






-----------
aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names <AutoScalingGroupName> --query "AutoScalingGroups[0].Instances[*].{InstanceID:InstanceId, Lifecycle:LifecycleState, InstanceType:InstanceType, LaunchTemplateConfiguration:LaunchConfigurationName, Version:LaunchTemplateVersion, AvailabilityZone:AvailabilityZone, HealthStatus:HealthStatus}" --output table


aws autoscaling describe-auto-scaling-instances --query "AutoScalingInstances[*].{InstanceID:InstanceId, Lifecycle:LifecycleState, InstanceType:InstanceType, LaunchTemplateConfiguration:LaunchConfigurationName, Version:LaunchTemplateVersion, AvailabilityZone:AvailabilityZone, HealthStatus:HealthStatus}" --output table



---
Good evening all,

I hope this message finds you well.

Could I kindly ask for your assistance in approving the following two Change Requests scheduled for tomorrow?

CH24001646853
CH24001658859
These CRs pertain to the renewal of AWS RDS Certificates, and they were recently transitioned to the assessment stage.

Your prompt attention to this would be greatly appreciated.

Thank you so much for your support.


-----
Good Morning All,

@Swapnil Rajgure, thank you for confirming the change state. I also appreciate your efforts in accepting the task and moving it to the accepted state.

We plan to proceed with the change this evening and will notify you once testing is complete.

Change Window:

Start: 2024-08-20, 16:00:00 BST
End: 2024-08-20, 18:30:00 BST




-----
Context:

Hello All,

Today is the planned date for the CR: CH24001658959, which involves renewing the AWS RDS certificates. The initial plan was to proceed with the renewal and have your team assist with validation to ensure there were no impacts on other services.

However, the Change Request (CR) was unexpectedly moved back to the "Assess" state. I’m not entirely sure of the reasons for this but plan to discuss it with the ServiceNow (SNOW) team later today.

Request:

To keep things on track, I have rescheduled the change for tomorrow at the same time. I kindly request your support in accepting and assisting with this task from your end.

Thank you in advance for your cooperation.


-----
Hi Chris,

Thank you for your email. I chose "None" as the correct answer regarding the type of service that could be impacted by this change due to the specific nature of the update being carried out.

This change involves updating the RDS (Relational Database Service) certificates, which is a back-end infrastructure update. It's focused solely on the RDS component and doesn't impact any application logic or processing services.

Given this, I believe "None" is the appropriate classification, as there’s no anticipated risk or impact on any services within the scope of this update. That said, I'm absolutely open to any suggestions or further discussions if you think there might be areas we should revisit.

Please feel free to reach out if you need any further clarification or if there's anything else you'd like to discuss.


----
I hope this message finds you well. We are in the process of the mandatory AWS RDS Certificate renewal and have successfully updated the certificates for our non-production and development environments earlier this week.

We are now preparing to proceed with the renewal of the production RDS certificates and kindly request your approval to move forward. Additionally, we would appreciate your insights on the following queries to ensure seamless coordination and compliance:

Has the client been informed of the upcoming renewal?
Could you provide a list of the Lines of Business (LOBs) that will be affected by this update?
Your prompt response will help us maintain the integrity and security of our services without disruption.

Thank you for your cooperation and support.


----
aws ec2 describe-instances --instance-id i-0123456789abcdef0 --query 'Reservations[*].Instances[*].IamInstanceProfile' --output json

aws ec2 disassociate-iam-instance-profile --association-id <association-id>


# Disassociate the old IAM role (if needed)
aws ec2 disassociate-iam-instance-profile --association-id <association-id>

# Associate the new IAM role
aws ec2 associate-iam-instance-profile --instance-id i-0123456789abcdef0 --iam-instance-profile Name=NewInstanceProfileName




----
Team,

We are undertaking a crucial upgrade of the EKS clusters within our EDP management AWS Account to enhance system integrity and security.

AWS has upgraded to version 1.22, which has introduced potential disruptions. Although some end users are currently utilizing these applications without reported issues, we must remain vigilant. To preemptively address any disruptions and ensure a seamless transition, we are planning a proactive migration to version 1.29. This upgrade is essential for maintaining compatibility with upcoming AWS updates and bolstering our overall system security.

While we anticipate that some applications may experience compatibility issues post-upgrade, we want to emphasize that resolving these issues may not always be within our control. The upgrade may result in certain applications breaking due to dependencies or compatibility problems. We will monitor the situation closely and address any issues that arise to the best of our abilities, but there may be limitations in our capacity to resolve all potential problems.





-----
We are initiating an upgrade of the EKS clusters within our EDP management AWS Account. This is a critical step to maintain the integrity and security of our infrastructure.

AWS has already upgraded the control plane to version 1.22, which has led to some potential application disruptions. To mitigate these issues and address security vulnerabilities, we are advancing to version 1.29. This upgrade is necessary to ensure compatibility with future AWS updates and to strengthen our overall system security.

During this upgrade, we will focus primarily on Jenkins and Keycloak. Our team is prepared to handle any issues that arise in these applications as part of the EKS upgrade process.

We are taking a proactive approach to resolve any disruptions and ensure a smooth transition. Your understanding and cooperation are appreciated as we work through these upgrades.


----
Thank you so much for your confirmation!

Could you please provide the details for our Prod deployment Readiness Checklist? I have prefilled some of the information and would appreciate your assistance in confirming and suggesting any necessary adjustments, especially regarding the impact on LOBs and their clients.

Your support is greatly appreciated.


---
Good afternoon Lokesh,

I hope this message finds you well.

As mentioned in the previous email, we have successfully updated the below RDS DB instance certificates.

To ensure seamless operations, please verify that your applications in the NonProd ARIC Accounts (DEVL-ARIC) are functioning as expected with the new certificates. If you could provide confirmation by this Friday, that would be greatly appreciated. In the absence of any issues, we will proceed with the assumption that everything is operating smoothly.

Additionally, we have scheduled the Prod Account (PPRD-ARIC) RDS Certificate renewals for early next week. Please note that if we do not hear back from you either positively or negatively by this Thursday, we will assume that as positive feedback and proceed with our changes from Monday.

Thank you for your cooperation and understanding. We are committed to providing you with the best support and ensuring minimal disruption.

Best regards,


---
Hi Team,

I am pleased to inform you that the SSL certificate upgrade for our AWS RDS instances has been successfully completed as of July 26, 2024. This upgrade ensures continued security and compliance for our database interactions.

What You Need to Do:

Please conduct a thorough test of your applications using the current RDS instances to confirm that they operate seamlessly with the updated certificates. This is crucial for ensuring that our applications maintain their functionality and performance standards.




----
Good morning all,

Thank you, Courtney and Casey, for your valuable feedback. Regarding your question about updating Keycloak to a newer version, we face a challenge due to our current deployment on the Keycloak 18.0.3-legacy version, which requires migration to a newer version rather than a straightforward update.

Rajesh, I would appreciate your insights and recommendations on migrating from Keycloak 18.0.3-legacy to the latest version, 25.0.2. Your expertise will be crucial in outlining our next steps.

Manuel, in light of our discussions, could you please share your thoughts on the feasibility of the Keycloak upgrade or suggest any alternative solutions that might better suit our needs?

Thanks for your cooperation.

Best regards,
Kiran

---
aws ec2 describe-instances \
    --query "Reservations[*].Instances[*].[InstanceId, Tags[?Key=='Name'].Value | [0], InstanceType]" \
    --filters "Name=instance-state-name,Values=running" \
    --output text



----
build_prompt() {
  # Load current AWS profile
  local current_profile=$AWS_PROFILE

  # Read the configuration file to get the account number, Kubernetes context, and environment type
  local config_line=$(awk -v profile="$current_profile" -F, '$1 == profile {print $0}' /path/to/aws_k8s_config.txt)

  local aws_account=$(echo $config_line | cut -d ',' -f2)
  local k8s_context=$(echo $config_line | cut -d ',' -f3)
  local env_type=$(echo $config_line | cut -d ',' -f4)

  # Your existing prompt components
  prompt_status
  prompt_context
  prompt_dir
  prompt_git
  prompt_bzr
  prompt_hg
  
  # Set AWS details in the prompt, change visual style based on environment type
  if [[ -n "$aws_account" ]]; then
    case "$env_type" in
      prod)
        prompt_segment red yellow "🛑 AWS: ${current_profile} (${aws_account})"
        ;;
      nonprod)
        prompt_segment green black "🌱 AWS: ${current_profile} (${aws_account})"
        ;;
    esac
  fi

  # Set Kubernetes context in the prompt, if applicable
  if [[ -n "$k8s_context" ]]; then
    PS1+="$(kube_ps1) "
  else
    PS1+="[K8s context not applicable] "
  fi

  prompt_end
}

profile1,123456789012,cluster1,prod
profile2,234567890123,cluster2,nonprod
profile3,345678901234,cluster3,nonprod







--------
build_prompt() {
  # Your existing prompt components
  prompt_status
  prompt_context
  prompt_dir
  prompt_git
  prompt_bzr
  prompt_hg
  prompt_aws

  # Fetch AWS account number
  local aws_account=$(aws sts get-caller-identity --output text --query 'Account' 2>/dev/null)
  
  # Decide whether to show kube-ps1 based on AWS account
  case "$aws_account" in
    "123456789012")  # Suppose this is the account linked to your K8s context
      PS1+="$(kube_ps1) "
      ;;
    *)
      # Optionally, you can display nothing or some placeholder
      PS1+="[K8s context not applicable] "
      ;;
  esac

  prompt_end
}




# AWS Profile and Account:
# - display current AWS_PROFILE name and AWS account number
# - displays yellow on red if profile name contains 'production' or ends in '-prod'
# - displays black on green otherwise
prompt_aws() {
  [[ -z "$AWS_PROFILE" || "$SHOW_AWS_PROMPT" = false ]] && return

  # Fetch AWS account number
  local aws_account=$(aws sts get-caller-identity --output text --query 'Account' 2>/dev/null)

  # Check if aws_account is successfully fetched, otherwise show only AWS_PROFILE
  local aws_detail
  if [[ -n "$aws_account" ]]; then
    aws_detail="AWS: ${AWS_PROFILE} (${aws_account})"
  else
    aws_detail="AWS: ${AWS_PROFILE}"
  fi

  # Determine color scheme based on profile name
  case "$AWS_PROFILE" in
    *-prod|*production*) prompt_segment red yellow  "${aws_detail:gs/%/%%}" ;;
    *) prompt_segment green black "${aws_detail:gs/%/%%}" ;;
  esac
}



_kube_ps1_get_context_ns() {
  # Set the command time
  if [[ "$(_kube_ps1_shell_type)" == "bash" ]]; then
    if ((BASH_VERSINFO[0] >= 4 && BASH_VERSINFO[1] >= 2)); then
      _KUBE_PS1_LAST_TIME=$(printf '%(%s)T')
    else
      _KUBE_PS1_LAST_TIME=$(date +%s)
    fi
  elif [[ "$(_kube_ps1_shell_type)" == "zsh" ]]; then
    _KUBE_PS1_LAST_TIME=$EPOCHREALTIME
  fi

  KUBE_PS1_CONTEXT="${KUBE_PS1_CONTEXT:-N/A}"
  KUBE_PS1_NAMESPACE="${KUBE_PS1_NAMESPACE:-N/A}"
  KUBE_PS1_AWS_PROFILE="${KUBE_PS1_AWS_PROFILE:-N/A}"

  # Cache which cfgfiles we can read in case they change.
  local conf
  _KUBE_PS1_CFGFILES_READ_CACHE=
  for conf in $(_kube_ps1_split_config : "${KUBECONFIG:-${HOME}/.kube/config}"); do
    [[ -r $conf ]] && _KUBE_PS1_CFGFILES_READ_CACHE+=":$conf"
  done

  _kube_ps1_get_context
  _kube_ps1_get_ns
  _kube_ps1_get_aws_profile
}

_kube_ps1_get_aws_profile() {
  KUBE_PS1_AWS_PROFILE=$(kubectl config view --minify --output "jsonpath={.contexts[?(@.name=='$(kubectl config current-context)')].context.aws-profile}")
  if [[ -n "$KUBE_PS1_AWS_PROFILE" ]]; then
    export AWS_PROFILE=$KUBE_PS1_AWS_PROFILE
  fi
}

_kube_ps1_get_aws_profile() {
  KUBE_PS1_AWS_PROFILE=$(kubectl config view --minify --output "jsonpath={.users[?(@.name=='$(kubectl config current-context)')].user.exec.env[?(@.name=='AWS_PROFILE')].value}")
  if [[ -n "$KUBE_PS1_AWS_PROFILE" ]]; then
    export AWS_PROFILE=$KUBE_PS1_AWS_PROFILE
  fi
}
